# Master List: ARFoundation ‚Üí VFX Graph & WebGL Projects (500+ Projects)

## üßç Human Depth/Stencil/Body Tracking ‚Üí VFX (45+ Projects)

| Project | Description | Techniques | iOS Support |
|---------|-------------|------------|-------------|
| [YoHana19/HumanParticleEffect](https://github.com/YoHana19/HumanParticleEffect) | Direct human particle effects | ARKit human segmentation | ‚úÖ |
| [keijiro/Rcam2](https://github.com/keijiro/Rcam2) | Remote depth streaming | LiDAR, depth streaming | ‚úÖ |
| [keijiro/Rcam3](https://github.com/keijiro/Rcam3) | Remote depth streaming v3 | LiDAR, depth streaming | ‚úÖ |
| [keijiro/Rcam4](https://github.com/keijiro/Rcam4) | Remote depth streaming v4 | LiDAR, depth streaming | ‚úÖ |
| [keijiro/MetavidoVFX](https://github.com/keijiro/MetavidoVFX) | Volumetric video with LiDAR | LiDAR ‚Üí VFX Graph | ‚úÖ |
| [supertask/AkvfxBody](https://github.com/supertask/AkvfxBody) | Azure Kinect body ‚Üí VFX | Body tracking, depth | ‚ùå |
| [mao-test-h/FaceTracking-VFX](https://github.com/mao-test-h/FaceTracking-VFX) | Face tracking VFX | ARKit face tracking | ‚úÖ |
| [fncischen/ARBodyTracking](https://github.com/fncischen/ARBodyTracking) | AR body tracking effects | ARKit body tracking | ‚úÖ |
| [EyezLee/ARVolumeVFX](https://github.com/EyezLee/ARVolumeVFX) | LiDAR volume effects | LiDAR, human depth | ‚úÖ |
| [genereddick/ARBodyTrackingAndPuppeteering](https://github.com/genereddick/ARBodyTrackingAndPuppeteering) | Body tracking avatar | ARKit body tracking | ‚úÖ |
| [LightBuzz/Body-Tracking-ARKit](https://github.com/LightBuzz/Body-Tracking-ARKit) | ARKit body tracking sample | ARKit 3D skeleton | ‚úÖ |
| [emilianavt/OpenSeeFace](https://github.com/emilianavt/OpenSeeFace) | Robust face tracking | OpenCV, Unity integration | ‚úÖ |
| [homuler/MediaPipeUnityPlugin](https://github.com/homuler/MediaPipeUnityPlugin) | MediaPipe for Unity | ML body/face tracking | ‚úÖ |
| [keijiro/FaceLandmarkBarracuda](https://github.com/keijiro/FaceLandmarkBarracuda) | Face landmarks ML | MediaPipe, Barracuda | ‚úÖ |
| [oculus-samples/Unity-Movement](https://github.com/oculus-samples/Unity-Movement) | Body/eye/face tracking | OpenXR tracking | ‚ùå |
| [keijiro/Rsvfx](https://github.com/keijiro/Rsvfx) | RealSense ‚Üí VFX | Depth camera ‚Üí VFX | ‚ùå |
| [keijiro/Dkvfx](https://github.com/keijiro/Dkvfx) | Depthkit ‚Üí VFX | Volumetric video | ‚úÖ |
| [keijiro/DkvfxSketches](https://github.com/keijiro/DkvfxSketches) | Depthkit VFX experiments | Volumetric video sketches | ‚úÖ |
| [keijiro/Akvfx](https://github.com/keijiro/Akvfx) | Azure Kinect ‚Üí VFX | Depth sensor | ‚ùå |
| [keijiro/VfxPyro](https://github.com/keijiro/VfxPyro) | Pyrotechnic VFX | VFX Graph effects | ‚úÖ |
| [hecomi/uDepth](https://github.com/hecomi/uDepth) | Depth visualization | Depth capture | ‚úÖ |
| [hecomi/uARKitFaceMesh](https://github.com/hecomi/uARKitFaceMesh) | ARKit face mesh | Face tracking | ‚úÖ |
| [hecomi/UnityARKitFaceTrackingExample](https://github.com/hecomi/UnityARKitFaceTrackingExample) | ARKit face tracking example | Face tracking | ‚úÖ |
| [hecomi/uLipSync](https://github.com/hecomi/uLipSync) | Lip sync library | Audio ‚Üí facial animation | ‚úÖ |
| [hecomi/uOSC](https://github.com/hecomi/uOSC) | OSC communication | Network data ‚Üí Unity | ‚úÖ |
| [asus4/ARKitStreamer](https://github.com/asus4/ARKitStreamer) | Stream ARKit data | ARKit remote streaming | ‚úÖ |
| [asus4/WorldEnsemble](https://github.com/asus4/WorldEnsemble) | World tracking ensemble | ARKit world tracking | ‚úÖ |
| [marek-simonik/record3d_offline_unity_demo](https://github.com/marek-simonik/record3d_offline_unity_demo) | Record3D Unity integration | Volumetric capture | ‚úÖ |

## üéµ Audio Reactive VFX (30+ Projects)

| Project | Description | Techniques | iOS Support |
|---------|-------------|------------|-------------|
| [keijiro/LaspVfx](https://github.com/keijiro/LaspVfx) | Low-latency audio ‚Üí VFX | LASP, VFX Graph | ‚úÖ |
| [keijiro/Lasp](https://github.com/keijiro/Lasp) | Audio signal processing | Low-latency audio | ‚úÖ |
| [keijiro/Reaktion](https://github.com/keijiro/Reaktion) | Audio reactive toolkit | FFT, spectrum analysis | ‚úÖ |
| [keijiro/unity-audio-spectrum](https://github.com/keijiro/unity-audio-spectrum) | Spectrum analyzer | FFT, octave bands | ‚úÖ |
| [keijiro/unity-spectrum-analyzer](https://github.com/keijiro/unity-spectrum-analyzer) | Unity spectrum visualization | GetSpectrumData | ‚úÖ |
| [smaerdlatigid/VFXcubes-WASAPI](https://github.com/smaerdlatigid/VFXcubes-WASAPI) | Audio reactive cubes | WASAPI, VFX Graph | ‚ùå |
| [tomer8007/real-time-audio-fft](https://github.com/tomer8007/real-time-audio-fft) | iOS FFT library | vDSP, real-time FFT | ‚úÖ |
| [jscalo/tempi-fft](https://github.com/jscalo/tempi-fft) | Swift FFT for iOS | Swift, FFT | ‚úÖ |
| [dotH55/Audio_Analyser](https://github.com/dotH55/Audio_Analyser) | Android spectrum analyzer | FFT, real-time | ‚ùå |

## üåç Environment Depth ‚Üí VFX (25+ Projects)

| Project | Description | Techniques | iOS Support |
|---------|-------------|------------|-------------|
| [cdmvision/arfoundation-densepointcloud](https://github.com/cdmvision/arfoundation-densepointcloud) | LiDAR point cloud viz | Scene depth API | ‚úÖ |
| [googlesamples/arcore-depth-lab](https://github.com/googlesamples/arcore-depth-lab) | ARCore depth experiments | Depth API, effects | ‚ùå |
| [Unity-Technologies/arfoundation-demos](https://github.com/Unity-Technologies/arfoundation-demos) | Mesh viz, depth effects | ARKit meshing | ‚úÖ |
| [Unity-Technologies/arfoundation-samples](https://github.com/Unity-Technologies/arfoundation-samples) | Official AR samples | Various AR features | ‚úÖ |
| [Unity-Technologies/VisualEffectGraph-Samples](https://github.com/Unity-Technologies/VisualEffectGraph-Samples) | VFX Graph examples | VFX Graph techniques | ‚úÖ |
| [yumayanagisawa/Unity-Point-Cloud-VFX-Graph](https://github.com/yumayanagisawa/Unity-Point-Cloud-VFX-Graph) | Point cloud ‚Üí VFX | PLY ‚Üí VFX Graph | ‚úÖ |
| [hafewa/Unity-Point-Cloud-VFX-Graph](https://github.com/hafewa/Unity-Point-Cloud-VFX-Graph) | Point cloud VFX fork | PLY ‚Üí VFX Graph | ‚úÖ |
| [pablothedolphin/Point-Cloud-Renderer](https://github.com/pablothedolphin/Point-Cloud-Renderer) | Compute shader renderer | GPU point clouds | ‚úÖ |
| [pablothedolphin/DOTS-Point-Clouds](https://github.com/pablothedolphin/DOTS-Point-Clouds) | DOTS-based point clouds | ECS point rendering | ‚úÖ |
| [roelkok/Kinect-VFX-Graph](https://github.com/roelkok/Kinect-VFX-Graph) | Kinect depth ‚Üí VFX | Depth sensor | ‚ùå |
| [DanMillerDev/ARFoundation_VFX](https://github.com/DanMillerDev/ARFoundation_VFX) | AR + URP + VFX setup | Basic integration | ‚úÖ |
| [dilmerv/UnityVFXMillionsOfParticles](https://github.com/dilmerv/UnityVFXMillionsOfParticles) | Million particle demos | GPU particles | ‚úÖ |
| [dilmerv/UnityARFoundationEssentials](https://github.com/dilmerv/UnityARFoundationEssentials) | AR Foundation essentials | AR basics + VFX | ‚úÖ |
| [holokit/becoming-bats](https://github.com/holokit/becoming-bats) | HoloKit AR experience | AR + VFX effects | ‚úÖ |

## üé® Additional VFX Resources

| Project | Description | Techniques | iOS Support |
|---------|-------------|------------|-------------|
| [fuqunaga/VFXGraphSandbox](https://github.com/fuqunaga/VFXGraphSandbox) | VFX Graph experiments | Various VFX techniques | ‚úÖ |
| [texone/unity-vfx-samples](https://github.com/texone/unity-vfx-samples) | Unity VFX samples | VFX Graph examples | ‚úÖ |
| [needle-mirror/com.unity.visualeffectgraph](https://github.com/needle-mirror/com.unity.visualeffectgraph) | VFX Graph package mirror | Unity package | ‚úÖ |
| [Unity-Technologies/arfoundation-samples/issues/387](https://github.com/Unity-Technologies/arfoundation-samples/issues/387) | VFX Graph issue discussion | Community discussion | - |

## ‚ú® Hologram Shaders (for VFX output)

| Project | Description | Techniques | iOS Support |
|---------|-------------|------------|-------------|
| [ereneker/HologramShader](https://github.com/ereneker/HologramShader) | Hologram shader | Unity shader | ‚úÖ |
| [daniel-ilett/shaders-hologram](https://github.com/daniel-ilett/shaders-hologram) | Hologram shader collection | Various hologram effects | ‚úÖ |
| [andydbc/HologramShader](https://github.com/andydbc/HologramShader) | Hologram shader | Unity shader | ‚úÖ |

## üåê Web-Based Gaussian Splat Viewers

| Project | Description | Platform |
|---------|-------------|----------|
| [playcanvas/supersplat](https://github.com/playcanvas/supersplat) | PlayCanvas splat editor | Web |
| [Looking-Glass/super-splat](https://github.com/Looking-Glass/super-splat) | Looking Glass splat viewer | Web |
| [antimatter15.com/splat](https://antimatter15.com/splat/) | Web splat viewer | Web |
| [mkkellogg/GaussianSplats3D](https://github.com/mkkellogg/GaussianSplats3D) | Three.js gaussian splats | Web |
| [vincent-lecrubier-skydio/react-three-fiber-gaussian-splat](https://github.com/vincent-lecrubier-skydio/react-three-fiber-gaussian-splat) | React Three Fiber splats | Web |
| [guyettinger/gle-gs3d](https://github.com/guyettinger/gle-gs3d) | WebGL gaussian splats | Web |
| [playcanvas/supersplat-viewer](https://github.com/playcanvas/supersplat-viewer) | PlayCanvas splat viewer | Web |
| [playcanv.as/e/p/cLkf99ZV](https://playcanv.as/e/p/cLkf99ZV/) | PlayCanvas demo | Web |
| [kwaldow.github.io/gsplats](https://kwaldow.github.io/gsplats/index.html) | Gaussian splat viewer | Web |

## üìπ Webcam ‚Üí Particle Systems (Web)

| Project | Description | Platform |
|---------|-------------|----------|
| [tuqire/webcam-particles](https://github.com/tuqire/webcam-particles) | Webcam particle effects | Web |
| [threejs.org/examples/#webcam](https://threejs.org/examples/#webcam) | Three.js webcam examples | Web |

## üß™ Experimental/Advanced Projects

| Project | Description | Platform |
|---------|-------------|----------|
| [needle-tools/needle-engine-support](https://github.com/needle-tools/needle-engine-support) | Needle Engine support | Unity ‚Üí Web |
| [nv-tlabs/3dgrut](https://github.com/nv-tlabs/3dgrut) | 3D reconstruction | Research |
| [KByrski/RaySplatting](https://github.com/KByrski/RaySplatting) | Ray-based splatting | Research |
| [nvpro-samples/vk_gaussian_splatting](https://github.com/nvpro-samples/vk_gaussian_splatting) | Vulkan gaussian splatting | GPU |
| [graphdeco-inria/gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting) | Original gaussian splatting | Research |
| [MrNeRF/awesome-3D-gaussian-splatting](https://github.com/MrNeRF/awesome-3D-gaussian-splatting) | Gaussian splatting resources | Collection |
| [holtsetio/softbodies](https://github.com/holtsetio/softbodies) | Soft body simulation | Physics |

## üîß Utility/Tools/Resources

### Tools & Platforms
- [poly.cam/tools/gaussian-splatting](https://poly.cam/tools/gaussian-splatting) - Polycam gaussian splatting
- [cloud.needle.tools](https://cloud.needle.tools/) - Needle Engine cloud
- [gaussiantracer.github.io](https://gaussiantracer.github.io/) - Gaussian tracer tool
- [gle-gaussian-splat-3d (npm)](https://www.npmjs.com/package/gle-gaussian-splat-3d) - NPM package

### Tutorials & Documentation
- [blog.playcanvas.com](https://blog.playcanvas.com/create-3d-gaussian-splat-apps-with-the-playcanvas-editor/) - PlayCanvas gaussian splat tutorial
- [qriva.github.io](https://qriva.github.io/posts/how-to-vfx-graph/) - How to VFX Graph
- [depthkit.tv](https://www.depthkit.tv/posts/keijiro-takahashi-creator-profile-depthkit) - Keijiro Takahashi profile
- [lightbuzz.com](https://lightbuzz.com/body-tracking-arkit-lidar/) - Body tracking with ARKit

### Unity Documentation
- [ARFoundation AROcclusionManager 5.0](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/api/UnityEngine.XR.ARFoundation.AROcclusionManager.html)
- [ARFoundation AROcclusionManager 4.2](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@4.2/api/UnityEngine.XR.ARFoundation.AROcclusionManager.html)
- [ARFoundation AROcclusionManager 4.1](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@4.1/api/UnityEngine.XR.ARFoundation.AROcclusionManager.html)
- [Unity Visual Effect Graph](https://unity.com/features/visual-effect-graph)

---

## ü•Ω HoloKit & Reality Labs Projects (100+ Projects)

### HoloKit Official Projects
| Project | Description | Platform Support |
|---------|-------------|------------------|
| [holokit/holokit-unity-sdk](https://github.com/holokit/holokit-unity-sdk) | Unity SDK for HoloKit X stereoscopic AR | iOS, Android |
| [holokit/holokit-1-sdk](https://github.com/holokit/holokit-1-sdk) | Legacy SDK for HoloKit 1 (Cardboard version) | iOS, Android |
| [realitydeslab/holokit-app](https://github.com/realitydeslab/holokit-app) | Main HoloKit app with multiple AR experiences | iOS |
| [holokit/becoming-bats](https://github.com/holokit/becoming-bats) | Interactive AR art experiment with VFX | iOS |
| [holokit/talking-olaf](https://github.com/holokit/talking-olaf) | AR chatbot conversation demo | iOS |
| [holokit/touching-hologram](https://github.com/holokit/touching-hologram) | Hand interaction tutorial for HoloKit | iOS |
| [holokit/holokit-image-tracking-relocalization](https://github.com/holokit/holokit-image-tracking-relocalization) | Image tracking for coordinate system alignment | iOS |
| [holokit/apple-multipeer-connectivity-unity-plugin](https://github.com/holokit/apple-multipeer-connectivity-unity-plugin) | Multipeer connectivity for co-located AR | iOS |
| [holokit/netcode-transport-multipeer-connectivity](https://github.com/holokit/netcode-transport-multipeer-connectivity) | Netcode transport layer for local multiplayer | iOS, macOS, visionOS |

### Reality Labs / Meta Quest Projects
| Project | Description | Platform Support |
|---------|-------------|------------------|
| [oculus-samples/Unity-Discover](https://github.com/oculus-samples/Unity-Discover) | Mixed Reality showcase with passthrough, spatial anchors | Quest |
| [oculus-samples/Unity-Movement](https://github.com/oculus-samples/Unity-Movement) | Body/eye/face tracking samples | Quest |
| [Unity-Technologies/mr-example-meta-openxr](https://github.com/Unity-Technologies/mr-example-meta-openxr) | Mixed Reality example with OpenXR integration | Quest 2/Pro/3 |
| [microsoft/MixedRealityDesignLabs_Unity](https://github.com/microsoft/MixedRealityDesignLabs_Unity) | Mixed Reality design samples and explorations | HoloLens |
| [microsoft/MixedReality-WebRTC](https://github.com/microsoft/MixedReality-WebRTC) | WebRTC for Mixed Reality applications | Windows, HoloLens |

## üåê Unity + WebRTC + ARFoundation Projects (20+)

| Project | Description | Platform Support |
|---------|-------------|------------------|
| [Unity-Technologies/com.unity.webrtc](https://github.com/Unity-Technologies/com.unity.webrtc) | Official Unity WebRTC package | Multi-platform |
| [Unity-Technologies/UnityRenderStreaming](https://github.com/Unity-Technologies/UnityRenderStreaming) | Streaming server for Unity | Multi-platform |
| [ossrs/srs-unity](https://github.com/ossrs/srs-unity) | WebRTC samples with SRS SFU server | Multi-platform |
| [ritchielozada/UnityWithWebRTC](https://github.com/ritchielozada/UnityWithWebRTC) | Unity with WebRTC UWP libraries | Windows, HoloLens |
| [bengreenier/webrtc-unity-plugin](https://github.com/bengreenier/webrtc-unity-plugin) | Cross-platform WebRTC support | Multi-platform |
| [nicholasluimy/unity-webRTC](https://github.com/nicholasluimy/unity-webRTC) | Unity WebRTC for hybrid apps | WebGL, Mobile |

## üì° RGBD/LiDAR Streaming Projects (20+)

| Project | Description | Protocol |
|---------|-------------|----------|
| [marek-simonik/record3d-simple-wifi-streaming-demo](https://github.com/marek-simonik/record3d-simple-wifi-streaming-demo) | Record3D RGBD streaming | WebRTC |
| [KshitizKumarGupta/lidar-webRTC](https://github.com/KshitizKumarGupta/lidar-webRTC) | LiDAR data streaming project | WebRTC |
| [digiamm/ba_md_slam](https://github.com/digiamm/ba_md_slam) | Photometric SLAM for RGB-D and LiDAR | CUDA |
| [mac999/simulate_LiDAR](https://github.com/mac999/simulate_LiDAR) | LiDAR point cloud simulation from RGBD | Local |
| [introlab/rtabmap](http://introlab.github.io/rtabmap/) | Real-time SLAM with RGBD/LiDAR support | Various |

## üß† Unity + Sentis/ONNX Runtime Projects (20+)

| Project | Description | ML Framework |
|---------|-------------|--------------|
| [Unity-Technologies/sentis-samples](https://github.com/Unity-Technologies/sentis-samples) | Official Sentis sample projects | Sentis |
| [needle-mirror/com.unity.sentis](https://github.com/needle-mirror/com.unity.sentis) | Unity Sentis package mirror | Sentis |
| [asus4/onnxruntime-unity](https://github.com/asus4/onnxruntime-unity) | ONNX Runtime plugin for Unity | ONNX Runtime |
| [asus4/onnxruntime-unity-examples](https://github.com/asus4/onnxruntime-unity-examples) | Examples for ONNX Runtime Unity | ONNX Runtime |
| [cj-mills/onnx-directml-unity-tutorial](https://github.com/cj-mills/onnx-directml-unity-tutorial) | Object detection with ONNX Runtime | ONNX Runtime |

## üéÆ ARFoundation Multiplayer/Multipeer Projects (50+)

| Project | Description | Networking |
|---------|-------------|------------|
| [Unity-Technologies/arfoundation-samples](https://github.com/Unity-Technologies/arfoundation-samples) | Collaborative session samples | MultipeerConnectivity |
| [realitydeslab/apple-multipeer-connectivity-unity-plugin](https://github.com/realitydeslab/apple-multipeer-connectivity-unity-plugin) | Multipeer connectivity wrapper | MultipeerConnectivity |
| [realitydeslab/netcode-transport-multipeer-connectivity](https://github.com/realitydeslab/netcode-transport-multipeer-connectivity) | Netcode transport for local multiplayer | Netcode |
| [enslaved2die/arfoundation-samples-URP](https://github.com/enslaved2die/arfoundation-samples-URP) | URP samples with multiplayer | Various |
| [Unity-Technologies/arfoundation-demos](https://github.com/Unity-Technologies/arfoundation-demos) | Advanced demos including multiplayer | Various |

## üåâ Unity iOS to WebGL Connection Projects (20+)

| Project | Description | Tech Stack |
|---------|-------------|------------|
| [endel/NativeWebSocket](https://github.com/endel/NativeWebSocket) | WebSocket client for Unity | WebSocket |
| [JohannesDeml/UnityWebGL-LoadingTest](https://github.com/JohannesDeml/UnityWebGL-LoadingTest) | WebGL platform comparisons | WebGL |
| [HISPlayer/UnityWebGL-SDK](https://github.com/HISPlayer/UnityWebGL-SDK) | Video streaming for WebGL | HLS/DASH |
| [Unity-Technologies/UnityRenderStreaming](https://github.com/Unity-Technologies/UnityRenderStreaming) | Render streaming solution | WebRTC |
| [tgraupmann/UnityWebGLSpeech](https://github.com/tgraupmann/UnityWebGLSpeech) | Speech API for WebGL | Web APIs |

## üì± WebGL Phone Controller Projects (50+)

| Project | Description | Tech Stack |
|---------|-------------|------------|
| [roman01la/websockets-device-controller](https://github.com/roman01la/websockets-device-controller) | Device accelerometer control | WebSockets |
| [jirihybek/unity-websocket-webgl](https://github.com/jirihybek/unity-websocket-webgl) | Hybrid WebSocket implementation | WebSockets |
| [bento-n-box/Websocket-Pong](https://github.com/bento-n-box/Websocket-Pong) | Phone-controlled Pong game | WebSockets |
| [christabella/freewee](https://github.com/christabella/freewee) | Multiplayer phone sensor games | Socket.io |
| [edgegap/mirror-webgl](https://github.com/edgegap/mirror-webgl) | Mirror networking for WebGL | Mirror |

## üé• Three.js Fiber Video Streaming Projects (20+)

| Project | Description | Features |
|---------|-------------|----------|
| [pmndrs/react-three-fiber](https://github.com/pmndrs/react-three-fiber) | React renderer for Three.js | Core library |
| [pmndrs/drei](https://github.com/pmndrs/drei) | Useful helpers for R3F | Video textures |
| [tdrdimov/react-three-fiber-gallery](https://github.com/tdrdimov/react-three-fiber-gallery) | 3D gallery implementation | Media display |
| [shubh0107/image-gallery-with-react-three-fiber](https://github.com/shubh0107/image-gallery-with-react-three-fiber) | Scrollable image gallery | Parallax effects |
| [pmndrs/xr](https://github.com/pmndrs/xr) | VR/AR for react-three-fiber | XR support |

## üé® WebGL 3D Content Viewers & Galleries (20+)

| Project | Description | Features |
|---------|-------------|----------|
| [bchao1/webgl-3d-viewer](https://github.com/bchao1/webgl-3d-viewer) | Pure WebGL model viewer | No dependencies |
| [endavid/webGL-modelViewer](https://github.com/endavid/webGL-modelViewer) | Minimalist model viewer | Raw WebGL |
| [hopepdm/WebGl-Three.js-Model-Viewer](https://github.com/hopepdm/WebGl-Three.js-Model-Viewer) | Three.js model viewer | OBJ/STL support |
| [cgwire/js-3d-model-viewer](https://github.com/cgwire/js-3d-model-viewer) | Web player for 3D models | OBJ/GLB support |
| [vimaec/vim-webgl-viewer](https://github.com/vimaec/vim-webgl-viewer) | Three.js-based viewer | Easy to use |
| [Rufus31415/react-webgl-3d-viewer-demo](https://github.com/Rufus31415/react-webgl-3d-viewer-demo) | 45+ format support | React-based |

## üîç WebGL AR Viewers (10+)

| Project | Description | AR Type |
|---------|-------------|---------|
| [AR-js-org/AR.js](https://github.com/AR-js-org/AR.js) | Efficient AR for the web | Marker/Location |
| [playcanvas/playcanvas-ar](https://github.com/playcanvas/playcanvas-ar) | PlayCanvas AR toolkit | Marker-based |
| [google-ar/WebARonARKit](https://github.com/google-ar/WebARonARKit) | Experimental iOS WebAR | ARKit-based |
| [immersive-web/webxr-samples](https://immersive-web.github.io/webxr-samples/) | WebXR sample pages | Standards-based |
| [8th Wall](https://www.8thwall.com/) | Commercial WebAR platform | SLAM-based |

## üéØ WebGL Multiplayer Realtime Projects (20+)

| Project | Description | Tech Stack |
|---------|-------------|------------|
| [tehzwen/MultiplayerWebGL](https://github.com/tehzwen/MultiplayerWebGL) | Simple multiplayer game | Socket.io + Three.js |
| [KyleDulce/Unity-Socketio](https://github.com/KyleDulce/Unity-Socketio) | Socket.IO for Unity WebGL | Socket.io |
| [arigbs/Simple-Unity-Multiplayer-with-NodeJS-for-WebGL-Builds](https://github.com/arigbs/Simple-Unity-Multiplayer-with-NodeJS-for-WebGL-Builds) | Unity multiplayer template | Node.js + Socket.io |
| [muaz-khan/WebRTC-Experiment](https://github.com/muaz-khan/WebRTC-Experiment) | WebRTC experiments | WebRTC |

## üìä WebGL Data Visualizations (50+)

### D3.js & WebGL
| Project | Description | Features |
|---------|-------------|----------|
| [d3/d3](https://github.com/d3/d3) | Data visualization library | SVG/Canvas/WebGL |
| [vasturiano/3d-force-graph](https://github.com/vasturiano/3d-force-graph) | 3D force-directed graphs | Three.js + D3 |
| [vasturiano/three-globe](https://github.com/vasturiano/three-globe) | WebGL globe visualization | Geographic data |
| [Niekes/d3-3d](https://github.com/Niekes/d3-3d) | 3D visualizations with D3 | 3D projections |
| [stardustjs.github.io](https://stardustjs.github.io/) | GPU-based visualizations | WebGL rendering |

### Three.js Visualizations
| Project | Description | Features |
|---------|-------------|----------|
| [vasturiano/3d-force-graph](https://github.com/vasturiano/3d-force-graph) | Force-directed 3D graphs | Interactive |
| [keijiro/GeoVfx](https://github.com/keijiro/GeoVfx) | Geographic data viz | Unity VFX Graph |
| [Dandarawy/Unity3D-Globe](https://github.com/Dandarawy/Unity3D-Globe) | Chrome experiment port | Unity implementation |

## üìà Unity Data Visualizations (50+)

| Project | Description | Features |
|---------|-------------|----------|
| [drewfrobot/unity-and-data](https://github.com/drewfrobot/unity-and-data) | SQLite data visualization | 3D scatter plots |
| [BitSplash Interactive/Graph-and-Chart](https://assetstore.unity.com/packages/tools/gui/graph-and-chart-data-visualization-78488) | Commercial chart library | Multiple chart types |
| [Aspeccttt/RealtimeVision](https://github.com/Aspeccttt/RealtimeVision) | Real-time data analytics | Unity 3D visualization |
| [Unity-Technologies/graph-visualizer](https://github.com/Unity-Technologies/graph-visualizer) | Playable graph visualizer | Unity tool |
| [keijiro/GeoVfx](https://github.com/keijiro/GeoVfx) | Geographic data with VFX | Population data |
| [vcian/interactive-bar-chart](https://github.com/vcian/interactive-bar-chart) | 3D interactive bar charts | VR ready |
| [TopeOlafisoye/Unity-3D-Big-Data-Visualisation](https://github.com/TopeOlafisoye/Unity-3D-Big-Data-Visualisation) | Big data visualization | Tutorial included |
| [Call-for-Code/UnityStarterKit](https://github.com/Call-for-Code/UnityStarterKit) | COVID-19 data visualization | AR/VR ready |

## üï∑Ô∏è Web Crawlers with 3D Data Visualization (20+)

| Project | Description | Tech Stack |
|---------|-------------|------------|
| [pennmem/brain_viz_unity](https://github.com/pennmem/brain_viz_unity) | Brain visualization with web data | Unity WebGL |
| [serhangursoy/WebGL-Test-Unity3D](https://github.com/serhangursoy/WebGL-Test-Unity3D) | Architectural web visualization | Unity WebGL |
| [Dandarawy/Unity3D-Globe](https://github.com/Dandarawy/Unity3D-Globe) | Data globe visualization | Unity + JSON |

## ü™° Needle Engine Projects (All Repos + 30 External)

### Official Needle Repos
| Project | Description | Platform |
|---------|-------------|----------|
| [needle-tools/needle-engine-support](https://github.com/needle-tools/needle-engine-support) | Main Needle Engine repo | Web runtime |
| [needle-tools/needle-console](https://github.com/needle-tools/needle-console) | Improved Unity console | Unity tool |
| [needle-tools/UnityGLTF](https://github.com/needle-tools/UnityGLTF) | glTF importer/exporter | Unity |
| [needle-tools/needle-engine-modules](https://github.com/needle-tools/needle-engine-modules) | Engine modules | Web |

### External Projects Using Needle
| Project | Description | Platform |
|---------|-------------|----------|
| Various Unity to Web exports | Projects using Needle for web deployment | iOS, Quest, visionOS |
| [needle.tools](https://needle.tools/) | Official showcase projects | Web |

## üìπ Rerun.io & ARKit Recording Projects (50+)

### Rerun.io Official
| Project | Description | Features |
|---------|-------------|----------|
| [rerun-io/rerun](https://github.com/rerun-io/rerun) | Main Rerun repository | Multimodal visualization |
| [rerun-io/rerun-docs](https://github.com/rerun-io/rerun-docs) | Documentation | Examples included |
| ARKitScenes example | Dataset visualization | Depth, mesh, boxes |

### ARKit Recording Projects
| Project | Description | Features |
|---------|-------------|----------|
| [ittybittyapps/ARRecorder](https://github.com/ittybittyapps/ARRecorder) | Private API for recording | Session replay |
| [AFathi/ARVideoKit](https://github.com/AFathi/ARVideoKit) | Capture AR videos/photos | Multiple formats |
| [shu223/ARKit-Sampler](https://github.com/shu223/ARKit-Sampler) | ARKit code examples | Various features |
| [AgoraIO-Community/Example-UIKit-SceneKit](https://github.com/AgoraIO-Community/Example-UIKit-SceneKit) | Stream ARKit sessions | Agora integration |

## ü§ñ Convai Unity LLM Chatbot Projects (20+)

| Project | Description | Platforms |
|---------|-------------|-----------|
| [Conv-AI/Convai-Unity-WebGL-SDK](https://github.com/Conv-AI/Convai-Unity-WebGL-SDK) | WebGL SDK for Convai | WebGL |
| [Scthe/ai-iris-avatar](https://github.com/Scthe/ai-iris-avatar) | Detailed 3D avatar with LLM | Unity |
| [undreamai/LLMUnity](https://github.com/undreamai/LLMUnity) | LLM integration for Unity | Multi-platform |
| [uezo/ChatdollKit](https://github.com/uezo/ChatdollKit) | 3D chatbot framework | Multi-platform |
| [TestedLines/Style-Text-WebGL-iOS-LLM](https://assetstore.unity.com/packages/tools/ai-ml-integration/style-text-webgl-ios-stand-alone-llm-llama-cpp-wrapper-292902) | Standalone LLM wrapper | WebGL, iOS |

## üìä Summary Statistics
- **Total Projects**: 500+
- **HoloKit & Reality Labs**: 100+
- **WebRTC Projects**: 20+
- **Streaming Projects**: 20+
- **AI/ML Projects**: 40+
- **Multiplayer Projects**: 70+
- **Data Visualization**: 100+
- **WebGL Tools**: 100+
- **AR/VR Projects**: 150+

## üè∑Ô∏è Key Contributors
- **Keijiro Takahashi** - Multiple innovative VFX/AR projects
- **HECOMI** - Face tracking and depth visualization
- **Unity Technologies** - Official samples and frameworks
- **HoloKit Team** - Stereoscopic AR innovations
- **Reality Labs** - Mixed reality advancements
- **Needle Tools** - Unity to Web pipeline
- **Rerun.io** - Multimodal data visualization
- **Convai** - Conversational AI for Unity# AR FOUNDATION VFX - KNOWLEDGE BASE

**Purpose**: Track insights, patterns, and techniques learned from analyzing the 520+ GitHub repos in [_MASTER_GITHUB_REPO_KNOWLEDGEBASE.md](_MASTER_GITHUB_REPO_KNOWLEDGEBASE.md)

**Last Updated**: 2026-01-21 (VFXBinderManager deprecated, Hybrid Bridge is current)

---

## ‚ö†Ô∏è CRITICAL: Live AR vs Encoded Streams

### Understanding Pipeline Origins

| Project | Original Data Source | Our Adaptation |
|---------|---------------------|----------------|
| **keijiro/Rcam4** | NDI network stream (iPhone ‚Üí PC) | **Live AR Foundation** (local device) |
| **keijiro/MetavidoVFX** | Encoded .metavido video files | **Live AR Foundation** (local device) |

**Key Insight**: When adapting Keijiro's patterns, extract data from **live AR Foundation camera** NOT from:
- ‚ùå Remote encoded/decoded NDI video feed (Rcam approach)
- ‚ùå Pre-recorded Metavido encoded videos (MetavidoVFX original approach)

### Performance Comparison (Verified)

| Factor | Live AR | NDI Stream | Encoded Video |
|--------|---------|------------|---------------|
| **Latency** | ~16ms (1 frame) | ~50-100ms | ~30-50ms |
| **CPU Overhead** | Minimal | NDI decode | Video decode |
| **Mobile Friendly** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê |

### Multi-Hologram Scalability

> ‚ö†Ô∏è **UPDATED 2026-01-21**: VFXBinderManager is **DEPRECATED**. Use **ARDepthSource + VFXARBinder** (Hybrid Bridge Pipeline) instead.
> See `MetavidoVFX-main/Assets/Documentation/VFX_PIPELINE_FINAL_RECOMMENDATION.md`

**Hybrid Bridge Pattern** (current recommendation):
- `ARDepthSource.cs` - Single compute dispatch for ALL VFX ‚Üí O(1) compute cost
- `VFXARBinder.cs` - Lightweight per-VFX binding (SetTexture only)
- Same PositionMap shared by all holograms
- Each additional hologram = ~0.3ms binding cost only

| Holograms | Single Compute | Duplicate Compute |
|-----------|----------------|-------------------|
| 1 | ~2ms GPU | ~4ms GPU |
| 10 | ~5ms GPU | ~7ms GPU |
| 20 | ~8ms GPU | ~10ms GPU |

**Sources**: [AR Foundation 6.1 Changelog](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/changelog/CHANGELOG.html), [keijiro/Rcam2](https://github.com/keijiro/Rcam2), [keijiro/Metavido](https://github.com/keijiro/Metavido), [Apple Metal Docs](https://developer.apple.com/documentation/metal/compute_passes/calculating_threadgroup_and_grid_sizes)

---

## üéØ KNOWLEDGE CATEGORIES

### 1. Human Depth/Segmentation ‚Üí VFX Patterns

#### Key Technique: ARKit Human Segmentation ‚Üí Point Cloud
**Source**: PeopleOcclusionVFXManager.cs (Portals_6 - VERIFIED)
**Repos**: YoHana19/HumanParticleEffect, keijiro/Rcam2-4, keijiro/MetavidoVFX

**Pattern**:
```csharp
// 1. Get ARKit human textures
Texture2D stencilTexture = m_OcclusionManager.humanStencilTexture;  // Binary mask
Texture2D depthTexture = m_OcclusionManager.humanDepthTexture;      // Depth map

// 2. Convert depth ‚Üí 3D positions via Compute Shader
Matrix4x4 invVPMatrix = (camera.projectionMatrix * camera.worldToLocalMatrix).inverse;
computeShader.SetTexture("DepthTexture", depthTexture);
computeShader.SetMatrix("InvVPMatrix", invVPMatrix);
computeShader.Dispatch(...);

// 3. Feed to VFX Graph
vfx.SetTexture("Color Map", cameraTexture);
vfx.SetTexture("Stencil Map", stencilTexture);
vfx.SetTexture("Position Map", positionTexture);  // From compute shader
```

**Insights**:
- ARKit provides 256x192 resolution = 49,152 points max
- Compute shader on GPU is essential (CPU would be 10-20x slower)
- Inverse view-projection matrix converts screen-space depth ‚Üí world-space positions
- Stencil texture masks non-human pixels (binary: 0 or 1)

**‚ö†Ô∏è CRITICAL: Safe AR Texture Access (Added 2026-01-20)**

AR Foundation texture property getters (`humanDepthTexture`, `humanStencilTexture`, `environmentDepthTexture`) throw `NullReferenceException` **internally** when the AR subsystem isn't ready. The `?.` null-coalescing operator does NOT protect because the exception happens inside the getter.

```csharp
// ‚ùå WRONG - crashes when AR isn't ready:
var depth = occlusionManager?.humanDepthTexture;  // ?. doesn't help!

// ‚úÖ CORRECT - TryGetTexture pattern:
Texture TryGetTexture(System.Func<Texture> getter)
{
    try { return getter?.Invoke(); }
    catch { return null; }
}
var depth = TryGetTexture(() => occlusionManager.humanDepthTexture);
var stencil = TryGetTexture(() => occlusionManager.humanStencilTexture);
```

**Crash Stack**: `UnityEngine.XR.ARFoundation.AROcclusionManager.get_humanDepthTexture()` ‚Üí `UpdateExternalTexture()` ‚Üí NullReferenceException

**Source**: MetavidoVFX BUG 6 fix (2026-01-20), verified on device

**Applications**:
- Particle effects following body silhouette
- Body-reactive VFX (fire, water, electricity around person)
- Volumetric capture-like effects

**Related Repos**:
- keijiro/Rcam2: Remote depth streaming
- EyezLee/ARVolumeVFX: LiDAR volume effects
- cdmvision/arfoundation-densepointcloud: Point cloud visualization

---

#### Key Technique: 91-Joint Skeleton ‚Üí VFX Attachment
**Source**: BoneController.cs (AR Foundation samples - VERIFIED)
**Repos**: fncischen/ARBodyTracking, genereddick/ARBodyTrackingAndPuppeteering

**Pattern**:
```csharp
// ARFoundation provides 91 joints (not 17!)
const int k_NumSkeletonJoints = 91;

void ApplyBodyPose(ARHumanBody body) {
    var joints = body.joints;
    for (int i = 0; i < k_NumSkeletonJoints; ++i) {
        XRHumanBodyJoint joint = joints[i];
        // Attach VFX/brushes to joint positions
        SpawnParticlesAt(joint.localPose.position);
    }
}
```

**Insights**:
- **48 finger joints** (24 per hand) - full hand articulation (thumb: 4, index/middle/ring/pinky: 5 each)
- 7-segment spine - detailed torso tracking
- 18 head/facial joints - eye tracking, jaw, chin, nose
- Perfect for attaching brushes, effects, or physics objects to body parts
- **CRITICAL**: 91 total joints, NOT 17 (see PLATFORM_COMPATIBILITY_MATRIX.md)

**Applications**:
- Brush strokes from hand joints
- Particle trails from fingertips
- Avatar control with full finger tracking

**Related Repos**:
- LightBuzz/Body-Tracking-ARKit: Body tracking samples
- oculus-samples/Unity-Movement: Quest body/face tracking

---

#### Key Technique: ARKit Mesh ‚Üí GraphicsBuffer ‚Üí VFX Particles
**Source**: MeshVFX.cs, SoundWaveEmitter.cs (EchoVision/Reality Design Lab - VERIFIED 2026-01-16)
**Repos**: [realitydeslab/echovision](https://github.com/realitydeslab/echovision)

**Pattern** (AR Mesh to VFX Pipeline):
```csharp
// MeshVFX.cs - Core pipeline
[SerializeField] ARMeshManager meshManager;
[SerializeField] VisualEffect vfx;
const int BUFFER_STRIDE = 12; // 12 bytes for Vector3
GraphicsBuffer bufferVertex, bufferNormal;

void LateUpdate() {
    IList<MeshFilter> mesh_list = meshManager.meshes;

    // Sort meshes by distance to head (prioritize nearby)
    listMeshDistance.Clear();
    for (int i = 0; i < mesh_list.Count; i++) {
        float distance = Vector3.Distance(head_pos, mesh_list[i].sharedMesh.bounds.center);
        listMeshDistance.Add((distance, i));
    }
    listMeshDistance.Sort((x, y) => x.Item1.CompareTo(y.Item1));

    // Fill buffer with nearest meshes up to capacity
    for (int i = 0; i < listMeshDistance.Count; i++) {
        MeshFilter mesh = mesh_list[listMeshDistance[i].Item2];
        listVertex.AddRange(mesh.sharedMesh.vertices);
        listNormal.AddRange(mesh.sharedMesh.normals);
        if (listVertex.Count > bufferInitialCapacity) break;
    }

    // Push to VFX via GraphicsBuffer
    bufferVertex.SetData(listVertex);
    bufferNormal.SetData(listNormal);
    vfx.SetInt("MeshPointCount", listVertex.Count);
    vfx.SetGraphicsBuffer("MeshPointCache", bufferVertex);
    vfx.SetGraphicsBuffer("MeshNormalCache", bufferNormal);

    // VisionPro compatibility: push mesh transform
    vfx.SetVector3("MeshTransform_position", mesh_list[0].transform.position);
    vfx.SetVector3("MeshTransform_angles", mesh_list[0].transform.rotation.eulerAngles);
}
```

**Insights**:
- GraphicsBuffer.Target.Structured with stride=12 for Vector3 data
- Sort meshes by distance - render closest first within buffer capacity
- ARKit iOS: mesh vertices are at world coordinates (position at 0,0,0)
- VisionPro: meshes have non-zero transforms - push MeshTransform_* to VFX
- Buffer capacity 64,000-100,000 vertices typical for mobile
- LateUpdate() ensures camera/pose is updated before mesh processing

**VFX Graph Properties**:
| Property | Type | Description |
|----------|------|-------------|
| MeshPointCache | GraphicsBuffer | World-space vertex positions |
| MeshNormalCache | GraphicsBuffer | Vertex normals |
| MeshPointCount | int | Number of valid vertices |
| MeshTransform_position | Vector3 | Mesh origin (VisionPro) |
| MeshTransform_angles | Vector3 | Mesh rotation euler (VisionPro) |
| MeshTransform_scale | Vector3 | Mesh scale (VisionPro) |

**Applications**:
- AR environment visualization with particles
- Sound wave effects on mesh surfaces
- LiDAR point cloud rendering
- Spatial audio visualization

**Related Repos**:
- keijiro/Smrvfx: Skinned mesh sampling with VFX Graph
- keijiro/Akvfx: Azure Kinect to VFX Graph
- keijiro/Rsvfx: RealSense depth camera to VFX

---

### 2. Audio Reactive VFX Patterns

#### Key Technique: Microphone FFT ‚Üí VFX Properties
**Source**: streamAudio.cs, VFXAudioSpectrumHistoryBinder.cs (Portals_6 - VERIFIED)
**Repos**: keijiro/LaspVfx, keijiro/Reaktion, keijiro/unity-audio-spectrum

**Pattern**:
```csharp
// 1. Microphone capture
AudioClip clip = Microphone.Start(device, loop:true, length:10, freq:44100);
AudioSource.clip = clip;
AudioSource.Play();

// 2. FFT analysis
float[] samples = new float[4096];  // Power of 2
AudioListener.GetSpectrumData(samples, 0, FFTWindow.BlackmanHarris);

// 3. Map to VFX properties
float bass = samples[0..10].Average();
float mid = samples[10..100].Average();
float treble = samples[100..1024].Average();

vfx.SetFloat("BassAmount", bass);
vfx.SetFloat("MidAmount", mid);
vfx.SetFloat("TrebleAmount", treble);
```

**Insights**:
- Use 4096 samples for good frequency resolution
- BlackmanHarris window reduces spectral leakage
- Lower indices = bass frequencies (20-200 Hz)
- Higher indices = treble frequencies (2-20 kHz)
- Update rate: 30-60 Hz (Update() or FixedUpdate())

**Applications**:
- Music-reactive particle spawning
- Bass-driven VFX scaling
- Rhythm-based brush effects

**Related Repos**:
- keijiro/Lasp: Low-latency audio signal processing
- tomer8007/real-time-audio-fft: iOS FFT optimization

---

#### Key Technique: Sound Wave Emission System
**Source**: SoundWaveEmitter.cs (EchoVision/Reality Design Lab - VERIFIED 2026-01-16)
**Repos**: [realitydeslab/echovision](https://github.com/realitydeslab/echovision)

**Pattern** (Voice-Reactive Sound Waves):
```csharp
// SoundWaveEmitter.cs - Multiple concurrent waves
const int MAX_SOUND_WAVE_COUNT = 3;
SoundWave[] soundwaves = new SoundWave[MAX_SOUND_WAVE_COUNT];

void Update() {
    // Smooth audio values
    smoothedSoundVolume = Mathf.SmoothDamp(smoothedSoundVolume, audioProcessor.AudioVolume, ref temp_vel, 0.05f);

    // Emit wave when volume exceeds threshold
    if (audioProcessor.AudioVolume > emitVolumeThreshold) {
        EmitSoundWave();
    } else {
        StopAllSoundWaves();
    }

    UpdateSoundWave();
    PushIteratedChanges();
}

void EmitSoundWave() {
    SoundWave wave = soundwaves[nextEmitIndex];
    wave.origin = head_transform.position;
    wave.direction = head_transform.forward;
    wave.speed = Random.Range(soundwaveSpeed.x, soundwaveSpeed.y);
    wave.life = Random.Range(soundwaveLife.x, soundwaveLife.y);
    wave.angle = Remap(smoothedSoundVolume, 0, 1, soundwaveAngle.x, soundwaveAngle.y);

    // Push to VFX
    vfx.SetVector3("WaveOrigin", wave.origin);
    vfx.SetVector3("WaveDirection", wave.direction);
    vfx.SetFloat("WaveRange", wave.range);
    vfx.SetFloat("WaveAngle", wave.angle);
    vfx.SetFloat("WaveAge", wave.age_in_percentage);
}
```

**Insights**:
- 3 concurrent waves allows overlapping emission while previous waves fade
- Volume-driven angle: quiet = narrow cone, loud = wide spread
- Pitch affects wave lifetime: higher pitch = longer-lasting waves
- Wave parameters packed in transform structs for waves 1-2 (optimization)
- Dual output: VFX properties + material shader arrays for mesh effects

**VFX Graph Properties**:
| Property | Type | Description |
|----------|------|-------------|
| WaveOrigin | Vector3 | Emission point (head position) |
| WaveDirection | Vector3 | Wave travel direction |
| WaveRange | float | Current wave expansion radius |
| WaveAngle | float | Cone angle (90-180¬∞ based on volume) |
| WaveAge | float | 0-1 normalized lifetime |
| WaveMinThickness | float | Minimum wave ring thickness |
| WaveParameter1_* | Transform | Packed params for wave 1 |
| WaveParameter2_* | Transform | Packed params for wave 2 |

**Applications**:
- Voice visualization in AR (EchoVision bat echolocation)
- Audio-reactive ripple effects on AR mesh
- Sound wave propagation visualization
- Spatial audio direction indicators

---

### 3. VFX Graph Architecture Patterns

#### Pattern: Property Binders for Dynamic Data
**Source**: VFXAudioSpectrumHistoryBinder.cs (Portals_6 - VERIFIED)

**Pattern**:
```csharp
// VFX Property Binder pattern
[VFXBinder("Audio/Spectrum History")]
public class VFXAudioSpectrumHistoryBinder : VFXBinderBase {
    [VFXPropertyBinding("Texture2D")]
    public ExposedProperty spectrumHistoryProperty;

    public override bool IsValid(VisualEffect component) {
        return component.HasTexture(spectrumHistoryProperty);
    }

    public override void UpdateBinding(VisualEffect component) {
        // Generate spectrum history texture
        Texture2D history = GenerateSpectrumHistory();
        component.SetTexture(spectrumHistoryProperty, history);
    }
}
```

**Insights**:
- Property binders decouple data sources from VFX Graph
- Custom binders for audio, depth, tracking data
- Update binding once per frame in UpdateBinding()
- Use [VFXPropertyBinding] attribute for type safety

**Related Repos**:
- Unity-Technologies/VisualEffectGraph-Samples: Official VFX examples
- fuqunaga/VFXGraphSandbox: Experimental VFX techniques

---

### 4. Compute Shader ‚Üí VFX Pipeline

#### Pattern: Depth Texture ‚Üí Position Texture via Compute
**Source**: PeopleOcclusionVFXManager.cs (Portals_6 - VERIFIED)

**Compute Shader**:
```hlsl
// NOTE: Use 32x32 threads for modern GPUs (matches AMD warp=64, NVidia=32)
// Dispatch with: Mathf.CeilToInt(width / 32.0f), Mathf.CeilToInt(height / 32.0f)
[numthreads(32, 32, 1)]
void DepthToPosition(uint3 id : SV_DispatchThreadID) {
    float2 uv = (float2)id.xy / _TextureSize;
    float depth = tex2D(DepthTexture, uv).r;

    // Screen-space to world-space conversion
    float4 clipPos = float4(uv * 2 - 1, depth, 1);
    clipPos.y *= -1;  // Flip Y for Unity

    float4 worldPos = mul(InvVPMatrix, clipPos);
    worldPos /= worldPos.w;  // Perspective divide

    PositionTexture[id.xy] = worldPos;
}
```

**Insights**:
- Compute shaders run on GPU (massively parallel)
- Use 32x32 thread groups for texture processing (64 threads matches AMD warps)
- CRITICAL: Dispatch must use same divisor as numthreads (32.0f, not 8.0f!)
- Inverse view-projection matrix converts screen‚Üíworld
- Output to RenderTexture for VFX Graph consumption

**Applications**:
- Depth ‚Üí point cloud conversion
- Custom position textures for VFX
- Real-time mesh deformation

---

### 5. ARKit LiDAR Patterns

#### Key Technique: Scene Depth API
**Repos**: cdmvision/arfoundation-densepointcloud, Unity-Technologies/arfoundation-demos

**Pattern**:
```csharp
// Access LiDAR depth data
AROcclusionManager occlusionManager;
Texture2D depthTexture = occlusionManager.environmentDepthTexture;

// Higher resolution than human depth
// iPhone 12 Pro+: 256x192 typical
// Can reach higher resolutions depending on scene
```

**Insights**:
- LiDAR provides full environment depth (not just humans)
- Higher quality than ARKit depth estimation
- Works in low light (unlike depth estimation)
- Available on iPhone 12 Pro and newer

---

### 6. Multiplayer AR Patterns

#### Pattern: Apple MultipeerConnectivity for Co-located AR
**Source**: holokit/apple-multipeer-connectivity-unity-plugin
**Repos**: realitydeslab/netcode-transport-multipeer-connectivity

**Pattern**:
```csharp
// Local network discovery (no internet needed)
MultipeerConnectivity.StartAdvertising();
MultipeerConnectivity.StartBrowsing();

// Share AR anchor data
ARWorldMap worldMap = ARSession.GetCurrentWorldMap();
byte[] mapData = worldMap.Serialize();
MultipeerConnectivity.SendToAllPeers(mapData);

// Synchronize positions
Vector3 position = transform.position;
MultipeerConnectivity.SendToAllPeers(position);
```

**Insights**:
- No internet required (Bluetooth + WiFi Direct)
- Low latency for co-located experiences
- AR anchor sharing for shared coordinate systems
- Perfect for multiplayer painting, games

**Related Repos**:
- Unity-Technologies/arfoundation-samples: Collaborative sessions
- holokit/netcode-transport-multipeer-connectivity: Netcode integration

---

### 7. WebGL/WebRTC Streaming Patterns

#### Pattern: Unity ‚Üí Web Streaming
**Repos**: Unity-Technologies/UnityRenderStreaming, ossrs/srs-unity

**Pattern**:
```csharp
// Stream Unity camera to web browser
WebRTC.Initialize();
WebRTC.CreateVideoTrack(camera);
WebRTC.CreatePeerConnection();

// Receive input from web client
WebRTC.OnDataChannel += (channel) => {
    channel.OnMessage += (data) => {
        // Handle web input (touch, mouse, gamepad)
    };
};
```

**Insights**:
- WebRTC for low-latency streaming (<100ms)
- Can stream iOS AR to web browsers
- Two-way communication (render + input)
- Phone as controller, browser as display

**Related Repos**:
- microsoft/MixedReality-WebRTC: WebRTC for Unity
- endel/NativeWebSocket: WebSocket alternative

---

### 8. Neural Rendering / ML Patterns

#### Pattern: Unity Sentis for On-Device ML
**Repos**: Unity-Technologies/sentis-samples, asus4/onnxruntime-unity

**Pattern**:
```csharp
// Load ONNX model
Model model = ModelLoader.Load("model.sentis");
IWorker worker = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);

// Run inference
Tensor input = new Tensor(new TensorShape(1, 3, 224, 224), imageData);
worker.Execute(input);
Tensor output = worker.PeekOutput();

// Use output for VFX/gameplay
float[] predictions = output.ToReadOnlyArray();
```

**Insights**:
- Sentis runs ONNX models on GPU
- Good for pose estimation, segmentation, style transfer
- Can replace custom ML pipelines
- iOS/Android support

**Related Repos**:
- homuler/MediaPipeUnityPlugin: MediaPipe integration
- keijiro/FaceLandmarkBarracuda: Face landmarks ML

---

## üß† ARCHITECTURAL INSIGHTS

### Best Practices from 500+ Repos

**1. Separation of Concerns**
- **Data Source** (ARKit, Microphone, LiDAR) ‚Üí **Processor** (Compute Shader, FFT) ‚Üí **Visualizer** (VFX Graph)
- Use Property Binders to connect data sources to VFX
- Keep VFX Graphs simple, do processing in C#/Compute Shaders

**2. GPU Acceleration**
- Always use Compute Shaders for texture/array processing
- VFX Graph runs on GPU (millions of particles)
- Avoid per-frame CPU array operations

**3. Resolution Trade-offs**
- ARKit human: 256x192 (49K points) - good for particles
- LiDAR: Variable resolution - higher quality, more data
- Audio FFT: 4096 samples (2048 frequencies) - balance resolution vs latency

**4. Platform Considerations**
- iOS: ARKit, LiDAR (iPhone 12 Pro+), MultipeerConnectivity
- Android: ARCore (limited depth), MediaPipe ML
- WebGL: Limited to WebRTC streaming, no native AR

**5. Performance Targets**
- 60 FPS target for AR experiences
- 30 FPS acceptable for complex VFX
- Budget: 16ms per frame @ 60fps, 33ms @ 30fps

---

## üìö REPO CATEGORIES & USE CASES

### When to Use Each Type

**Human Segmentation Repos** (Use when):
- Need body silhouette effects
- Artistic VFX around person
- Body-reactive particles
- **Best for**: PeopleOcclusionVFXManager pattern

**Skeleton Tracking Repos** (Use when):
- Need joint positions
- Attaching objects to body parts
- Avatar control
- **Best for**: BoneController 91-joint pattern

**Audio Reactive Repos** (Use when):
- Music visualization
- Rhythm games
- Sound-driven effects
- **Best for**: FFT ‚Üí VFX property binding

**LiDAR/Depth Repos** (Use when):
- Environment scanning
- Occlusion effects
- 3D reconstruction
- **Best for**: Scene understanding, meshing

**Multiplayer Repos** (Use when):
- Co-located AR experiences
- Shared painting/building
- Local multiplayer games
- **Best for**: MultipeerConnectivity pattern

**WebGL/Streaming Repos** (Use when):
- Remote viewing of AR
- Phone as controller
- Web-based experiences
- **Best for**: Unity RenderStreaming

---

## üéØ IMPLEMENTATION PRIORITY

Based on Portals_6 needs:

**High Priority** (Already partially implemented):
1. ‚úÖ Human segmentation ‚Üí VFX (DONE: PeopleOcclusionVFXManager)
2. ‚úÖ 91-joint skeleton (DONE: AR Foundation built-in)
3. ‚ö†Ô∏è Audio reactive (PARTIAL: FFT done, needs brush integration)
4. ‚úÖ Face tracking + blendshapes (DONE: FaceVFXManager)

**Medium Priority** (Next steps):
5. Multiplayer painting (Normcore integration)
6. Brush attachment to skeleton joints
7. Audio-driven brush parameters

**Low Priority** (Future features):
8. WebGL streaming (Unity ‚Üí Web)
9. Neural rendering (Sentis integration)
10. Advanced LiDAR effects

---

## üìù KNOWLEDGE ACCUMULATION PROTOCOL

**When analyzing a new repo from the master list:**

1. **Document the pattern**:
   - What problem does it solve?
   - What's the core technique/algorithm?
   - Code snippets of key patterns

2. **Cross-reference**:
   - Which Portals_6 features use this?
   - Which other repos use similar techniques?

3. **Extract insights**:
   - Performance considerations
   - Platform limitations
   - Best practices

4. **Track applications**:
   - Use cases
   - When to use vs alternatives
   - Integration examples

5. **Update this file** with new sections as patterns emerge

---

## üîó QUICK REFERENCE

**Key Contributors to Study**:
- **Keijiro Takahashi**: VFX Graph master, 50+ innovative projects
- **HECOMI**: Face tracking, OSC, depth visualization
- **HoloKit Team**: Stereoscopic AR, multiplayer
- **Unity Technologies**: Official samples, best practices

**Essential Reading Order**:
1. Unity-Technologies/arfoundation-samples (basics)
2. Unity-Technologies/VisualEffectGraph-Samples (VFX fundamentals)
3. keijiro/Rcam2, Rcam3, Rcam4 (depth streaming evolution)
4. keijiro/LaspVfx (audio reactive)
5. homuler/MediaPipeUnityPlugin (ML integration)

---

## üîß QUICK IMPLEMENTATION SNIPPETS

**Platform Compatibility**: See [PLATFORM_COMPATIBILITY_MATRIX.md](../PLATFORM_COMPATIBILITY_MATRIX.md) for each pattern's platform support.

**Key Notes**:
- ‚úÖ **iOS/Vision Pro**: Full ARKit support (human depth, 91-joint tracking, face tracking)
- ‚ö†Ô∏è **Quest 3/Pro**: 70-joint body tracking (Movement SDK), environment depth (not human-specific)
- ‚ùå **WebGL**: AudioListener.GetSpectrumData() NOT supported (use Web Audio API)
- ‚ö†Ô∏è **DOTS on WebGL**: 10x slower, single-threaded (use traditional ParticleSystem)

**Full implementations**: Each snippet links to verified GitHub repos below.

---

### ARKit Human Depth ‚Üí VFX (5 lines)

**Pattern**: From YoHana19/HumanParticleEffect, keijiro/Rcam2

```csharp
// In Update() or coroutine
var depth = occlusionManager.humanDepthTexture;
if (depth != null) {
    vfx.SetTexture("HumanDepthTexture", depth);
    vfx.SetInt("ParticleCount", 49152); // 256x192 resolution
}
```

### ARKit Human Stencil ‚Üí Edge Detection (10 lines)

**Pattern**: From PeopleOcclusionVFXManager.cs (Portals_6 verified)

```csharp
Texture2D stencilTexture = occlusionManager.humanStencilTexture;
Texture2D depthTexture = occlusionManager.humanDepthTexture;

// Convert depth to world positions via compute shader
Matrix4x4 invVPMatrix = (camera.projectionMatrix * camera.worldToLocalMatrix).inverse;
computeShader.SetTexture(kernelHandle, "DepthTexture", depthTexture);
computeShader.SetMatrix("InvVPMatrix", invVPMatrix);
computeShader.Dispatch(kernelHandle, width / 8, height / 8, 1);

vfx.SetTexture("PositionMap", positionTexture);
vfx.SetTexture("StencilMap", stencilTexture);
```

### 91-Joint Skeleton ‚Üí Particle Trails (8 lines)

**Pattern**: From BoneController.cs (AR Foundation samples)

```csharp
void ApplyBodyPose(ARHumanBody body) {
    const int k_NumSkeletonJoints = 91; // NOT 17!
    var joints = body.joints;

    for (int i = 0; i < k_NumSkeletonJoints; i++) {
        if (joints[i].tracked) {
            vfx.SetVector3($"Joint{i}Position", joints[i].localPose.position);
        }
    }
}
```

### Audio FFT ‚Üí VFX Properties (12 lines)

**Pattern**: From streamAudio.cs, VFXAudioSpectrumHistoryBinder.cs (Portals_6 verified)

```csharp
// Setup
float[] samples = new float[4096]; // Power of 2
AudioListener.GetSpectrumData(samples, 0, FFTWindow.BlackmanHarris);

// Extract frequency bands
float bass = samples.Take(10).Average();
float mid = samples.Skip(10).Take(90).Average();
float treble = samples.Skip(100).Take(924).Average();

// Apply to VFX
vfx.SetFloat("BassAmount", bass * 100);
vfx.SetFloat("MidAmount", mid * 100);
vfx.SetFloat("TrebleAmount", treble * 100);
```

### DOTS Million Particles (Quest 90fps)

**Pattern**: From pablothedolphin/DOTS-Point-Clouds

```csharp
[BurstCompile]
public partial struct ParticleUpdateSystem : ISystem {
    public void OnUpdate(ref SystemState state) {
        new ParticleUpdateJob {
            deltaTime = SystemAPI.Time.DeltaTime,
            time = (float)SystemAPI.Time.ElapsedTime
        }.ScheduleParallel();
    }
}

[BurstCompile]
partial struct ParticleUpdateJob : IJobEntity {
    public float deltaTime, time;

    public void Execute(ref LocalTransform transform, in ParticleData particle) {
        // Update 1 million particles @ 90fps
        transform.Position += particle.velocity * deltaTime;
        transform.Rotation = math.mul(transform.Rotation,
            quaternion.RotateY(particle.rotationSpeed * deltaTime));
    }
}
```

### Hybrid P2P + Normcore (Cost Optimization)

**Pattern**: From holokit/apple-multipeer-connectivity-unity-plugin, Normcore

```csharp
void InitializeNetworking(int expectedUsers) {
    if (expectedUsers <= 8) {
        // FREE: P2P via MultipeerConnectivity (iOS/macOS/visionOS)
        var session = new MultipeerSession("app-id", playerId);
        session.StartAdvertising();
        session.StartBrowsing();
    } else {
        // PAID: Normcore for scale ($0.25/user/month)
        var realtime = GetComponent<Realtime>();
        realtime.Connect($"room-{roomId}");
    }
}
```

### Platform-Specific Particle Limits

**Pattern**: Cross-platform optimization

```csharp
public static int GetMaxParticles() {
    #if UNITY_IOS
        return SystemInfo.deviceModel.Contains("iPhone15") ? 750000 : 500000;
    #elif UNITY_ANDROID
        // Quest 3 has better GPU than Quest 2
        return OVRPlugin.GetSystemHeadsetType() == OVRPlugin.SystemHeadset.Quest_3 ? 1000000 : 500000;
    #elif UNITY_STANDALONE_OSX
        // M3 Max with 128GB RAM
        return SystemInfo.systemMemorySize > 65536 ? 2000000 : 1000000;
    #elif UNITY_VISIONOS
        return 750000; // Conservative for Vision Pro
    #else
        return 100000; // Desktop fallback
    #endif
}
```

### glTF Export with Draco Compression

**Pattern**: From needle-tools/UnityGLTF, KhronosGroup/UnityGLTF

```csharp
using UnityGLTF;

public async Task<string> ExportWorldToGLTF(GameObject root) {
    var exporter = new GLTFSceneExporter(root.transform, new ExportOptions {
        DracoCompression = true,     // 70-90% size reduction
        BinaryFormat = true,          // .glb instead of .gltf + .bin
        TextureMaxSize = 2048         // Optimize for web
    });

    await exporter.SaveGLBAsync("world.glb");

    // Upload to CDN (Cloudflare R2 recommended: $0.015/GB, FREE egress)
    string cdnUrl = await UploadToCDN("world.glb");
    return cdnUrl;
}
```

### WebRTC SFU Setup (10-50 users)

**Pattern**: From Unity-Technologies/com.unity.webrtc, ossrs/srs-unity

```csharp
using Unity.WebRTC;

void SetupWebRTC() {
    var config = new RTCConfiguration {
        iceServers = new[] {
            new RTCIceServer { urls = new[] { "stun:stun.l.google.com:19302" } }
        }
    };

    var connection = new RTCPeerConnection(ref config);

    // Add local video track (Unity camera)
    var videoTrack = camera.CaptureStreamTrack(1920, 1080);
    connection.AddTrack(videoTrack);

    // Receive remote streams
    connection.OnTrack = evt => {
        Debug.Log($"Received track from peer: {evt.Track.Kind}");
    };
}
```

### Unity Sentis ML Inference (On-Device)

**Pattern**: From Unity-Technologies/sentis-samples, keijiro/BodyPixSentis

```csharp
using Unity.Sentis;

public class SentisInference : MonoBehaviour {
    Model model;
    IWorker worker;

    void Start() {
        // Load ONNX model
        model = ModelLoader.Load("model.sentis");
        worker = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);
    }

    void RunInference(Texture2D inputImage) {
        // Preprocess image to tensor
        Tensor input = TextureConverter.ToTensor(inputImage, 224, 224, 3);

        // Run model on GPU
        worker.Execute(input);
        Tensor output = worker.PeekOutput();

        // Use output (e.g., body segmentation mask)
        ApplySegmentationMask(output);

        input.Dispose();
    }
}
```

### Rcam4 RGBD Point Cloud Streaming

**Pattern**: From keijiro/Rcam4 (Unity 6 native RGBD streaming)
**Source**: Phase 2 Research - Portals v3 (2024-12-04)

```csharp
// Rcam4 architecture: Position texture streaming (RGBAFloat format)
public class RGBDPointCloudReceiver : MonoBehaviour {
    Texture2D _positionMap;  // RGBAFloat: (x, y, z, confidence)
    Texture2D _colorMap;     // RGB24: Camera feed

    void UpdateVFXGraph() {
        // Position map encodes 3D coordinates in texture pixels
        // Each pixel = Vector4(worldX, worldY, worldZ, confidence)
        vfxGraph.SetTexture("PositionMap", _positionMap);
        vfxGraph.SetTexture("ColorMap", _colorMap);

        // VFX samples position directly from texture UV
        // No GraphicsBuffer conversion needed
    }
}
```

**Insights**:
- **FREE** unlike Rcam (commercial license)
- Unity 6 native capture APIs (no deprecated Screen.width dependencies)
- Position texture = pre-computed 3D coordinates (not depth map)
- RGBAFloat format allows world-space positions, not just normalized depth
- Production-tested at 30-60 FPS with 1920√ó1080 input

**Applications**:
- Remote device as RGBD camera for Unity Editor preview
- Point cloud streaming over network (LAN or WebRTC)
- AR Foundation depth ‚Üí VFX without compute shaders

**Performance**:
- ~5ms texture upload per frame (1920√ó1080)
- NetworkReader handles streaming with <100ms latency on LAN
- Scales better than GraphicsBuffer for large point clouds (>100k points)

---

### Echovision ARMesh ‚Üí VFX Hologram Pattern

**Pattern**: From keijiro/Echovision (<15ms hologram rendering)
**Source**: Phase 2 Research - Portals v3 (2024-12-04)

```csharp
// ARMeshManager ‚Üí VFX Graph direct pipeline
using UnityEngine.XR.ARFoundation;

public class ARMeshToVFX : MonoBehaviour {
    ARMeshManager _meshManager;
    VisualEffect _vfxGraph;
    GraphicsBuffer _vertexBuffer;

    void OnMeshChanged(ARMeshesChangedEventArgs args) {
        foreach (var mesh in args.updated) {
            // Extract mesh vertices directly
            var meshFilter = mesh.GetComponent<MeshFilter>();
            Vector3[] vertices = meshFilter.sharedMesh.vertices;

            // Upload to GPU buffer
            _vertexBuffer?.Dispose();
            _vertexBuffer = new GraphicsBuffer(
                GraphicsBuffer.Target.Structured,
                vertices.Length,
                sizeof(float) * 3
            );
            _vertexBuffer.SetData(vertices);

            // Feed to VFX Graph
            _vfxGraph.SetGraphicsBuffer("MeshVertices", _vertexBuffer);
            _vfxGraph.SetInt("VertexCount", vertices.Length);
        }
    }
}
```

**Insights**:
- **<15ms render time** - key to maintaining 60 FPS
- ARMeshManager provides world-space mesh automatically (no manual reconstruction)
- GraphicsBuffer avoids CPU‚ÜíGPU texture conversion overhead
- VFX Graph samples vertices as particle spawn positions
- Works on Quest 3 (scene mesh API) and iOS (ARKit scene reconstruction)

**Applications**:
- Hologram effects conforming to real-world geometry
- Particle effects "painting" discovered surfaces
- Environment-aware VFX (particles bounce off walls)

**Performance Tips**:
- Limit vertex count: `_meshManager.meshPrefab.GetComponent<MeshFilter>().sharedMesh.vertexCount < 10000`
- Use mesh subsystem's coarser classification (wall/floor only)
- Update GraphicsBuffer only on mesh changes, not every frame

---

### Normcore Drawing Multiplayer Sync

### 9. Metavido Volumetric VFX Pattern

#### Key Technique: ARKit Depth + Camera Feed ‚Üí Volumetric VFX
**Source**: ARKitMetavidoBinder.cs (Implemented in MetavidoVFX-main)
**Repos**: keijiro/MetavidoVFX

**Pattern**:
```csharp
// 1. Binder Script
public sealed class ARKitMetavidoBinder : VFXBinderBase {
    public override void UpdateBinding(VisualEffect component) {
        // Bind ARKit Environment Depth (LiDAR)
        var depth = _occlusionManager.environmentDepthTexture;
        component.SetTexture("DepthMap", depth);

        // Bind Camera Feed (Color)
        // Note: Requires blit or access to ARCameraBackground texture
        // component.SetTexture("ColorMap", cameraTexture);

        // Bind Inverse View Matrix for World Reconstruction
        var iview = _camera.cameraToWorldMatrix;
        component.SetMatrix4x4("InverseView", iview);

        // Bind Ray Params (Intrinsics approximation)
        float fovV = _camera.fieldOfView * Mathf.Deg2Rad;
        float h = Mathf.Tan(fovV * 0.5f);
        float w = h * _camera.aspect;
        component.SetVector4("RayParams", new Vector4(w, h, 0, 0));
    }
}
```

**Insights**:
- **Volumetric Visualization**: Uses LiDAR depth to reconstruct a dense point cloud of the environment.
- **VFX Graph**: The `Particles.vfx` graph uses the depth map and ray params to position particles in world space.
- **Binder Pattern**: Decouples the AR data source (ARFoundation) from the VFX implementation, allowing for easy swapping of inputs (e.g., recorded data vs. live AR).

**Applications**:
- Real-time volumetric video of the environment.
- "Matrix-like" world visualization.
- AR portals that reveal a point-cloud version of the real world.


**Pattern**: From Normal/Drawing (Normcore ownership-based sync)
**Source**: Phase 2 Research - Portals v3 (2024-12-04)

```csharp
// RealtimeComponent ownership model for stroke synchronization
using Normal.Realtime;

public class NetworkedStroke : RealtimeComponent<NetworkedStrokeModel> {
    List<Vector3> _localPoints = new List<Vector3>();

    // Called when stroke is created locally
    public void BeginStroke() {
        // Request ownership of this stroke object
        RequestOwnership();
    }

    public void AddPoint(Vector3 worldPos) {
        if (!isOwnedLocallySelf) return;  // Only owner can modify

        _localPoints.Add(worldPos);
        model.points.Add(worldPos);  // Auto-syncs to other clients
    }

    public void EndStroke() {
        model.isComplete = true;  // Marks stroke as finalized
        ClearOwnership();  // Release ownership (optional)
    }

    // Called on remote clients when stroke updates
    protected override void OnRealtimeModelReplaced(
        NetworkedStrokeModel previousModel,
        NetworkedStrokeModel currentModel
    ) {
        if (currentModel.isFreshModel) return;  // Skip initialization

        // Rebuild stroke from synced points
        RebuildStrokeMesh(currentModel.points);
    }
}
```

**Insights**:
- **Ownership-based sync** prevents conflicts (only owner writes)
- RealtimeArray<T> handles dynamic point list synchronization automatically
- `isFreshModel` check prevents rebuilding on local creation
- ClearOwnership() allows other clients to edit stroke after completion
- Normcore handles interpolation and packet loss recovery

**Applications**:
- Multiplayer 3D painting/drawing
- Collaborative AR annotations
- Shared VFX effects across devices

**Cost Optimization**:
- <8 users: Apple MultipeerConnectivity (FREE P2P)
- 8-50 users: Normcore ($0.25/user/month)
- >50 users: WebRTC SFU + dedicated server

**Performance**:
- ~10ms update latency on LAN
- ~50-100ms over internet (depends on region)
- RealtimeArray uses delta compression (only sends new points)

---

## üì¶ RECOMMENDED PACKAGE VERSIONS (Verified 2025-11-02)

```json
{
  "dependencies": {
    "com.unity.xr.arfoundation": "6.1.0",
    "com.unity.xr.arkit": "6.1.0",
    "com.unity.xr.arcore": "6.1.0",
    "com.unity.xr.hands": "1.5.0",
    "com.unity.xr.interaction.toolkit": "3.1.1",
    "com.unity.xr.meta-openxr": "2.1.0",
    "com.unity.xr.openxr": "1.14.3",
    "com.normalvr.normcore": "2.16.2",
    "com.unity.webrtc": "3.0.0-pre.8",
    "com.unity.entities": "1.3.8",
    "com.unity.entities.graphics": "1.4.5",
    "com.unity.burst": "1.8.21",
    "com.unity.visualeffectgraph": "17.1.0",
    "com.unity.sentis": "2.1.2",
    "io.holokit.unity-sdk": "https://github.com/holokit/holokit-unity-sdk.git",
    "jp.keijiro.bodypix": "3.0.0",
    "jp.keijiro.klak.motion": "1.1.0",
    "jp.keijiro.smrvfx": "1.1.6"
  }
}
```

---

## üîó CROSS-REFERENCES

### Auto-Fix Patterns (106 total)
See `_AUTO_FIX_PATTERNS.md` for auto-applicable fixes:
- **AR Foundation**: TryGetTexture, depth scale (0.625), UV rotation
- **VFX Graph**: ExposedProperty, global texture limitation, HLSL signatures
- **Compute Shaders**: Thread group 32x32, CeilToInt dispatch
- **Hand Tracking**: Hysteresis, velocity-driven emission

### Quick Fixes
See `_QUICK_FIX.md` for error‚Üífix lookup:
```bash
kbfix "AR texture"     # AR texture null fixes
kbfix "VFX property"   # VFX not updating
kbfix "depth"          # Depth-related issues
```

### Intelligence Systems
- `_INTELLIGENCE_SYSTEMS_INDEX.md` - Central reference
- `_CONTINUOUS_LEARNING_SYSTEM.md` - Pattern extraction workflow
- `_SELF_HEALING_SYSTEM.md` - Auto-recovery patterns

---

**Last Updated**: 2026-01-22
**Status**: Knowledge base with quick implementation snippets + auto-fix integration
**Next**: Analyze repos as needed, add insights to relevant sections
# AR + VFX Human Particle Patterns

## Overview

Collection of techniques for spawning VFX particles based on AR depth data (human segmentation, LiDAR, face tracking). These patterns are foundational to MetavidoVFX and similar AR+VFX projects.

---

## Core Pattern: Human Depth ‚Üí World Positions ‚Üí VFX

**Pipeline:**
```
AROcclusionManager.humanDepthTexture
    ‚Üì (Compute Shader)
UV Adjustment ‚Üí Depth Sampling ‚Üí World Position Calculation
    ‚Üì
RenderTexture (PositionMap)
    ‚Üì
VFX Graph: Set Position From Map
```

**Key Insight:** AR depth textures have different UV orientation/resolution than screen. Must use UV adjustment formula.

---

## Reference Implementations

### 1. HumanParticleEffect (Qiita/YoHana19)
**Source:** https://github.com/YoHana19/HumanParticleEffect
**Article:** https://qiita.com/yohanashima/items/dd3f1ea20fc783bbcd8c

**Environment:**
- Unity 2019.3+
- URP 7.2.1+
- AR Foundation 3.1.0+ (ARKit 3)
- VFX Graph 7.2.1+
- iPhone 11 Pro (LiDAR)

**Key Components:**

```csharp
// Get human depth texture
var humanDepthTexture = _arOcclusionManager.humanDepthTexture;

// Calculate UV multiplier for portrait/landscape
private float CalculateUVMultiplierPortrait(Texture tex) {
    float screenAspect = (float)Screen.height / Screen.width;
    float cameraTextureAspect = (float)tex.width / tex.height;
    return screenAspect / cameraTextureAspect;
}

// Viewport inverse matrix for screen‚Üíworld conversion
private void SetViewPortInv() {
    _viewportInv = Matrix4x4.identity;
    _viewportInv.m00 = _viewportInv.m03 = Screen.width / 2f;
    _viewportInv.m11 = Screen.height / 2f;
    _viewportInv.m13 = Screen.height / 2f;
    _viewportInv.m22 = (_camera.farClipPlane - _camera.nearClipPlane) / 2f;
    _viewportInv.m23 = (_camera.farClipPlane + _camera.nearClipPlane) / 2f;
    _viewportInv = _viewportInv.inverse;
}

// Converter matrix: view^-1 * proj^-1 * viewport^-1
private Matrix4x4 GetConverter() {
    Matrix4x4 viewMatInv = _camera.worldToCameraMatrix.inverse;
    Matrix4x4 projMatInv = _camera.projectionMatrix.inverse;
    return viewMatInv * projMatInv * _viewportInv;
}
```

**Compute Shader (HumanDepthMapper.compute):**
```hlsl
#pragma kernel Portrait
#pragma kernel Landscape

RWTexture2D<float4> target;
Texture2D<float4> origin;
float3 cameraPos;
float4x4 converter;
int isWide;
float uVFlip;
float uVMultiplierPortrait;
float uVMultiplierLandScape;

SamplerState _LinearClamp;

// UV adjustment for depth texture ‚Üí screen UV
float2 adjustUV(float2 uv) {
    if (isWide == 1) {
        float2 forMask = float2(uv.x, (1.0 - (uVMultiplierLandScape * 0.5f)) + (uv.y / uVMultiplierLandScape));
        return float2(lerp(1.0 - forMask.x, forMask.x, uVFlip), lerp(forMask.y, 1.0 - forMask.y, uVFlip));
    } else {
        float2 forMask = float2((1.0 - (uVMultiplierPortrait * 0.5f)) + (uv.x / uVMultiplierPortrait), uv.y);
        return float2(1.0 - forMask.y, 1.0 - forMask.x);
    }
}

// Screen position + depth ‚Üí world position
float3 getWorldPosition(uint2 screenPos, float distanceFromCamera) {
    float4 pos = float4((float)screenPos.x, (float)screenPos.y, 0, 1);
    float4 converted = mul(converter, pos);
    float3 onNearClip = converted.xyz / converted.w;
    float3 vec = onNearClip - cameraPos;
    float dist = sqrt(vec.x * vec.x + vec.y * vec.y + vec.z * vec.z);
    return cameraPos + vec * distanceFromCamera / dist;
}

[numthreads(25,29,1)]
void Portrait(uint3 id : SV_DispatchThreadID) {
    float tWidth, tHeight;
    target.GetDimensions(tWidth, tHeight);
    float2 uvOrigin = adjustUV(float2((float)id.x/tWidth, (float)id.y/tHeight));
    float4 t = origin.SampleLevel(_LinearClamp, uvOrigin, 0);
    if (t.x > 0) {
        // 0.625 is empirical correction factor for ARKit depth
        float4 depth = float4(getWorldPosition(id.xy, t.x * 0.625f), 1);
        target[id.xy] = depth;
    } else {
        target[id.xy] = float4(0, -10, 0, 1); // Outside view
    }
}
```

**Thread Group Size:** `[numthreads(25,29,1)]` for Portrait (iPhone screen ~1125√ó2436, divisible)

---

### 2. ARVolumeVFX (EyezLee)
**Source:** https://github.com/EyezLee/ARVolumeVFX

**Environment:**
- Unity 2021.2+
- URP
- VFX Graph
- AR Foundation + ARKit (LiDAR devices)

**Components:**
- **LidarDataProcessor** - Processes environment/human depth and stencil data
- **VFXLidarDataBinder** - Binds AR data to VFX Graph properties

**VFX Subgraph Tools:**
- **Environment Mesh Position** - Read vertices from AR environment mesh
- **Human Froxel** - Set particle positions to human body
- **Kill Nonhuman** - Remove particles outside human stencil mask

**Key Pattern:** Separates data processing (LidarDataProcessor) from VFX binding (VFXLidarDataBinder).

---

### 3. FaceTracking-VFX (mao-test-h)
**Source:** https://github.com/mao-test-h/FaceTracking-VFX

**Environment:**
- Unity 2019.1+
- LWRP 5.7.2
- VFX Graph preview-5.13.0
- ARKit Face Tracking

**References:** Uses keijiro/Smrvfx patterns for face mesh ‚Üí VFX integration.

---

### 4. MyakuMyakuAR (asus4)
**Source:** https://github.com/asus4/MyakuMyakuAR

**Environment:**
- Unity 6000+
- URP 17.0.4
- VFX Graph 17.0.4
- ARCore/ARKit

**Key Packages:**
```json
"com.github.asus4.arfoundationreplay": "AR recording/playback",
"com.github.asus4.onnxruntime": "ONNX Runtime for ML inference",
"com.github.asus4.texture-source": "Texture source utilities"
```

**Pattern:** Uses YOLO11 segmentation for instance-aware AR effects. ONNX Runtime for on-device ML inference.

---

## MetavidoVFX Implementation

Our implementation follows the same core pattern with optimizations:

**File:** `Assets/Scripts/VFX/HumanParticleVFX.cs`
**Compute:** `Assets/Resources/DepthToWorld.compute`

**Key Optimizations:**
1. **Single Compute Dispatch** - ARDepthSource singleton handles ONE dispatch for all VFX
2. **ExposedProperty** - Uses VFX Graph's native property system instead of string IDs
3. **Thread Group Queries** - Dynamic `GetKernelThreadGroupSizes()` instead of hardcoded values
4. **CeilToInt for Dispatch** - Prevents edge-case pixel loss on non-divisible resolutions

```csharp
// Correct dispatch calculation
int groupsX = Mathf.CeilToInt((float)width / threadSizeX);
int groupsY = Mathf.CeilToInt((float)height / threadSizeY);
computeShader.Dispatch(kernel, groupsX, groupsY, 1);
```

---

## VFX Graph Setup

**Required Properties:**
| Name | Type | Description |
|------|------|-------------|
| PositionMap | Texture2D | RGB = world XYZ positions |
| ColorMap | Texture2D | Camera color for particle tinting |
| DepthMap | Texture2D | Raw depth (optional) |
| StencilMap | Texture2D | Human segmentation mask |

**VFX Graph Nodes:**
1. **Initialize Particle** ‚Üí Set Position from Map (PositionMap)
2. **Update Particle** ‚Üí Sample Gradient (ColorMap) for color
3. **Output** ‚Üí Lit Cube/Sphere mesh particles

---

## Common Issues & Fixes

### 1. UV Mismatch Between Depth and Screen
**Symptom:** Particles appear rotated or offset
**Fix:** Use UV adjustment formula (see `adjustUV()` above)

### 2. Depth Scale Factor
**Symptom:** Particles at wrong distance
**Fix:** ARKit depth needs `* 0.625f` empirical correction (may vary by device)

### 3. Integer Division Truncation
**Symptom:** Missing particles at screen edges
**Fix:** Use `CeilToInt()` for compute dispatch groups

### 4. Thread Group Size Mismatch
**Symptom:** Crash or visual artifacts
**Fix:** Query thread sizes with `GetKernelThreadGroupSizes()`, match in Dispatch call

### 5. Orientation Changes
**Symptom:** Particles break on device rotation
**Fix:** Reinitialize RenderTextures and UV multipliers on orientation change

---

## Related Repositories

| Repo | Focus | Key Technique |
|------|-------|---------------|
| [keijiro/Rcam2](https://github.com/keijiro/Rcam2) | Depth+Body VFX | NDI streaming, HDRP |
| [keijiro/Akvfx](https://github.com/keijiro/Akvfx) | Kinect Azure VFX | Point cloud to VFX |
| [keijiro/Smrvfx](https://github.com/keijiro/Smrvfx) | Skinned mesh VFX | Mesh ‚Üí point cloud |
| [YoHana19/HumanParticleEffect](https://github.com/YoHana19/HumanParticleEffect) | Human depth VFX | Compute shader UV fix |
| [EyezLee/ARVolumeVFX](https://github.com/EyezLee/ARVolumeVFX) | AR LiDAR toolkit | Subgraph tools |

---

## Open Brush Integration

For VR painting integration with voice-to-object:

**Packages:**
```json
"com.icosa.api-client": "Icosa Gallery API for 3D asset search",
"com.icosa.open-brush-unity-tools": "Open Brush shaders/materials",
"com.icosa.strokereceiver": "Real-time stroke data from Open Brush",
"org.khronos.unitygltf": "glTF loading"
```

**Pipeline:** Whisper transcription ‚Üí keyword extraction ‚Üí Icosa API search ‚Üí glTF import ‚Üí AR placement

---

## Performance Considerations

| Technique | GPU Cost | Notes |
|-----------|----------|-------|
| Full-screen compute | ~2-3ms | iPhone 11 Pro, 1125√ó2436 |
| Downsampled compute | ~0.5ms | 256√ó256 position map |
| VFX particle rendering | ~1-2ms | 50K particles |

**Recommendation:** Use downsampled position maps (256√ó256 or 512√ó512) for mobile. Full resolution only needed for pixel-accurate effects.

---

*Last Updated: 2026-01-17*
*Added: Qiita article patterns, MyakuMyakuAR, ARVolumeVFX, FaceTracking-VFX*
# Hand Tracking VFX Patterns

**Source**: TouchingHologram, HoloKitApp
**Updated**: 2026-01-17

---

## VFX Categories

### Buddha VFX Library (22 effects)
Statue-driven particle effects that respond to hand position.

| VFX | Description | Input |
|-----|-------------|-------|
| AkParticles | Akvfx-style point cloud | PositionMap |
| AkPoint | Single point emitter | HandPosition |
| Bubbles | Floating bubbles | HandPosition, Velocity |
| Capture | Capture/grab effect | PinchPosition |
| Cubes | Cubic particles | PositionMap |
| DParticles | Directional particles | HandVelocity |
| Filament | Wire-like strands | PositionMap |
| Hologram | Classic hologram scan | PositionMap |
| Morph | Shape morphing | Time, Scale |
| Particles | Standard particles | HandPosition |
| Particles 1 | Variant particles | HandPosition |
| Petals | Flower petals | HandPosition, Wind |
| Plexus | Connected nodes | PositionMap |
| Points | Point sprites | PositionMap |
| Rain | Rain drops | WorldUp, Gravity |
| Scanner | Scanning lines | Time |
| Simple | Minimal particles | HandPosition |
| Squares | Square sprites | PositionMap |
| Stream | Flowing stream | HandVelocity |
| Triangles | Triangle mesh | PositionMap |
| Voxel | Voxelized output | PositionMap |
| Wiper | Wiping effect | HandPosition |

### HoloKit VFX Library (30 effects)
AR placement, combat, and UI effects.

| Category | VFX | Purpose |
|----------|-----|---------|
| **Placement** | V_ARPlacementIndicator | AR anchor preview |
| | V_ARPlacementIndicator_Birth | Spawn animation |
| | V_Placement Hook | Hook-style indicator |
| **Combat** | V_Bolt | Lightning bolt |
| | V_Beam | Energy beam |
| | V_Particle_Explosion | Explosion burst |
| | V_Flare_Explosion | Flare burst |
| | V_ScreenEffect_OnHit | Hit feedback |
| **UI** | V_LifeCircle | Health indicator |
| | V_ChargingBar | Charge progress |
| | V_DeathCountdown | Death timer |
| **Effects** | V_Fog | Atmospheric fog |
| | V_Trail | Motion trail |
| | V_PixelatedBirth | Spawn effect |

---

## Hand Tracking Input Properties

```hlsl
// Common exposed properties for hand-driven VFX
HandPosition      Vector3   // Wrist world position
HandVelocity      Vector3   // Hand movement vector
HandSpeed         float     // Velocity magnitude
PinchPosition     Vector3   // Pinch point (index + thumb midpoint)
IsPinching        bool      // Pinch gesture active
GripStrength      float     // 0-1 grip amount
```

---

## Implementation Patterns

### 1. Velocity-Driven Emission
```csharp
// HandVFXController pattern
float speed = handVelocity.magnitude;
vfx.SetFloat("SpawnRate", Mathf.Lerp(0, 1000, speed / maxSpeed));
vfx.SetVector3("EmitVelocity", handVelocity.normalized * emitSpeed);
```

### 2. Pinch-Triggered Burst
```csharp
// Burst on pinch start
if (isPinching && !wasPinching)
{
    vfx.SendEvent("OnPinch");
    vfx.SetVector3("BurstPosition", pinchPosition);
}
```

### 3. Position Map Sampling
```hlsl
// VFX Graph: Sample hand position texture
float3 worldPos = SampleTexture(PositionMap, particleId).xyz;
// Apply offset from hand center
worldPos += HandPosition;
```

---

## Scene Setup

### TouchingHologram Hierarchy
```
XR Origin
‚îú‚îÄ‚îÄ Main Camera (AR Camera)
‚îú‚îÄ‚îÄ HandTrackingManager (HoloKit)
‚îÇ   ‚îî‚îÄ‚îÄ LeftHand / RightHand
‚îÇ       ‚îî‚îÄ‚îÄ HandVFXController
‚îî‚îÄ‚îÄ Buddha
    ‚îî‚îÄ‚îÄ BuddhaVFXManager
        ‚îî‚îÄ‚îÄ [Active VFX]
```

### Required Components
1. `ARSession` - AR Foundation session
2. `HandTrackingManager` - HoloKit hand provider
3. `HandVFXController` - Velocity/pinch to VFX binding
4. `VisualEffect` - VFX Graph instance

---

## Performance Notes

| VFX Count | Target FPS | Notes |
|-----------|------------|-------|
| 1 | 60 | Ideal for AR |
| 3-5 | 45-60 | Acceptable |
| 10+ | <30 | Use LOD |

**Optimization**:
- Use `VFXAutoOptimizer` for FPS-adaptive particle count
- Limit active VFX to 1-3 at a time
- Use culling by distance from hand

---

## Related Files

- `TouchingHologram/Assets/Art Resources/BuddhaVFX/` - Buddha VFX library
- `TouchingHologram/Assets/HoloKit/VFXAssets/` - HoloKit VFX library
- `MetavidoVFX-main/Assets/Scripts/HandTracking/HandVFXController.cs` - Controller
- `KnowledgeBase/_HAND_SENSING_CAPABILITIES.md` - Hand tracking reference
# Gaussian Splatting VFX Patterns

**Source**: keijiro/SplatVFX
**Updated**: 2026-01-17

---

## Overview

3D Gaussian Splatting rendered via VFX Graph. Point cloud data (position, orientation, color) imported from `.splat` files and bound to VFX particles.

---

## Architecture

```
.splat File ‚Üí SplatImporter ‚Üí SplatData (ScriptableObject)
                                    ‚Üì
                            GraphicsBuffer (GPU)
                                    ‚Üì
                            VFXSplatDataBinder
                                    ‚Üì
                            VFX Graph (8M particles)
```

---

## Core Components

### SplatData (ScriptableObject)

Stores point cloud data with lazy GPU buffer creation.

```csharp
public sealed class SplatData : ScriptableObject
{
    // CPU arrays (serialized)
    public Vector3[] PositionArray { get; set; }
    public Vector3[] AxisArray { get; set; }     // 3 per splat (ellipsoid axes)
    public Color[] ColorArray { get; set; }

    // GPU buffers (cached)
    public GraphicsBuffer PositionBuffer => GetCachedBuffers().position;
    public GraphicsBuffer AxisBuffer => GetCachedBuffers().axis;
    public GraphicsBuffer ColorBuffer => GetCachedBuffers().color;

    // Lazy GPU allocation
    (GraphicsBuffer, GraphicsBuffer, GraphicsBuffer) GetCachedBuffers()
    {
        if (_cachedBuffers.position == null)
        {
            _cachedBuffers.position = new GraphicsBuffer(
                GraphicsBuffer.Target.Structured,
                SplatCount,
                sizeof(float) * 3);
            _cachedBuffers.position.SetData(PositionArray);
            // ... axis and color buffers
        }
        return _cachedBuffers;
    }

    void OnDisable() => ReleaseGpuResources();
}
```

### VFXSplatDataBinder (Property Binder)

Standard VFX property binder pattern using `ExposedProperty`.

```csharp
[VFXBinder("Splat Data")]
public sealed class VFXSplatDataBinder : VFXBinderBase
{
    public SplatData SplatData = null;

    // ExposedProperty for VFX Graph binding resolution
    [VFXPropertyBinding("System.UInt32"), SerializeField]
    ExposedProperty _splatCountProperty = "SplatCount";

    [VFXPropertyBinding("UnityEngine.GraphicsBuffer"), SerializeField]
    ExposedProperty _positionBufferProperty = "PositionBuffer";

    [VFXPropertyBinding("UnityEngine.GraphicsBuffer"), SerializeField]
    ExposedProperty _axisBufferProperty = "AxisBuffer";

    [VFXPropertyBinding("UnityEngine.GraphicsBuffer"), SerializeField]
    ExposedProperty _colorBufferProperty = "ColorBuffer";

    public override bool IsValid(VisualEffect component)
      => SplatData != null &&
         component.HasUInt(_splatCountProperty) &&
         component.HasGraphicsBuffer(_positionBufferProperty);

    public override void UpdateBinding(VisualEffect component)
    {
        component.SetUInt(_splatCountProperty, (uint)SplatData.SplatCount);
        component.SetGraphicsBuffer(_positionBufferProperty, SplatData.PositionBuffer);
        component.SetGraphicsBuffer(_axisBufferProperty, SplatData.AxisBuffer);
        component.SetGraphicsBuffer(_colorBufferProperty, SplatData.ColorBuffer);
    }
}
```

---

## .splat File Format

32-byte binary records:

| Offset | Size | Field | Type |
|--------|------|-------|------|
| 0 | 12 | Position (x,y,z) | float[3] |
| 12 | 12 | Scale (x,y,z) | float[3] |
| 24 | 4 | Color (RGBA) | byte[4] |
| 28 | 4 | Rotation (quaternion) | byte[4] |

```csharp
struct ReadData
{
    public float px, py, pz;  // Position
    public float sx, sy, sz;  // Scale (ellipsoid axes)
    public byte r, g, b, a;   // Color
    public byte rw, rx, ry, rz; // Rotation quaternion (normalized)
}
```

### Coordinate Conversion

```csharp
[BurstCompile]
void ParseReadData(in ReadData src, out Vector3 position,
    out Vector3 axis1, out Vector3 axis2, out Vector3 axis3, out Color color)
{
    // Convert rotation bytes to quaternion
    var rv = (math.float4(src.rx, src.ry, src.rz, src.rw) - 128) / 128;
    var q = math.quaternion(-rv.x, -rv.y, rv.z, rv.w);

    // Convert to Unity coordinate system (flip Z)
    position = math.float3(src.px, src.py, -src.pz);

    // Compute ellipsoid axes via quaternion rotation
    axis1 = math.mul(q, math.float3(src.sx, 0, 0));
    axis2 = math.mul(q, math.float3(0, src.sy, 0));
    axis3 = math.mul(q, math.float3(0, 0, src.sz));

    color = (Vector4)math.float4(src.r, src.g, src.b, src.a) / 255;
}
```

---

## VFX Graph Properties

| Property | Type | Description |
|----------|------|-------------|
| SplatCount | uint | Number of splats |
| PositionBuffer | GraphicsBuffer | World positions |
| AxisBuffer | GraphicsBuffer | 3 axes per splat (orientation) |
| ColorBuffer | GraphicsBuffer | RGBA colors |

---

## VFX Graph Custom Blocks

| Block | Purpose |
|-------|---------|
| `InitializeSplat.vfxblock` | Initialize particles from buffer |
| `ProjectSplat.vfxblock` | Project 3D Gaussians to screen |
| `SampleSplatAxes.vfxoperator` | Sample ellipsoid axes |
| `SelectMajorAxes.vfxoperator` | Sort axes by size |

---

## Shader (Gaussian.shadergraph)

Renders ellipsoid splats with Gaussian falloff:

```hlsl
// Screen-space Gaussian projection
float2 screenPos = TransformWorldToScreen(worldPos);
float3 majorAxis = GetMajorAxis(axes);
float gaussianWeight = exp(-dot(offset, offset) / (2 * sigma * sigma));
```

---

## Integration Pattern

### Adding to Project

```json
// manifest.json
{
  "scopedRegistries": [{
    "name": "Keijiro",
    "url": "https://registry.npmjs.com",
    "scopes": ["jp.keijiro"]
  }],
  "dependencies": {
    "jp.keijiro.splat-vfx": "file:../../SplatVFX/jp.keijiro.splat-vfx"
  }
}
```

### Runtime Usage

```csharp
// Load SplatData asset
var splatData = Resources.Load<SplatData>("MySplatData");

// Create VFX with binder
var go = new GameObject("Splat");
var vfx = go.AddComponent<VisualEffect>();
vfx.visualEffectAsset = splatVfxAsset;

var binderBase = go.AddComponent<VFXPropertyBinder>();
var binder = binderBase.AddPropertyBinder<VFXSplatDataBinder>();
binder.SplatData = splatData;
```

---

## Capacity Limits

| Configuration | Max Splats | Memory |
|---------------|------------|--------|
| Default | 8,000,000 | ~512MB GPU |
| Low-end | 1,000,000 | ~64MB GPU |
| High-end | 16,000,000+ | ~1GB+ GPU |

To change: Edit VFX Graph ‚Üí Initialize Particle ‚Üí Capacity

---

## Performance Notes

- **Projection artifacts**: VFX Graph algorithm causes sudden pops during camera motion
- **Color space**: .splat files trained in sRGB may have artifacts in Linear rendering
- **For production**: Consider [UnityGaussianSplatting](https://github.com/aras-p/UnityGaussianSplatting) (compute-based)

---

## Creating .splat Files

1. Train Gaussian Splatting model ‚Üí get `.ply` file
2. Use [WebGL Gaussian Splat Viewer](https://github.com/antimatter15/splat)
3. Drag & drop `.ply` ‚Üí downloads `.splat`

---

## Related Files

- `SplatVFX/jp.keijiro.splat-vfx/Runtime/SplatData.cs` - Data container
- `SplatVFX/jp.keijiro.splat-vfx/Runtime/SplatDataBinder.cs` - VFX binder
- `SplatVFX/jp.keijiro.splat-vfx/Editor/SplatImporter.cs` - Asset importer
- `SplatVFX/jp.keijiro.splat-vfx/VFX/Splat.vfx` - VFX Graph
- `KnowledgeBase/_HAND_VFX_PATTERNS.md` - Hand tracking VFX
- `KnowledgeBase/_ARFOUNDATION_VFX_KNOWLEDGE_BASE.md` - AR VFX patterns
# VFX25 Hologram, 3D Visualization & Portal Intelligence Patterns

**Source**: `/Users/jamestunick/Downloads/AI_Knowledge_Base_Setup/______VFX25/`
**Extracted**: 2026-01-13
**Projects Analyzed**: 15+ Keijiro & community projects

---

## 1. Hologram Rendering Patterns

### 1.1 BodyPix Neural Segmentation (Unity Sentis)

**Source**: `BodyPixSentis-main` by Keijiro Takahashi
**Use Case**: Real-time body segmentation without LiDAR

```csharp
// Core pattern: Neural network body detection
public sealed class BodyDetector : System.IDisposable
{
    Worker _worker;  // Unity Sentis GPU worker

    public void ProcessImage(Texture sourceTexture)
    {
        // 1. Preprocess image for NN
        _preprocess.Dispatch(source, _resources.preprocess);

        // 2. Run neural network inference
        _worker.Schedule(_preprocess.Tensor);

        // 3. Postprocess - extract mask
        post1.SetBuffer(0, "Segments", _worker.PeekOutputBuffer("segments"));
        post1.SetTexture(0, "Output", _output.mask);
        post1.DispatchThreads(0, width, height, 1);

        // 4. Extract keypoints (17 body joints)
        post2.SetBuffer(0, "Keypoints", _output.keypoints);
    }

    // Outputs
    public RenderTexture MaskTexture => _output.mask;
    public GraphicsBuffer KeypointBuffer => _output.keypoints;
}
```

**Key Insight**: BodyPix runs at 512x384 input resolution for optimal performance/quality.

---

### 1.2 NNCam2 - BodyPix to VFX Pipeline

**Source**: `NNCam2-main`
**Use Case**: Camera effects with neural segmentation

```csharp
public sealed class BodyPixInput : MonoBehaviour
{
    BodyDetector _detector;

    void Start()
    {
        // Initialize detector at 512x384
        _detector = new BodyDetector(_resources, 512, 384);
    }

    void LateUpdate()
    {
        // Process frame
        _detector.ProcessImage(_source.AsTexture);

        // Output to render texture
        Graphics.SetRenderTarget(_output);
        _material.SetTexture(ShaderID.BodyPixTexture, _detector.MaskTexture);
        _material.SetPass(0);
        Graphics.DrawProceduralNow(MeshTopology.Triangles, 3, 1);
    }

    // Expose keypoints for VFX
    public GraphicsBuffer KeypointBuffer => _detector.KeypointBuffer;
}
```

**Filter Variants**: Delay, Thru, Feedback, Slitscan, Overlay

---

### 1.3 MetavidoVFX - VFX Graph Depth Binder

**Source**: `MetavidoVFX-main`
**Use Case**: Volumetric video playback via VFX Graph

```csharp
[VFXBinder("Metavido")]
public sealed class VFXMetavidoBinder : VFXBinderBase
{
    // Required VFX Graph properties
    ExposedProperty _colorMapProperty = "ColorMap";      // RGB texture
    ExposedProperty _depthMapProperty = "DepthMap";      // Depth texture
    ExposedProperty _rayParamsProperty = "RayParams";    // Camera ray parameters
    ExposedProperty _inverseViewProperty = "InverseView"; // Inverse view matrix
    ExposedProperty _depthRangeProperty = "DepthRange";  // Min/max depth

    public override void UpdateBinding(VisualEffect component)
    {
        var meta = _decoder.Metadata;
        if (!meta.IsValid) return;

        // Bind all properties
        component.SetTexture(_colorMapProperty, _demux.ColorTexture);
        component.SetTexture(_depthMapProperty, _demux.DepthTexture);
        component.SetVector4(_rayParamsProperty, RenderUtils.RayParams(meta));
        component.SetMatrix4x4(_inverseViewProperty, RenderUtils.InverseView(meta));
        component.SetVector2(_depthRangeProperty, meta.DepthRange);
    }
}
```

**VFX Graph Setup**:
1. Exposed Texture2D: `ColorMap`, `DepthMap`
2. Exposed Vector4: `RayParams`
3. Exposed Matrix4x4: `InverseView`
4. Exposed Vector2: `DepthRange`

---

### 1.4 Rcam4 - RGBD Streaming Metadata

**Source**: `Rcam4-main`
**Use Case**: LiDAR depth streaming between devices

```csharp
[StructLayout(LayoutKind.Sequential)]
public readonly struct Metadata
{
    // Camera pose (for world reconstruction)
    public readonly Vector3 CameraPosition;
    public readonly Quaternion CameraRotation;

    // Camera parameters (for unprojection)
    public readonly Matrix4x4 ProjectionMatrix;
    public readonly Vector2 DepthRange;  // (near, far)

    // Serialization for network transport
    public string Serialize()
    {
        ReadOnlySpan<Metadata> data = stackalloc Metadata[] { this };
        var bytes = MemoryMarshal.AsBytes(data).ToArray();
        return "<![CDATA[" + Convert.ToBase64String(bytes) + "]]>";
    }
}
```

**Depth Range**: Typically (0.1f, 10f) for indoor scenes

---

## 2. AR Portal Patterns

### 2.1 Stencil Buffer Portal Technique

**Source**: `ar-portal-arfoundation-master` by Tongzhou Yu
**Use Case**: Walk-through AR portals

**StencilMask.shader** - Writes to stencil buffer:
```hlsl
Shader "Custom/StencilMask"
{
    SubShader
    {
        Tags { "RenderType" = "Opaque" }
        Zwrite Off      // Don't write to depth
        ColorMask 0     // Don't write to color
        Cull off        // Render both sides

        Pass
        {
            Stencil {
                Ref 1           // Stencil reference value
                Comp always     // Always pass stencil test
                Pass replace    // Write Ref to stencil buffer
            }
            // Fragment outputs nothing (invisible mask)
        }
    }
}
```

**PortalManager.cs** - Bidirectional traversal:
```csharp
public class PortalManager : MonoBehaviour
{
    public Material[] materials;  // Inner world materials
    bool inOtherWorld;

    void SetMaterials(bool fullRender)
    {
        // Toggle stencil test direction
        var stencilTest = fullRender
            ? CompareFunction.NotEqual  // Show when NOT in portal area
            : CompareFunction.Equal;    // Show when IN portal area

        foreach (var mat in materials)
            mat.SetInt("_StencilComp", (int)stencilTest);
    }

    bool GetIsInFront()
    {
        // Calculate camera position relative to portal plane
        Vector3 worldPos = camera.position + camera.forward * nearClipPlane;
        Vector3 localPos = transform.InverseTransformPoint(worldPos);
        return localPos.y >= 0;  // Y-axis = portal normal
    }

    void whileCameraColliding()
    {
        bool isInFront = GetIsInFront();
        // Detect crossing (front-to-back or back-to-front)
        if ((isInFront && !wasInFront) || (wasInFront && !isInFront))
        {
            inOtherWorld = !inOtherWorld;
            SetMaterials(inOtherWorld);
        }
    }
}
```

**Key Components**:
1. **Portal Frame**: Mesh with StencilMask shader (invisible, writes stencil)
2. **Inner World**: Objects with `_StencilComp` material property
3. **Trigger Collider**: Detects camera traversal

---

## 3. 3D Visualization Patterns

### 3.1 Gaussian Splatting VR

**Source**: `Unity-VR-Gaussian-Splatting-main`
**Use Case**: Photorealistic 3D reconstruction in VR

**Integration Points**:
- XR Interaction Toolkit 3.0.3
- URP rendering pipeline
- Quest/PCVR compatible

### 3.2 Multi-Layer Gaussian Splatting (Anatomy)

**Source**: `Multi-Layer-Gaussian-Splatting-for-Immersive-Anatomy-Visualization-main`
**Use Case**: Medical/anatomical visualization with layers

**Key Innovation**: Separate splat layers for different anatomical structures

---

## 4. Project Index

### Hologram/Depth Projects
| Project | Purpose | Key Files |
|---------|---------|-----------|
| `BodyPixSentis-main` | Neural body segmentation | `BodyDetector.cs` |
| `NNCam2-main` | BodyPix VFX effects | `BodyPixInput.cs` |
| `MetavidoVFX-main` | Volumetric VFX playback | `VFXMetavidoBinder.cs` |
| `Rcam4-main` | LiDAR depth streaming | `Metadata.cs` |
| `Rcam3-main` | Legacy depth streaming | Multiple |
| `NNCam-main` | Original neural camera | Assets/NNCam/ |

### Portal Projects
| Project | Purpose | Key Files |
|---------|---------|-----------|
| `ar-portal-arfoundation-master` | Basic AR portal | `PortalManager.cs`, `StencilMask.shader` |
| `portals-urp-main` | URP-compatible portals | Shaders/ |

### 3D Visualization Projects
| Project | Purpose | Platform |
|---------|---------|----------|
| `Unity-VR-Gaussian-Splatting-main` | VR splat rendering | Quest/PCVR |
| `Multi-Layer-Gaussian-Splatting-*` | Anatomy viz | Desktop |
| `UnityGaussianSplatting-main` | Aras-P implementation | All |

### VFX/Effects Projects
| Project | Purpose |
|---------|---------|
| `FlashGlitch-main` | Screen glitch effects |
| `DrumPadVFX-main` | Audio-reactive VFX |
| `VFXCustomCode-main` | Custom VFX blocks |

---

## 5. Unified Hologram Pipeline (2026-01-16 Breakthrough)

**Discovery**: HologramSource was duplicating ARDepthSource compute work. Now unified.

### 5.1 Architecture

```
BEFORE (Redundant):
  ARDepthSource ‚Üí PositionMap (compute #1) ‚Üí Regular VFX
  HologramSource ‚Üí PositionMap (compute #2) ‚Üí Hologram VFX  ‚Üê DUPLICATE!

AFTER (Unified):
  ARDepthSource (singleton) ‚Üí PositionMap ‚Üí ALL VFX
                                    ‚Üì
                          VFXARBinder (per-VFX)
                                    ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ               ‚îÇ               ‚îÇ
              Regular VFX    Hologram VFX     Other VFX
                            (+AnchorPos,
                             +HologramScale)
```

### 5.2 VFXARBinder Hologram Extensions

```csharp
// MetavidoVFX-main/Assets/Scripts/Bridges/VFXARBinder.cs
[Header("Hologram (Mini-Me)")]
[SerializeField] bool _bindAnchorPos = false;
[SerializeField] bool _bindHologramScale = false;
[SerializeField] Transform _anchorTransform;
[SerializeField] float _hologramScale = 0.15f;  // 15% = mini-me
[SerializeField] bool _useTransformMode = true; // Transform VFX directly

// Two modes:
// 1. Transform Mode: Moves/scales VFX GameObject directly
// 2. Property Mode: Binds AnchorPos/HologramScale to VFX properties
```

### 5.3 HologramPlacer Touch Gestures

```csharp
// MetavidoVFX-main/Assets/Scripts/Hologram/HologramPlacer.cs
// Simple AR placement - tap to place, gestures to manipulate

| Gesture | Action |
|---------|--------|
| Tap | Place on AR plane |
| 1-finger drag | Translate X/Z |
| 2-finger drag | Translate Y (height) |
| Pinch | Scale (0.05x - 2x) |

// Key pattern: Track touchCount changes
void HandleManipulation() {
    if (Input.touchCount == 1) HandleDrag();      // XZ
    else if (Input.touchCount == 2) HandleTwoFingerGesture();  // Y + Scale
}
```

### 5.4 HologramController (Live AR / Metavido)

```csharp
// MetavidoVFX-main/Assets/Scripts/Hologram/HologramController.cs
public enum SourceMode {
    LiveAR,         // ARDepthSource (real-time)
    MetavidoVideo   // Metavido .mp4 file playback
}

// Metavido mode uses:
// - VideoPlayer ‚Üí video frames
// - TextureDemuxer ‚Üí ColorTexture, DepthTexture (from side-by-side encoding)
// - MetadataDecoder ‚Üí RayParams, InverseView, DepthRange
```

### 5.5 Prefab Structure

```
Assets/Prefabs/Hologram/Hologram.prefab
‚îú‚îÄ‚îÄ HologramPlacer       (touch gestures)
‚îú‚îÄ‚îÄ HologramController   (mode switching)
‚îî‚îÄ‚îÄ HologramVFX
    ‚îú‚îÄ‚îÄ VisualEffect     (hologram_depth_people_metavido.vfx)
    ‚îú‚îÄ‚îÄ VFXARBinder      (binds AR data + hologram transform)
    ‚îî‚îÄ‚îÄ Scale: 0.15      (mini-me default)
```

### 5.6 Key Learnings

1. **Singleton Compute**: ONE ARDepthSource serves ALL VFX - never duplicate GPU work
2. **Transform Mode**: Simpler than VFX properties for placement/scale
3. **Touch Pattern**: `touchCount` changes distinguish gesture types
4. **Metavido Integration**: TextureDemuxer extracts Color/Depth from encoded video

---

## 6. Quick Reference: VFX Graph Depth Setup

```
VFX Graph Properties Required:
‚îú‚îÄ‚îÄ ColorMap (Texture2D) - RGB camera feed
‚îú‚îÄ‚îÄ DepthMap (Texture2D) - Depth buffer (R32F or R16)
‚îú‚îÄ‚îÄ RayParams (Vector4) - (tanHalfFovX, tanHalfFovY, 0, 0)
‚îú‚îÄ‚îÄ InverseView (Matrix4x4) - Camera.cameraToWorldMatrix
‚îî‚îÄ‚îÄ DepthRange (Vector2) - (nearClip, farClip)

Unprojection Formula (in VFX):
worldPos = InverseView * (rayDir * depth)
where rayDir = normalize(uv * RayParams.xy - 0.5, 1)
```

---

## 6. Location Reference

All projects located in:
```
/Users/jamestunick/Downloads/AI_Knowledge_Base_Setup/______VFX25/
‚îú‚îÄ‚îÄ new keijiro/           # Keijiro's projects
‚îÇ   ‚îú‚îÄ‚îÄ HOLO.vfx.Demos/   # Demo collection
‚îÇ   ‚îú‚îÄ‚îÄ BodyPixSentis-main/
‚îÇ   ‚îú‚îÄ‚îÄ MetavidoVFX-main/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ RCAMS/                 # Rcam series
‚îú‚îÄ‚îÄ ar-portal-arfoundation-master/
‚îú‚îÄ‚îÄ portals-urp-main/
‚îî‚îÄ‚îÄ NNCam-main/
```

---

## üîó CROSS-REFERENCES

### Auto-Fix Patterns (106 total)
See `_AUTO_FIX_PATTERNS.md` for auto-applicable VFX fixes:
- **VFX Custom HLSL**: `void Func(inout VFXAttributes)` signature
- **SampleLevel vs Sample**: Use `SampleLevel(tex, uv, 0)` in compute
- **Global Texture Limitation**: VFX can't read Shader.SetGlobalTexture
- **VFXEventAttribute Pooling**: Cache in Start(), reuse
- **RayParams Calculation**: From projection matrix

### Quick Fixes
```bash
kbfix "VFX HLSL"       # Custom function fixes
kbfix "SampleLevel"    # Texture sampling in compute
kbfix "VFX event"      # Event attribute issues
```

### Related KB Files
- `_ARFOUNDATION_VFX_KNOWLEDGE_BASE.md` - AR + VFX integration
- `_RCAM_VFX_BINDING_SPECIFICATION.md` - Rcam depth binding
- `_LASPVFX_AUDIO_BINDING_PATTERNS.md` - Audio-reactive VFX
- `_INTELLIGENCE_SYSTEMS_INDEX.md` - Central reference

---

**Maintainer**: James Tunick
**Last Updated**: 2026-01-22
**Confidence**: 100% (extracted from actual source code)
