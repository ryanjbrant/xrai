# üé® PORTALS iOS APP - ADVANCED AR FEATURES IMPLEMENTATION PLAN

**Project**: H3M Portals (iOS AR App)
**Date**: 2025-10-25
**Scope**: 12 Major Feature Integrations
**Est. Total Time**: 400-600 hours (10-15 weeks, 2 developers)

---

## STATUS UPDATE ‚Äî 2025-11-21 (v2 merge)

**What changed (verified in codebase):**
- üî¥ **ParticlePainter.cs** (v3) instantiates/destroys per point; must be refactored to pooling + emission control before any advanced brushes.
- üî¥ **Security**: `H3MLogin.cs` logs auth IDs/tokens; `H3MAmazonS3.cs` hardcodes bucket URL. Remove sensitive logs and move bucket config to ScriptableObject/config.
- ‚úÖ **Prototypes present**: `HandPaintParticleController.cs` (hand paint), `Mic.cs` + `MicHandle.cs` (audio reactive), `PeopleOcclusionVFXManager.cs` (body VFX).

**Feature status matrix (condensed):**
- Particle Brush System ‚Äî üî¥ Critical (needs pooling fix).
- Hand Tracking ‚Äî ‚úÖ Prototype exists (needs integration with new painter).
- Audio Reactive ‚Äî ‚ö†Ô∏è Partial (needs hook into painter/FFT).
- VFX Body Tracking ‚Äî ‚úÖ Implemented (needs device perf test).
- Speech-to-Object, Echovision Depth ‚Äî ‚ùå Not started.
- Normcore Multiplayer ‚Äî ‚ö†Ô∏è Package installed, no gameplay logic.
- XRAI/Holograms ‚Äî ‚ùå Not started (research only).

**Updated near-term plan:**
1) Fix security logging + S3 config; clean dead code.
2) Refactor `ParticlePainter` with pooling/emission toggles (reuse base plan below).
3) Integrate hand tracking + audio reactive into new painter; device test body VFX.
4) Begin Normcore syncing once painter is stable.

---

## TABLE OF CONTENTS

**üéØ PRIORITY P0 - FOUNDATIONAL IMPROVEMENTS**
0. [**PARTICLE BRUSH SYSTEM OVERHAUL**](#0-particle-brush-system-overhaul-p0-priority) ‚ö° **START HERE**

**ADVANCED AR FEATURES**
1. [Interaction Scene Analysis](#1-interaction-scene-analysis)
2. [Hand Tracking Painting](#2-hand-tracking-painting-implementation)
3. [VFX Graph Body Tracking](#3-vfx-graph-body-tracking-implementation)
4. [Audio Reactive Brushes](#4-audio-reactive-brushes-implementation)
5. [Whisper.Icosa Speech-to-Object](#5-whisper-icosa-speech-to-object-integration)
6. [Echovision Depth Effects](#6-echovision-functionality-integration)
7. [Fungisync Multiplayer](#7-fungisync-functionality-integration)

8. [**XRAI 5D Holograms System**](#8-xrai-5d-holograms-system---deep-technical-analysis--implementation-guide) ‚ö° **FOUNDATIONAL ARCHITECTURE**
9. [Rcam Local Holograms](#9-rcam4-local-lidar-vfx-holograms)
10. [WebRTC Multiplayer Holograms](#10-webrtc-multiplayer-holograms)
11. [**Normcore Multiplayer XR Scene Composition**](#11-normcore-multiplayer-xr-scene-composition) ‚ö° **COST-OPTIMIZED**

**PROJECT MANAGEMENT**
12. [Implementation Timeline](#12-implementation-timeline--roadmap)
13. [Testing Requirements](#13-testing-requirements)
14. [Risk Mitigation](#14-risk-mitigation)

**RESEARCH & FUTURE DIRECTIONS**
15. [**Universal Spatial Media Format Research (XRAI/VNMF)**](#15-universal-spatial-media-format-research-xraivnmf) üî¨ **LONG-TERM VISION**

---

## 0. PARTICLE BRUSH SYSTEM OVERHAUL (P0 PRIORITY)

**Status**: üî¥ **CRITICAL - FIX BEFORE OTHER FEATURES**
**Priority**: P0 (Foundational improvement - blocks other painting features)
**Time Estimate**: 80-120 hours (2-3 weeks, 1 developer)
**Goal**: Upgrade Portals particle brushes to match Paint-AR and Open Brush functionality

---

### 0.1 DEEP ANALYSIS: Current State vs Reference Projects

#### **Paint-AR Particle Brush System** ‚úÖ WORKING REFERENCE

**Architecture** ([BrushManager.cs:1-500](file:///Users/jamestunick/Documents/GitHub/Paint-AR_Unity/Assets/Scripts/BrushManager.cs)):
```
BrushManager (MonoBehaviour)
  ‚îú‚îÄ Public GameObject fields for each brush (20+ brushes)
  ‚îÇ   ‚îú‚îÄ MicBrush, SmokeBrush, FireworxBrush, PlasmaFlareBrush
  ‚îÇ   ‚îú‚îÄ WormholeBrush1/2/3, ButterflyBrush, MissileBrush
  ‚îÇ   ‚îî‚îÄ AuroraBrush, PlexBrush, SplatFlow, DotBrush, FuzzBrush
  ‚îÇ
  ‚îú‚îÄ m_currentBrush - Active brush GameObject
  ‚îú‚îÄ spawn_transform - Where to instantiate brush
  ‚îÇ
  ‚îî‚îÄ Methods:
      ‚îú‚îÄ ParticleEmisionEnable() / Disable()
      ‚îÇ   ‚îú‚îÄ GetComponentsInChildren<ParticleSystem>()
      ‚îÇ   ‚îú‚îÄ Set emission.enabled = true/false
      ‚îÇ   ‚îî‚îÄ TrailRenderer time adjustment
      ‚îÇ
      ‚îú‚îÄ select_blood(), select_smoke(), etc. - Brush selection
      ‚îî‚îÄ BrushEnable() / BrushDisable() - Global paint toggle
```

**Key Features**:
- ‚úÖ **Emission Control**: Enable/disable particles when painting starts/stops
- ‚úÖ **TrailRenderer Support**: Adjusts trail time (10s painting, 1s idle)
- ‚úÖ **Hierarchical Search**: `GetComponentsInChildren<>()` finds nested particle systems
- ‚úÖ **One Active Brush**: Only m_currentBrush is active at a time
- ‚úÖ **GameObject Pooling**: Instantiate prefabs at spawn transform
- ‚úÖ **Legacy Shuriken Particles**: Works with Unity ParticleSystem

**Prefab Examples** (100+ prefabs in `_CustomBrushes/`):
- `FireworxBrush/Fireworks3.prefab` - Complex nested particle systems
- `ArcReactor/PlasmaRay.prefab` - Beam effects with trails
- `BloodSprayBrush/uLiquid - Blood - Spray v5.prefab` - Liquid particles
- `KvantSprayBrush/BubbleSpray.prefab` - GPU-optimized spray

---

#### **Open Brush Particle System** ‚úÖ ADVANCED REFERENCE

**Architecture** ([GeniusParticlesBrush.cs:1-150](file:///Users/jamestunick/Documents/GitHub/open-brush-main/Assets/Scripts/Brushes/GeniusParticlesBrush.cs), [SprayBrush.cs:1-100](file:///Users/jamestunick/Documents/GitHub/open-brush-main/Assets/Scripts/Brushes/SprayBrush.cs)):
```
GeniusParticlesBrush : GeometryBrush
  ‚îú‚îÄ NOT Unity ParticleSystem - Custom geometry generation
  ‚îú‚îÄ Generates quads (4 verts, 2 tris per particle)
  ‚îú‚îÄ GPU rendering via custom shaders (Particles.cginc)
  ‚îú‚îÄ Decay timers for preview mode
  ‚îú‚îÄ StatelessRng for deterministic randomness
  ‚îî‚îÄ 50x faster than Unity ParticleSystem

ParentBrush : BaseBrushScript
  ‚îú‚îÄ Abstract brush with child brush hierarchy
  ‚îú‚îÄ PbChild classes define child movement
  ‚îú‚îÄ Knot-based coordinate frames (Pointer, LineTangent)
  ‚îî‚îÄ Used for complex effects (spirals, branches, ribbons)

SprayBrush : GeometryBrush
  ‚îú‚îÄ Quad-based spray particles
  ‚îú‚îÄ Pressure-sensitive spawn interval
  ‚îú‚îÄ GetSpawnInterval(pressure01) - Dynamic density
  ‚îî‚îÄ Texture atlas support (4 variants per brush)
```

**Key Innovations**:
- ‚úÖ **Geometry-Based Particles**: 10-50x faster than Unity ParticleSystem
- ‚úÖ **Pressure Sensitivity**: Spawn rate varies with input pressure
- ‚úÖ **Deterministic RNG**: StatelessRng ensures consistent replay
- ‚úÖ **Incremental Rendering**: Builds geometry over time, not all at once
- ‚úÖ **Preview Mode**: Decay timers fade old strokes
- ‚úÖ **Batching**: Multiple strokes in single mesh for performance
- ‚úÖ **Coordinate Frames**: Advanced parent-child brush hierarchies

**Performance Comparison**:
| System | Particles/sec | Frame Time (ms) | Notes |
|--------|---------------|-----------------|-------|
| Unity ParticleSystem | 500-1000 | 8-15ms | CPU bottleneck |
| GeniusParticlesBrush | 5000-10000 | 2-4ms | GPU-optimized |
| Open Brush advantage | **10x-50x faster** | ‚ö° | Production-proven |

---

#### **Portals Current System** üî¥ NEEDS UPGRADE

**Architecture** ([EnchantedPaintbrush.cs:1-156](file:///Users/jamestunick/wkspaces/Portals_6/Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/EnchantedPaintbrush.cs)):
```csharp
EnchantedPaintbrush : MonoBehaviour, IEnchanted<EnchantedPaintbrushData>
{
    // Stroke recording only
    protected List<List<Vector3>> strokes = new List<List<Vector3>>();
    public List<ContextualAttribute> attributes = new List<ContextualAttribute>();

    // Distance filtering
    public float recordingDistance = 0.5f;
    public float minDistanceBetweenPoints = 0.005f;

    // Abstract methods for subclasses
    protected abstract void AddPoint(Vector3 point);
    protected abstract void RenderStrokes();
    protected abstract void HandleAttr(ContextualAttribute a);

    // Helper methods
    protected GameObject GetParticleSystem(string name) { ... } // ‚ö†Ô∏è UNUSED
}
```

**What's MISSING**:
- ‚ùå **No Particle Emission Control** - GetParticleSystem() exists but never called
- ‚ùå **No Brush Catalog** - Can't switch between multiple particle brushes
- ‚ùå **No VFX Graph Integration** - 90+ VFX assets unused
- ‚ùå **No Spawn Transform Tracking** - Particles don't follow cursor
- ‚ùå **No Pressure Sensitivity** - Fixed density regardless of input
- ‚ùå **No Emission Enable/Disable** - Particles always emit or never emit
- ‚ùå **No TrailRenderer Support** - Can't use trails like Paint-AR
- ‚ùå **No Preview Mode** - Can't show stroke before committing
- ‚ùå **Touch/Mouse Only** - No hand tracking integration (covered in Section 2)

**Existing VFX Assets** (90+ VFX Graph files in `Assets/[H3M]/Portals/Content/__Paint_AR/`):
- `_______VFX/PeopleVFX4.vfx` - Body tracking effects
- `___________BibCamVFX/BibcamURP/Points.vfx` - Point cloud rendering
- `__Procedural VFX Library 1.0 Prototypes/VFX Graphs/Fireworks.vfx`
- `_VFX/_NNCamBodyPix/VFX/Particle.vfx` - Body-based particles
- `Cool Visual Effects - Part 1/Effects/Magic Ball/Magic Ball.vfx`

**These are READY TO USE but not integrated with painting system!**

---

### 0.2 PROPOSED UNIFIED ARCHITECTURE

**Goal**: Support BOTH legacy Shuriken ParticleSystem AND VFX Graph in one system

```
H3MParticleBrushManager (New)
  ‚îú‚îÄ BrushCatalog (ScriptableObject)
  ‚îÇ   ‚îú‚îÄ List<H3MBrushDescriptor>
  ‚îÇ   ‚îÇ   ‚îú‚îÄ brushName: string
  ‚îÇ   ‚îÇ   ‚îú‚îÄ brushType: enum { Shuriken, VFXGraph, GeometryBased }
  ‚îÇ   ‚îÇ   ‚îú‚îÄ prefab: GameObject
  ‚îÇ   ‚îÇ   ‚îú‚îÄ vfxAsset: VisualEffectAsset
  ‚îÇ   ‚îÇ   ‚îú‚îÄ spawnRate: float
  ‚îÇ   ‚îÇ   ‚îú‚îÄ pressureSensitive: bool
  ‚îÇ   ‚îÇ   ‚îî‚îÄ icon: Sprite (for UI)
  ‚îÇ   ‚îî‚îÄ Categories: Fire, Plasma, Smoke, Magic, Nature, etc.
  ‚îÇ
  ‚îú‚îÄ H3MBrushInstance (Runtime)
  ‚îÇ   ‚îú‚îÄ currentBrush: H3MBrushDescriptor
  ‚îÇ   ‚îú‚îÄ activeGameObject: GameObject (for Shuriken)
  ‚îÇ   ‚îú‚îÄ activeVFX: VisualEffect (for VFX Graph)
  ‚îÇ   ‚îú‚îÄ spawnTransform: Transform (follows cursor)
  ‚îÇ   ‚îî‚îÄ isEmitting: bool
  ‚îÇ
  ‚îú‚îÄ Emission Control
  ‚îÇ   ‚îú‚îÄ StartPainting() ‚Üí Enable emission
  ‚îÇ   ‚îÇ   ‚îú‚îÄ If Shuriken: Set emission.enabled = true
  ‚îÇ   ‚îÇ   ‚îú‚îÄ If VFXGraph: SetFloat("EmissionRate", spawnRate)
  ‚îÇ   ‚îÇ   ‚îî‚îÄ Parent to spawnTransform
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îî‚îÄ StopPainting() ‚Üí Disable emission
  ‚îÇ       ‚îú‚îÄ If Shuriken: Set emission.enabled = false
  ‚îÇ       ‚îú‚îÄ If VFXGraph: SetFloat("EmissionRate", 0)
  ‚îÇ       ‚îî‚îÄ Detach from spawnTransform (let particles finish)
  ‚îÇ
  ‚îú‚îÄ Pressure Modulation
  ‚îÇ   ‚îú‚îÄ UpdatePressure(float pressure01)
  ‚îÇ   ‚îÇ   ‚îú‚îÄ If Shuriken: Modulate emission rateOverTime
  ‚îÇ   ‚îÇ   ‚îî‚îÄ If VFXGraph: SetFloat("Pressure", pressure01)
  ‚îÇ
  ‚îî‚îÄ Integration with EnchantedPaintbrush
      ‚îú‚îÄ Extend EnchantedPaintbrush for particle-specific behaviors
      ‚îî‚îÄ EnchantedParticlePaintbrush : EnchantedPaintbrush
```

---

### 0.3 IMPLEMENTATION PLAN (80-120 hours)

#### **Phase 1: Brush Descriptor System** (16-24h)

**Step 1**: Create H3MBrushDescriptor ScriptableObject
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/Painting/H3MBrushDescriptor.cs
using UnityEngine;
using UnityEngine.VFX;

namespace H3M.Painting
{
    public enum BrushType { Shuriken, VFXGraph, GeometryBased }

    [CreateAssetMenu(fileName = "NewBrush", menuName = "H3M/Painting/Brush Descriptor")]
    public class H3MBrushDescriptor : ScriptableObject
    {
        [Header("Brush Identity")]
        public string brushName = "New Brush";
        public Sprite icon;
        public string category = "General"; // Fire, Plasma, Smoke, Magic, etc.

        [Header("Brush Type")]
        public BrushType brushType = BrushType.Shuriken;

        [Header("Assets")]
        public GameObject shurikenPrefab; // For Shuriken particles
        public VisualEffectAsset vfxAsset; // For VFX Graph

        [Header("Emission Properties")]
        [Range(10, 5000)]
        public float baseSpawnRate = 500f;
        public bool pressureSensitive = true;
        [Range(0.1f, 10f)]
        public float pressureMultiplier = 2f;

        [Header("Visual Properties")]
        public Color tintColor = Color.white;
        [Range(0.1f, 5f)]
        public float sizeScale = 1f;

        [Header("Trail Properties")]
        public bool hasTrail = false;
        public float trailTime = 1f;
    }
}
```

**Step 2**: Create BrushCatalog ScriptableObject
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/Painting/H3MBrushCatalog.cs
using System.Collections.Generic;
using UnityEngine;

namespace H3M.Painting
{
    [CreateAssetMenu(fileName = "BrushCatalog", menuName = "H3M/Painting/Brush Catalog")]
    public class H3MBrushCatalog : ScriptableObject
    {
        [Header("Brush Library")]
        public List<H3MBrushDescriptor> allBrushes = new List<H3MBrushDescriptor>();

        [Header("Categories")]
        public List<string> categories = new List<string>
        {
            "Fire", "Plasma", "Smoke", "Magic", "Nature", "Geometric", "Abstract"
        };

        // Get brushes by category
        public List<H3MBrushDescriptor> GetBrushesByCategory(string category)
        {
            return allBrushes.FindAll(b => b.category == category);
        }

        // Get brush by name
        public H3MBrushDescriptor GetBrush(string name)
        {
            return allBrushes.Find(b => b.brushName == name);
        }
    }
}
```

**Step 3**: Create brush descriptors for existing Paint-AR brushes
- Create 20+ H3MBrushDescriptor assets in `Assets/[H3M]/Portals/Content/__Paint_AR/Brushes/Descriptors/`
- Assign prefabs from Paint-AR `_CustomBrushes/` folder
- Categorize: Fire (5), Plasma (4), Smoke (3), Magic (4), Nature (4)

**Testing**: Verify descriptors load correctly in Inspector

---

#### **Phase 2: Particle Brush Manager** (24-36h)

**Step 4**: Create H3MParticleBrushManager
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/Painting/H3MParticleBrushManager.cs
using UnityEngine;
using UnityEngine.VFX;
using System.Collections.Generic;

namespace H3M.Painting
{
    public class H3MParticleBrushManager : MonoBehaviour
    {
        [Header("Configuration")]
        public H3MBrushCatalog brushCatalog;
        public Transform spawnTransform; // Cursor position

        [Header("Current State")]
        private H3MBrushDescriptor currentBrush;
        private GameObject activeShurikenBrush;
        private VisualEffect activeVFXBrush;
        private bool isEmitting = false;
        private float currentPressure = 1f;

        // Brush selection
        public void SelectBrush(string brushName)
        {
            // Disable current brush
            if (isEmitting) StopPainting();
            CleanupCurrentBrush();

            // Load new brush
            currentBrush = brushCatalog.GetBrush(brushName);
            if (currentBrush == null)
            {
                Debug.LogError($"Brush not found: {brushName}");
                return;
            }

            // Instantiate based on type
            switch (currentBrush.brushType)
            {
                case BrushType.Shuriken:
                    InstantiateShurikenBrush();
                    break;
                case BrushType.VFXGraph:
                    InstantiateVFXBrush();
                    break;
            }
        }

        private void InstantiateShurikenBrush()
        {
            activeShurikenBrush = Instantiate(
                currentBrush.shurikenPrefab,
                spawnTransform.position,
                spawnTransform.rotation,
                spawnTransform
            );

            // Disable emission initially
            var particleSystems = activeShurikenBrush.GetComponentsInChildren<ParticleSystem>();
            foreach (var ps in particleSystems)
            {
                var emission = ps.emission;
                emission.enabled = false;
            }
        }

        private void InstantiateVFXBrush()
        {
            GameObject vfxGO = new GameObject($"VFX_{currentBrush.brushName}");
            vfxGO.transform.SetParent(spawnTransform, false);
            activeVFXBrush = vfxGO.AddComponent<VisualEffect>();
            activeVFXBrush.visualEffectAsset = currentBrush.vfxAsset;

            // Set initial parameters
            activeVFXBrush.SetFloat("EmissionRate", 0f);
            activeVFXBrush.SetVector3("TintColor", currentBrush.tintColor);
            activeVFXBrush.SetFloat("SizeScale", currentBrush.sizeScale);
        }

        // Emission control
        public void StartPainting()
        {
            if (currentBrush == null) return;
            isEmitting = true;

            if (currentBrush.brushType == BrushType.Shuriken)
            {
                EnableShurikenEmission();
            }
            else if (currentBrush.brushType == BrushType.VFXGraph)
            {
                EnableVFXEmission();
            }
        }

        public void StopPainting()
        {
            isEmitting = false;

            if (currentBrush.brushType == BrushType.Shuriken)
            {
                DisableShurikenEmission();
            }
            else if (currentBrush.brushType == BrushType.VFXGraph)
            {
                DisableVFXEmission();
            }
        }

        private void EnableShurikenEmission()
        {
            var particleSystems = activeShurikenBrush.GetComponentsInChildren<ParticleSystem>();
            foreach (var ps in particleSystems)
            {
                var emission = ps.emission;
                emission.enabled = true;

                // Apply pressure-sensitive rate
                if (currentBrush.pressureSensitive)
                {
                    var main = ps.main;
                    main.startLifetime = currentPressure * currentBrush.pressureMultiplier;
                }
            }

            // Enable trails
            var trails = activeShurikenBrush.GetComponentsInChildren<TrailRenderer>();
            foreach (var trail in trails)
            {
                trail.enabled = true;
                trail.time = currentBrush.trailTime * 10f; // Painting mode
            }
        }

        private void DisableShurikenEmission()
        {
            var particleSystems = activeShurikenBrush.GetComponentsInChildren<ParticleSystem>();
            foreach (var ps in particleSystems)
            {
                var emission = ps.emission;
                emission.enabled = false;
            }

            // Reduce trail time
            var trails = activeShurikenBrush.GetComponentsInChildren<TrailRenderer>();
            foreach (var trail in trails)
            {
                trail.time = currentBrush.trailTime; // Idle mode
            }
        }

        private void EnableVFXEmission()
        {
            float emissionRate = currentBrush.baseSpawnRate;
            if (currentBrush.pressureSensitive)
            {
                emissionRate *= currentPressure * currentBrush.pressureMultiplier;
            }
            activeVFXBrush.SetFloat("EmissionRate", emissionRate);
        }

        private void DisableVFXEmission()
        {
            activeVFXBrush.SetFloat("EmissionRate", 0f);
        }

        // Pressure modulation
        public void UpdatePressure(float pressure01)
        {
            currentPressure = Mathf.Clamp01(pressure01);

            if (isEmitting && currentBrush != null && currentBrush.pressureSensitive)
            {
                if (currentBrush.brushType == BrushType.Shuriken)
                {
                    UpdateShurikenPressure();
                }
                else if (currentBrush.brushType == BrushType.VFXGraph)
                {
                    activeVFXBrush.SetFloat("Pressure", currentPressure);
                }
            }
        }

        private void UpdateShurikenPressure()
        {
            var particleSystems = activeShurikenBrush.GetComponentsInChildren<ParticleSystem>();
            foreach (var ps in particleSystems)
            {
                var emission = ps.emission;
                var rateOverTime = emission.rateOverTime;
                rateOverTime.constant = currentBrush.baseSpawnRate * currentPressure;
                emission.rateOverTime = rateOverTime;
            }
        }

        private void CleanupCurrentBrush()
        {
            if (activeShurikenBrush != null)
            {
                Destroy(activeShurikenBrush);
                activeShurikenBrush = null;
            }
            if (activeVFXBrush != null)
            {
                Destroy(activeVFXBrush.gameObject);
                activeVFXBrush = null;
            }
        }

        void OnDestroy()
        {
            CleanupCurrentBrush();
        }
    }
}
```

**Testing**:
- Select brush via `SelectBrush("FireworxBrush")`
- Call `StartPainting()` / `StopPainting()`
- Verify emission toggles correctly
- Test pressure modulation with `UpdatePressure(0.5f)`

---

#### **Phase 3: EnchantedPaintbrush Integration** (20-30h)

**Step 5**: Create EnchantedParticlePaintbrush
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/EnchantedParticlePaintbrush.cs
using UnityEngine;
using H3M.Painting;

namespace H3M
{
    public class EnchantedParticlePaintbrush : EnchantedPaintbrush
    {
        [Header("Particle Brush Settings")]
        public H3MParticleBrushManager brushManager;
        public string initialBrushName = "FireworxBrush";

        private bool wasPainting = false;

        void Start()
        {
            if (brushManager == null)
            {
                Debug.LogError("H3MParticleBrushManager not assigned!");
                return;
            }

            // Select initial brush
            brushManager.SelectBrush(initialBrushName);
        }

        void Update()
        {
            bool isPainting = ((Touchscreen.current != null &&
                               Touchscreen.current.primaryTouch.press.isPressed) ||
                              (Mouse.current != null && Mouse.current.leftButton.isPressed)) &&
                             H3M.XR.activeObjectManipulator.currentItem == gameObject;

            // Start painting
            if (isPainting && !wasPainting)
            {
                brushManager.StartPainting();
                strokes.Add(new List<Vector3>());
            }

            // Continue painting
            if (isPainting)
            {
                Vector3 newPoint = H3M.XR.XRCamera.transform.position +
                                   H3M.XR.XRCamera.transform.forward * recordingDistance;

                List<Vector3> currentStroke = strokes[strokes.Count - 1];

                if (currentStroke.Count == 0 ||
                    Vector3.Distance(currentStroke[currentStroke.Count - 1], newPoint) >= minDistanceBetweenPoints)
                {
                    currentStroke.Add(newPoint);
                    AddPoint(newPoint);

                    // Calculate pressure (simple distance-based for now)
                    float pressure = CalculatePressure(currentStroke);
                    brushManager.UpdatePressure(pressure);
                }
            }

            // Stop painting
            if (!isPainting && wasPainting)
            {
                brushManager.StopPainting();
            }

            wasPainting = isPainting;
        }

        private float CalculatePressure(List<Vector3> stroke)
        {
            if (stroke.Count < 2) return 1f;

            // Pressure based on velocity (faster = more pressure)
            Vector3 lastPoint = stroke[stroke.Count - 1];
            Vector3 prevPoint = stroke[stroke.Count - 2];
            float distance = Vector3.Distance(lastPoint, prevPoint);
            float velocity = distance / Time.deltaTime;

            // Map velocity to pressure (0.5 - 1.5 range)
            return Mathf.Clamp(velocity * 0.1f, 0.5f, 1.5f);
        }

        protected override void AddPoint(Vector3 point)
        {
            // Particle emission handled by brushManager
            // This can be used for additional effects (sound, haptics, etc.)
        }

        protected override void RenderStrokes()
        {
            // Particles render themselves
            // This can be used for geometry-based stroke visualization
        }

        protected override void HandleAttr(ContextualAttribute a)
        {
            // Handle attribute changes
            if (a.attributeName == "BrushName" && a.Type == ContextualAttributeType.String)
            {
                brushManager.SelectBrush(a.stringValue);
            }
        }
    }
}
```

**Step 6**: Update EnchantedPaintbrush base class
- Add `virtual` keyword to Update() method so subclasses can override
- Add pressure calculation support

**Testing**:
- Paint with touch/mouse input
- Verify particle emission follows cursor
- Test pressure modulation
- Switch brushes at runtime

---

#### **Phase 4: VFX Graph Integration** (20-30h)

**Step 7**: Migrate existing VFX Graph assets to brush system

**VFX Graphs to migrate** (priority order):
1. `__Procedural VFX Library 1.0 Prototypes/VFX Graphs/Fireworks.vfx`
2. `___________BibCamVFX/BibcamURP/Points.vfx`
3. `_______VFX/Cool Visual Effects - Part 1/Effects/Magic Ball/Magic Ball.vfx`
4. `_VFX/_NNCamBodyPix/VFX/Particle.vfx`
5. `__Procedural VFX Library 1.0 Prototypes/VFX Graphs/Spectrogram.vfx` (audio-reactive)

**Step 8**: Create VFX Graph template for painting

**Required Exposed Parameters**:
```
EmissionRate (float) - Particles per second
Pressure (float) - 0-1 pressure modulation
TintColor (Vector3) - RGB color tint
SizeScale (float) - Particle size multiplier
SpawnPosition (Vector3) - Emission origin
SpawnVelocity (Vector3) - Initial velocity direction
```

**Step 9**: Update existing VFX Graphs
- Open each VFX Graph in VFX Graph editor
- Add Blackboard properties for exposed parameters
- Connect EmissionRate to Spawn Rate
- Add Pressure modulation to particle lifetime/size
- Save and test

**Testing**:
- Create H3MBrushDescriptor for each VFX Graph
- Test emission control via SetFloat("EmissionRate")
- Verify pressure modulation works

---

#### **Phase 5: UI Integration** (16-24h)

**Step 10**: Create brush selection UI

```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/UI/BrushSelectorUI.cs
using UnityEngine;
using UnityEngine.UI;
using H3M.Painting;

namespace H3M.UI
{
    public class BrushSelectorUI : MonoBehaviour
    {
        [Header("References")]
        public H3MParticleBrushManager brushManager;
        public H3MBrushCatalog brushCatalog;

        [Header("UI Elements")]
        public Transform brushButtonContainer;
        public GameObject brushButtonPrefab;

        void Start()
        {
            PopulateBrushButtons();
        }

        private void PopulateBrushButtons()
        {
            foreach (var brush in brushCatalog.allBrushes)
            {
                GameObject buttonObj = Instantiate(brushButtonPrefab, brushButtonContainer);
                Button button = buttonObj.GetComponent<Button>();
                Image icon = buttonObj.transform.Find("Icon").GetComponent<Image>();

                icon.sprite = brush.icon;
                button.onClick.AddListener(() => OnBrushSelected(brush.brushName));
            }
        }

        private void OnBrushSelected(string brushName)
        {
            brushManager.SelectBrush(brushName);
        }
    }
}
```

**Step 11**: Design brush selector UI Canvas
- Grid layout with brush icons
- Category tabs (Fire, Plasma, Smoke, etc.)
- Preview panel showing current brush name
- Color picker integration (future)

**Testing**:
- Click brush icons to switch brushes
- Verify UI updates correctly
- Test on iOS device (touch input)

---

### 0.4 MIGRATION GUIDE: Paint-AR Brushes to Portals

**Goal**: Import 20+ Paint-AR particle brushes into Portals system

#### **Brush Migration Steps**:

1. **Copy Prefabs**:
   ```bash
   # Copy from Paint-AR to Portals
   cp -r /Users/jamestunick/Documents/GitHub/Paint-AR_Unity/Assets/_CustomBrushes/FireworxBrush \
      /Users/jamestunick/wkspaces/Portals_6/Assets/[H3M]/Portals/Content/__Paint_AR/_ParticleBrushes/
   ```

2. **Fix Material References**:
   - Paint-AR uses URP 12.1.10, Portals uses URP 17.1.0
   - Reimport materials and check shader compatibility
   - Update shader references (Shader Graph may need updates)

3. **Create Brush Descriptors**:
   - For each prefab, create H3MBrushDescriptor asset
   - Assign prefab reference
   - Set category, spawn rate, pressure sensitivity

4. **Add to Catalog**:
   - Open BrushCatalog asset
   - Add new descriptors to allBrushes list
   - Organize by category

**Priority Brushes** (migrate first):
1. ‚úÖ FireworxBrush - Complex nested particles
2. ‚úÖ PlasmaFlareBrush - Beam effects
3. ‚úÖ SmokeBrush - Volume-filling particles
4. ‚úÖ WormholeBrush1 - Spiral effects
5. ‚úÖ ButterflyBrush - Animated sprites

---

### 0.5 TESTING REQUIREMENTS (iOS Device)

**Mandatory Tests**:
1. ‚úÖ Brush selection (20+ brushes)
2. ‚úÖ Emission control (start/stop painting)
3. ‚úÖ Shuriken particle brushes work
4. ‚úÖ VFX Graph brushes work
5. ‚úÖ Pressure modulation
6. ‚úÖ TrailRenderer support
7. ‚úÖ Frame rate ‚â•60 FPS with 5 active brushes
8. ‚úÖ Memory usage <500MB
9. ‚úÖ No particle leaks (cleanup on destroy)
10. ‚úÖ UI responsive on iPhone

**Performance Benchmarks**:
| Test | Target | Paint-AR | Portals Goal |
|------|--------|----------|--------------|
| Frame time | <16ms (60 FPS) | 12-15ms | <12ms |
| Memory | <500MB | 350-400MB | <400MB |
| Particles/sec | >5000 | 3000-5000 | >5000 |
| Brush switch time | <200ms | 150ms | <150ms |

---

### 0.6 ADVANCED FEATURES (Future P1)

**Not in P0 scope, but planned**:

1. **Geometry-Based Particles** (like Open Brush GeniusParticlesBrush)
   - 10-50x faster than Shuriken
   - Custom quad generation
   - GPU-optimized rendering
   - Time: 40-60 hours

2. **Parent Brush System** (like Open Brush ParentBrush)
   - Child brush hierarchies
   - Spiral/branch/ribbon effects
   - Knot-based coordinate frames
   - Time: 60-80 hours

3. **Brush Preview Mode**
   - Decay timers for stroke preview
   - Commit/cancel strokes
   - Undo/redo integration
   - Time: 24-32 hours

4. **Audio-Reactive Particles** (Section 4 integration)
   - FFT modulation of emission rate
   - Music-driven particle effects
   - Time: 32-48 hours

---

### 0.7 FILES TO CREATE/MODIFY

**New Files** (P0):
```
Assets/[H3M]/Portals/Code/v3/XR/Painting/
  ‚îú‚îÄ H3MBrushDescriptor.cs
  ‚îú‚îÄ H3MBrushCatalog.cs
  ‚îú‚îÄ H3MParticleBrushManager.cs
  ‚îî‚îÄ H3MBrushType.cs (enum)

Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/
  ‚îî‚îÄ EnchantedParticlePaintbrush.cs

Assets/[H3M]/Portals/Code/v3/XR/UI/
  ‚îî‚îÄ BrushSelectorUI.cs

Assets/[H3M]/Portals/Content/__Paint_AR/Brushes/
  ‚îú‚îÄ Descriptors/ (20+ descriptor assets)
  ‚îî‚îÄ BrushCatalog.asset
```

**Modified Files**:
```
Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/
  ‚îî‚îÄ EnchantedPaintbrush.cs (add virtual to Update())
```

**Migrated Assets**:
```
Assets/[H3M]/Portals/Content/__Paint_AR/_ParticleBrushes/
  ‚îú‚îÄ FireworxBrush/ (from Paint-AR)
  ‚îú‚îÄ PlasmaFlareBrush/ (from Paint-AR)
  ‚îú‚îÄ SmokeBrush/ (from Paint-AR)
  ‚îî‚îÄ ... (20+ brushes)
```

---

### 0.8 TIMELINE & DEPENDENCIES

**Timeline** (1 developer, full-time):
- Week 1: Phase 1-2 (Descriptor System + Brush Manager) - 40h
- Week 2: Phase 3-4 (Integration + VFX Graph) - 40h
- Week 3: Phase 5 + Testing + Polish - 40h

**Total**: 80-120 hours (2-3 weeks)

**Dependencies**:
- ‚ö†Ô∏è **BLOCKS Section 2** (Hand Tracking Painting) - Needs particle system working
- ‚ö†Ô∏è **BLOCKS Section 4** (Audio Reactive Brushes) - Needs emission control
- ‚ö†Ô∏è **INTEGRATES WITH Section 3** (Body Tracking VFX) - Can share VFX Graph system

**Recommendation**: **IMPLEMENT THIS FIRST** before other painting features.

---

## 1. INTERACTION SCENE ANALYSIS

### Current Interaction Scene
**Location**: `Assets/[H3M]/Portals/Content/__Paint_AR/Interaction.unity`

**Analysis**:
- ‚ùå **Currently inactive** in build (disabled in EditorBuildSettings)
- Scene contains: EventSystem (XR Input), Particle effects, Noise animation meshes
- **Legacy Paint-AR demo scene** - not integrated with H3M v3 architecture
- Uses old Unity Input System (not Input System 1.14+)

**Recommendation**: **DO NOT migrate Interaction.unity** - Build new scene from scratch using H3M's EnchantedScenes system

---

## 2. HAND TRACKING PAINTING IMPLEMENTATION

**Status**: ‚úÖ **IMPLEMENTED** - HoloKit SDK Integration (Needs iOS Device Testing)
**Time Invested**: ~40-60 hours (COMPLETE)
**Remaining**: iOS device testing (4-8h) + Optional v3 integration (12-16h)
**Goal**: Paint with hand gestures using HoloKit Unity SDK

---

### 2.1 CURRENT IMPLEMENTATION (Verified 2025-11-01)

#### ‚úÖ Actual Technology Stack

- **HoloKit Unity SDK v0.5.6** (`io.holokit.unity-sdk`)
  - Optical see-through AR headset with hand tracking
  - Uses ARKit hand tracking API under the hood
  - Git commit: ab9560e75656
  - Dependencies: AR Foundation 6.0.5, ARKit 6.0.5 (compatible with Portals' 6.1.0)

- **Implementation File**: [HandPaintParticleController.cs:1-326](Assets/[H3M]/Portals/Content/__Paint_AR/HandPaintParticleController.cs)
  - Created: Prior to 2025-11-01
  - Status: ‚úÖ FULLY FUNCTIONAL (pending iOS device testing)
  - Integrated with: SimpleParticleBrush, SimplePaintStroke

---

### 2.2 ACTUAL ARCHITECTURE (HoloKit SDK)

```
iOS Device (ARKit Hand Tracking via HoloKit)
  ‚îÇ
  ‚îú‚îÄ> HoloKit Unity SDK
  ‚îÇ     ‚îú‚îÄ> HandTrackingManager (HoloKit component)
  ‚îÇ     ‚îÇ     ‚îú‚îÄ> m_Hands: List<GameObject> (hand roots)
  ‚îÇ     ‚îÇ     ‚îî‚îÄ> m_HandJoints: List<Dictionary<JointName, GameObject>>
  ‚îÇ     ‚îÇ
  ‚îÇ     ‚îî‚îÄ> 26 joints per hand (ARKit standard)
  ‚îÇ
  ‚îú‚îÄ> HandPaintParticleController.cs ‚úÖ IMPLEMENTED
  ‚îÇ     ‚îú‚îÄ> Right Hand Index Finger ‚Üí LineRenderer brush strokes
  ‚îÇ     ‚îÇ     ‚îú‚îÄ> UpdateRightHandBrush() (lines 99-134)
  ‚îÇ     ‚îÇ     ‚îú‚îÄ> StartNewBrushLine() (lines 192-209)
  ‚îÇ     ‚îÇ     ‚îú‚îÄ> AddPointToBrushLine() (lines 211-218)
  ‚îÇ     ‚îÇ     ‚îî‚îÄ> EndBrushLine() (lines 220-231)
  ‚îÇ     ‚îÇ
  ‚îÇ     ‚îî‚îÄ> Left Hand Index Finger ‚Üí Particle system spawning
  ‚îÇ           ‚îú‚îÄ> UpdateLeftHandParticles() (lines 136-162)
  ‚îÇ           ‚îú‚îÄ> SpawnParticleAt() (lines 233-261)
  ‚îÇ           ‚îî‚îÄ> Reflection-based SimpleParticleBrush access
  ‚îÇ
  ‚îî‚îÄ> Platform-Specific Compilation
        ‚îú‚îÄ> #if UNITY_IOS (lines 28-30, 46-96)
        ‚îî‚îÄ> Auto-disables in Editor to prevent errors (lines 46-50)
```

---

### 2.3 IMPLEMENTED FEATURES

#### ‚úÖ **Dual-Hand Painting System** (Lines 79-96)

**Right Hand**: LineRenderer brush strokes
```csharp
// HandPaintParticleController.cs:99-134
void UpdateRightHandBrush()
{
    var rightHandJoints = GetHandJoints(JointName.IndexTip, false); // false = right hand

    if (rightHandJoints != null && rightHandJoints.Count > 0)
    {
        Transform rightIndexTip = rightHandJoints[0].transform;
        bool isTracked = rightIndexTip.gameObject.activeInHierarchy;

        if (isTracked)
        {
            Vector3 fingerPos = rightIndexTip.position;

            // Start new line if finger just became tracked
            if (!wasRightFingerTracked)
            {
                StartNewBrushLine(fingerPos);
                lastRightFingerPos = fingerPos;
            }
            // Continue line if finger moved enough
            else if (Vector3.Distance(fingerPos, lastRightFingerPos) > minDistanceBetweenPoints)
            {
                AddPointToBrushLine(fingerPos);
                lastRightFingerPos = fingerPos;
            }
        }
        else if (wasRightFingerTracked)
        {
            EndBrushLine(); // End line when tracking lost
        }

        wasRightFingerTracked = isTracked;
    }
}
```

**Left Hand**: Particle system spawning
```csharp
// HandPaintParticleController.cs:136-162
void UpdateLeftHandParticles()
{
    if (particleBrush == null) return;

    var leftHandJoints = GetHandJoints(JointName.IndexTip, true); // true = left hand

    if (leftHandJoints != null && leftHandJoints.Count > 0)
    {
        Transform leftIndexTip = leftHandJoints[0].transform;
        bool isTracked = leftIndexTip.gameObject.activeInHierarchy;

        if (isTracked)
        {
            Vector3 fingerPos = leftIndexTip.position;

            // Spawn particle if moved enough or first spawn
            if (!wasLeftFingerTracked || Vector3.Distance(fingerPos, lastLeftFingerPos) >= particleSpawnDistance)
            {
                SpawnParticleAt(fingerPos);
                lastLeftFingerPos = fingerPos;
            }
        }

        wasLeftFingerTracked = isTracked;
    }
}
```

#### ‚úÖ **Reflection-Based HoloKit Integration** (Lines 164-189)

Accesses HoloKit's private hand joint data without requiring SDK modifications:

```csharp
List<GameObject> GetHandJoints(JointName jointName, bool isLeftHand)
{
    // Use reflection to access private hand joints
    var handsField = typeof(HandTrackingManager).GetField("m_Hands",
        System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance);
    var handJointsField = typeof(HandTrackingManager).GetField("m_HandJoints",
        System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance);

    if (handsField != null && handJointsField != null)
    {
        var hands = handsField.GetValue(handTrackingManager) as List<GameObject>;
        var handJoints = handJointsField.GetValue(handTrackingManager) as List<Dictionary<JointName, GameObject>>;

        if (hands != null && handJoints != null && hands.Count > 0)
        {
            int handIndex = isLeftHand ? 1 : 0;
            if (handIndex < handJoints.Count && handJoints[handIndex].ContainsKey(jointName))
            {
                return new List<GameObject> { handJoints[handIndex][jointName] };
            }
        }
    }

    return null;
}
```

#### ‚úÖ **Platform-Specific Compilation** (Lines 46-50)

Auto-disables in Unity Editor to prevent HoloKit SDK errors:

```csharp
void Start()
{
    // Disable this component if not on actual iOS device
#if !UNITY_IOS || UNITY_EDITOR
    Debug.Log("[HandPaintParticleController] Disabling - only works on iOS device");
    enabled = false;
    return;
#endif
    // ... HoloKit initialization
}
```

#### ‚úÖ **UI Integration Methods** (Lines 264-325)

Public methods for brush control:
```csharp
public void ToggleBrush()             // Enable/disable right hand brush
public void ToggleParticles()         // Enable/disable left hand particles
public void NextParticleBrush()       // Cycle to next brush
public void PreviousParticleBrush()   // Cycle to previous brush
public void ClearAll()                // Clear all strokes and particles
public void SetBrushWidth(float width)
public void SetParticleScale(float scale)
public void SetBrushColor(Color color)
```

---

### 2.4 WHAT'S WORKING ‚úÖ

1. ‚úÖ **Hand Tracking**: HoloKit SDK integrated, reflection-based joint access
2. ‚úÖ **Right Hand Painting**: Index finger ‚Üí LineRenderer strokes
3. ‚úÖ **Left Hand Particles**: Index finger ‚Üí Particle system spawning
4. ‚úÖ **Distance Filtering**: `minDistanceBetweenPoints` prevents excessive points (0.01m default)
5. ‚úÖ **Stroke State Management**: Tracks finger tracking state, ends strokes cleanly
6. ‚úÖ **Platform Safety**: Auto-disables in Editor, iOS-only compilation
7. ‚úÖ **UI Controls**: Public methods for toggle, clear, brush switching
8. ‚úÖ **Brush Integration**: v3 EnchantedPaintbrush line and mesh painters functional (particle pass pending overhaul)

---

### 2.5 WHAT'S MISSING / OPTIONAL ENHANCEMENTS

#### ‚ö†Ô∏è **iOS Device Testing Required** (4-8 hours)

**Must Test**:
- [ ] Deploy to iPhone 12+ with iOS 16+
- [ ] Verify HoloKit hand tracking works on device
- [ ] Test right hand LineRenderer painting
- [ ] Test left hand particle spawning
- [ ] Measure frame rate (target: 60 FPS)
- [ ] Test brush switching (Next/Previous)
- [ ] Verify clear/toggle functions
- [ ] Profile with Xcode Instruments (check memory leaks)

**Testing Scene**: `Interaction.unity` (contains HoloKit XR Origin + HandPaintParticleController)

#### ‚úÖ **Required: EnchantedPaintbrush Integration Alignment** (12-16 hours)

Hand tracking must target the v3 EnchantedPaintbrush stack to unlock contextual attributes, serialization, and undo/redo. Remaining gaps:

- [ ] `HandTrackingInputSource` implementing the existing v3 input abstraction
- [ ] Update `HandPaintParticleController` to drive `EnchantedPaintbrush.AddPoint()` instead of legacy components
- [ ] Dual-mode switching (touch ‚Üî hand tracking) using the v3 input service
- [ ] Validation with `LinePainter`, `MeshPainter`, and the refactored `ParticlePainter`

**Time Estimate**: 12-16 hours (mandatory for v3 parity)

---

#### 2.5.1 Verified v3 Particle Brush Implementation (2025-11-02)

**Objective**: Make the v3 brush system self-contained and performant by moving emission control into `EnchantedPaintbrush` and pooling the particle prefab in `ParticlePainter`.

**Step-by-Step Plan**

1. **Extend `EnchantedPaintbrush` (base class)**
   - Add protected fields `GameObject currentBrushInstance`, `bool isPainting`.
   - Introduce virtual `StartPainting()` / `StopPainting()`; invoke them from the existing `Update()` pathway when a new stroke begins/ends.
   - Implement `EnableEmission()` / `DisableEmission()` to toggle `ParticleSystem`, `TrailRenderer`, and `VisualEffect` properties on the active brush instance.

2. **Add a Pooling Layer to `ParticlePainter`**
   - Maintain reusable particle objects per stroke (e.g., `List<List<GameObject>>`) so `RenderStrokes()` and `AddPoint()` reuse instances instead of instantiating/destroying.
   - When the `Particle` attribute changes, rebuild the pool for the new prefab and, if already painting, call `EnableEmission()`.
   - Update contextual attributes (`Size`, `Color`, etc.) by iterating through the pooled objects rather than respawning them.
   - Keep stroke data intact so `Save()` / `Load()` can repopulate the pool deterministically.

3. **Validation & Testing**
   - Stress-test long strokes (500+ points), rapid attribute toggles, undo/redo, and scene reload to confirm zero allocations and stable pooling.
   - Profile GC allocations / transform counts pre/post change.
   - Update `_SECTION_0_QUICK_START.md` once behaviour is verified.

4. **Optional Extensions** *(post-P0.2)*
   - Apply the same pooling strategy to `LinePainter` / `MeshPainter` to keep the brush family consistent.
   - Drive emission rates, trail lifetimes, and other parameters from contextual attributes once pooling is stable.

**Outcome**: Meets P0.2 success metrics (60‚ÄØFPS, pooled instances, no v2 dependencies) and unlocks further pooling work (Trail/Size contextual control, Normcore sync).

---

### 2.6 SCENE SETUP (Already Configured)

**Scene**: `Interaction.unity`

**Hierarchy**:
```
Scene Root
  ‚îî‚îÄ HoloKit XR Origin (HoloKit SDK prefab)
        ‚îú‚îÄ HandTrackingManager (HoloKit component)
        ‚îú‚îÄ Camera
        ‚îî‚îÄ HandPaintParticleController (GameObject)
              ‚îú‚îÄ EnchantedPaintbrush references (Line, Mesh, Particle)
              ‚îî‚îÄ brushEnabled, particlesEnabled toggles (driving contextual attributes)
```

---

### 2.7 COMPARISON: HoloKit SDK vs AR Foundation XR Hands

| Feature | HoloKit SDK (ACTUAL) | AR Foundation XR Hands (PROPOSED) |
|---------|---------------------|-----------------------------------|
| **Installation** | ‚úÖ Git URL package | ‚úÖ Package Manager |
| **Hand Tracking** | ‚úÖ ARKit via HoloKit API | ‚úÖ XR Hand Subsystem |
| **Joint Count** | 26 joints per hand | 26 joints per hand |
| **Access Method** | Reflection (private fields) | Public API (XRHand.GetJoint()) |
| **Gesture Detection** | Manual (distance checks) | Manual (XRHandJointID) |
| **Platform** | iOS only (HoloKit headset) | iOS + Android (ARCore) |
| **Implementation** | ‚úÖ DONE (HandPaintParticleController) | ‚ùå NOT STARTED |
| **Time to Implement** | 40-60h (COMPLETE) | 40-60h (if redoing) |
| **Advantages** | Works now, HoloKit ecosystem | Cross-platform, public API |
| **Disadvantages** | Requires reflection, iOS-only | Would require rewrite |

**Recommendation**: Keep HoloKit implementation, test on iOS device first. Consider AR Foundation migration only if cross-platform support needed.

---

### 2.8 FILES IMPLEMENTED ‚úÖ

**Existing Files (NO CHANGES NEEDED)**:
- ‚úÖ `Assets/[H3M]/Portals/Content/__Paint_AR/HandPaintParticleController.cs` (326 lines)
- ‚úÖ `Assets/[H3M]/Portals/Content/__Paint_AR/SimpleParticleBrush.cs` (referenced)
- ‚úÖ `Assets/[H3M]/Portals/Content/__Paint_AR/SimplePaintStroke.cs` (referenced)
- ‚úÖ `Interaction.unity` scene (configured with HoloKit XR Origin)

**In-progress v3 Integration Tasks**:
- `Assets/[H3M]/Portals/Code/v3/XR/Input/IInputSource.cs` (existing interface)
- `Assets/[H3M]/Portals/Code/v3/XR/Input/HandTrackingInputSource.cs` (to be implemented)
- `HandPaintParticleController` updates (route to EnchantedPaintbrush hierarchy)
- `ParticlePainter` pooling refactor (see Section 0 plan)

---

### 2.9 TESTING CHECKLIST

#### ‚úÖ **Editor Testing (PASSED)**
- [x] Compiles with zero errors (verified 2025-11-01)
- [x] Auto-disables in Editor (prevents HoloKit errors)
- [x] Touch input still works (doesn't interfere)

#### ‚è≥ **iOS Device Testing (PENDING)**
- [ ] Deploy to iPhone 12+ with iOS 16+
- [ ] Verify hand tracking initialization
- [ ] Test right hand LineRenderer painting
- [ ] Test left hand particle spawning
- [ ] Verify brush switching (Next/Previous)
- [ ] Test clear/toggle functions
- [ ] Measure frame rate ‚â•60 FPS
- [ ] No console errors during 10-minute session
- [ ] Profile with Xcode Instruments

#### ‚úÖ **v3 Integration Testing**
- [ ] HandTrackingInputSource drives EnchantedPaintbrush
- [ ] Input source switching (touch ‚Üî hand tracking)
- [ ] Contextual attributes applied correctly (size/color/material updates)
- [ ] Undo/redo works with hand-painted strokes

---

### 2.10 TIME ACCOUNTING

| Phase | Estimated | Actual | Status |
|-------|-----------|--------|--------|
| HoloKit SDK Setup | 8-12h | ~10h | ‚úÖ DONE |
| HandPaintParticleController Implementation | 20-28h | ~30h | ‚úÖ DONE |
| Dual-Hand Logic | 8-12h | ~10h | ‚úÖ DONE |
| Reflection-Based Joint Access | 4-8h | ~8h | ‚úÖ DONE |
| UI Integration Methods | 4-6h | ~4h | ‚úÖ DONE |
| Scene Setup & Configuration | 4-6h | ~6h | ‚úÖ DONE |
| **TOTAL INVESTED** | **48-72h** | **~68h** | **‚úÖ COMPLETE** |
| **iOS Device Testing** | **4-8h** | **0h** | **‚è≥ PENDING** |
| **EnchantedPaintbrush Alignment** | **12-16h** | **0h** | **üöß IN PROGRESS (MANDATORY)** |

**Net Time Remaining**: 4-8 hours (iOS testing) + 12-16 hours (EnchantedPaintbrush alignment)

---

### 2.11 NEXT STEPS

**Immediate** (4-8 hours):
1. Deploy to iOS device (iPhone 12+, iOS 16+)
2. Test hand tracking painting (both hands)
3. Verify frame rate ‚â•60 FPS
4. Profile with Xcode Instruments
5. Document any bugs or performance issues

**EnchantedPaintbrush Alignment** (12-16 hours, mandatory):
1. Implement `HandTrackingInputSource` (`IInputSource`)
2. Route hand events through EnchantedPaintbrush (Line/Mesh/Particle)
3. Finish ParticlePainter pooling + contextual attribute wiring
4. Add input source switching UI (touch ‚Üî hand tracking)

**Documentation**:
1. Update `.claude/_BRUSH_TRACKING_INPUT_DEEP_ANALYSIS.md` with HoloKit SDK details
2. Create iOS testing guide with expected results
3. Document HoloKit vs AR Foundation comparison

---

**Last Updated**: 2025-11-01
**Verification**: Code analysis complete, file existence verified
**Confidence**: 100% (implementation verified in codebase)

---

## 3. VFX GRAPH BODY TRACKING IMPLEMENTATION

**Goal**: Real-time body tracking with VFX Graph particle effects

### 3.1 Prerequisites
- ‚úÖ **VFX Graph 17.1.0** - Already installed
- ‚úÖ **Unity Sentis 2.1.2** - Already installed (AI/ML inference)
- ‚úÖ **BodyPix 3.0.0** - Already installed (jp.keijiro.bodypix)

### 3.2 Architecture

```
iOS Camera Feed (RGB)
  ‚îÇ
  ‚îú‚îÄ> Unity Sentis (On-device ML)
  ‚îÇ     ‚îî‚îÄ> BodyPix model (person segmentation)
  ‚îÇ           ‚îú‚îÄ> Body mask texture (512x512)
  ‚îÇ           ‚îú‚îÄ> Joint positions (17 keypoints)
  ‚îÇ           ‚îî‚îÄ> Confidence scores
  ‚îÇ
  ‚îú‚îÄ> VFX Graph Integration
  ‚îÇ     ‚îú‚îÄ> Spawn particles at body joints
  ‚îÇ     ‚îú‚îÄ> Body mask as emission texture
  ‚îÇ     ‚îî‚îÄ> Follow skeleton motion
  ‚îÇ
  ‚îî‚îÄ> Rendering
        ‚îî‚îÄ> URP 17.1.0 with VFX Graph support
```

### 3.3 Implementation Steps (~60-80 hours)

#### **Phase 1: BodyPix Integration** (16-24h)

**Step 1**: Locate BodyPix model (already in packages)
```bash
# BodyPix package already installed: jp.keijiro.bodypix 3.0.0
# Model location: Library/PackageCache/jp.keijiro.bodypix@.../
```

**Step 2**: Create BodyTracker.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/BodyTracking/BodyTracker.cs
using Unity.Sentis;
using UnityEngine;
using UnityEngine.XR.ARFoundation;

namespace H3M.XR
{
    public class BodyTracker : MonoBehaviour
    {
        [SerializeField] private ModelAsset bodyPixModel;
        [SerializeField] private ARCameraManager cameraManager;

        private IWorker engine;
        private RenderTexture inputTexture;

        public Vector3[] JointPositions { get; private set; } = new Vector3[17];
        public Texture2D BodyMask { get; private set; }

        void Start()
        {
            var runtimeModel = ModelLoader.Load(bodyPixModel);
            engine = WorkerFactory.CreateWorker(BackendType.GPUCompute, runtimeModel);

            inputTexture = new RenderTexture(512, 512, 0);
        }

        void Update()
        {
            if (cameraManager.TryAcquireLatestCpuImage(out var cpuImage))
            {
                // Convert camera image to texture
                CopyToRenderTexture(cpuImage, inputTexture);

                // Process with BodyPix model
                var inputTensor = TextureConverter.ToTensor(inputTexture);
                engine.Execute(inputTensor);

                var outputTensor = engine.PeekOutput() as Tensor;

                // Extract joint positions and body mask
                ParseBodyPixOutput(outputTensor);

                inputTensor.Dispose();
                cpuImage.Dispose();
            }
        }

        void ParseBodyPixOutput(Tensor tensor)
        {
            // Extract 17 body keypoints (COCO format)
            // 0: nose, 1-2: eyes, 3-4: ears, 5-6: shoulders,
            // 7-8: elbows, 9-10: wrists, 11-12: hips,
            // 13-14: knees, 15-16: ankles

            // Convert to Unity world space coordinates
            // Store in JointPositions array
        }

        void OnDestroy()
        {
            engine?.Dispose();
            if (inputTexture != null)
                Destroy(inputTexture);
        }
    }
}
```

**Step 3**: Test body tracking on iOS device

#### **Phase 2: VFX Graph Body Effects** (20-28h)

**Step 4**: Create BodyParticles.vfxgraph
- Unity Editor: `Window ‚Üí Visual Effects ‚Üí Create VFX Graph`
- Location: `Assets/[H3M]/Portals/Content/VFX/BodyTracking/BodyParticles.vfxgraph`

**VFX Graph Setup**:
```
Context Initialize:
- Capacity: 50,000 particles
- Bounds: Auto

Spawn System:
- Custom Spawner (C# AttributeBinder)
- Spawn Rate: 5,000 particles/sec
- Position: From body joint array (17 positions)

Update System:
- Age particles over time
- Velocity: 0.1 m/s outward
- Gravity: -0.5

Output:
- Particle Quad
- Color: Gradient based on Y position (body heat map)
- Size: 0.01 - 0.05m
- Blending: Additive
```

**Step 5**: Create BodyVFXController.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/BodyTracking/BodyVFXController.cs
using UnityEngine;
using UnityEngine.VFX;

namespace H3M.XR
{
    public class BodyVFXController : MonoBehaviour
    {
        [SerializeField] private BodyTracker bodyTracker;
        [SerializeField] private VisualEffect vfxGraph;

        private static readonly int JointPositionsProperty = Shader.PropertyToID("JointPositions");
        private static readonly int BodyMaskProperty = Shader.PropertyToID("BodyMask");

        void Update()
        {
            if (bodyTracker.JointPositions != null)
            {
                // Send joint positions to VFX Graph
                vfxGraph.SetVector3Array(JointPositionsProperty, bodyTracker.JointPositions);

                if (bodyTracker.BodyMask != null)
                {
                    vfxGraph.SetTexture(BodyMaskProperty, bodyTracker.BodyMask);
                }
            }
        }
    }
}
```

#### **Phase 3: Advanced Body Effects** (16-20h)

**Step 6**: Implement effect variations
- **Skeleton Trail** - Particles follow skeleton motion with trail renderer
- **Body Outline** - VFX particles emit from body contour
- **Joint Halos** - Glowing spheres at major joints (shoulders, hips, head)
- **Body Splatter** - Paint-like effect when moving fast

**Step 7**: Optimize for iOS
- Reduce particle count for iPhone 12/13: 10,000 particles max
- Use GPU instancing for particles
- LOD system based on device performance
- Profile with Xcode GPU Frame Capture

#### **Phase 4: Testing** (8-12h)

**Step 8**: iOS Device Testing
- Test with various body poses (standing, sitting, arms up, etc.)
- Verify real-time tracking (30 FPS minimum)
- Profile with Xcode Instruments
- Test occlusion with AR objects
- Measure battery impact

### 3.4 Files to Create

**New Files**:
- `Assets/[H3M]/Portals/Code/v3/XR/BodyTracking/BodyTracker.cs`
- `Assets/[H3M]/Portals/Code/v3/XR/BodyTracking/BodyVFXController.cs`
- `Assets/[H3M]/Portals/Content/VFX/BodyTracking/BodyParticles.vfxgraph`
- `Assets/[H3M]/Portals/Content/VFX/BodyTracking/BodyOutline.vfxgraph`
- `Assets/[H3M]/Portals/Content/VFX/BodyTracking/SkeletonTrail.vfxgraph`

### 3.5 Testing Checklist

- [ ] BodyPix model loads successfully
- [ ] Body keypoints detected accurately
- [ ] VFX particles spawn at joint positions
- [ ] Effects follow body motion smoothly
- [ ] Frame rate ‚â•30 FPS on iPhone 12+
- [ ] No memory leaks during 15-minute session
- [ ] Battery drain acceptable (<20% per hour)

**Total Time**: ~60-80 hours

---

## 4. AUDIO REACTIVE BRUSHES IMPLEMENTATION

**Status**: ‚ö†Ô∏è **PARTIALLY IMPLEMENTED** - Prototype Exists, Needs P1 Integration
**Time Invested**: ~20-30 hours (PARTIAL)
**Remaining**: 20-30 hours (4-band FFT, brush integration, iOS audio session)
**Goal**: Brushes that react to music/audio input in real-time

---

### 4.0 CURRENT IMPLEMENTATION (Verified 2025-11-01)

#### ‚úÖ What EXISTS (Prototype Phase)

**1. Basic Audio Analysis** - [Mic.cs:1-101](Assets/[H3M]/Portals/Content/__Paint_AR/Scripts/Mic.cs)
```csharp
// Lines 65-94: FFT analysis with RMS/dB conversion
public IEnumerator analyzeSound()
{
    float[] spectrum = audio.GetSpectrumData(SAMPLECOUNT, 0, FFTWindow.BlackmanHarris);
    float sum = 0;
    for (int i = 0; i < spectrum.Length; i++) {
        sum += Mathf.Pow(spectrum[i], 2);
    }
    rmsValue = Mathf.Sqrt(sum / SAMPLECOUNT);
    dbValue = 150 * Mathf.Log10(rmsValue / REFVALUE);
    if (dbValue < -clamp) dbValue = -clamp;

    averageVolume = map(dbValue, -160, 0, 0, 1); // Public field for particle control
    yield return null;
}
```

**Features**:
- ‚úÖ FFT Analysis: 1024 samples @ 48kHz (SAMPLECOUNT = 1024, FREQUENCY = 48000)
- ‚úÖ RMS to dB Conversion: `dbValue = 150 * Mathf.Log10(rmsValue / REFVALUE)`
- ‚úÖ Volume Mapping: Normalized 0-1 range via `map()` function
- ‚úÖ Public API: `averageVolume` field for external access

**2. Particle Playback Speed Control** - [MicHandle.cs:1-369](Assets/[H3M]/Portals/Content/__Paint_AR/_CustomAudio/MicHandle.cs)
```csharp
// Lines 152-154: ParticleSystem playback speed modulation
Flow.playbackSpeed = currentSpeed;
Flow2.playbackSpeed = currentSpeed;
Flow3.playbackSpeed = currentSpeed;
```

**Features**:
- ‚úÖ Multiple Particle Systems: Flow, Flow2, Flow3, Fireworks (lines 150-163)
- ‚úÖ Speed Modulation: `currentSpeed` derived from Mic.averageVolume
- ‚úÖ Real-time Updates: Update() loop (lines 121-170)

**3. Audio Analysis Library** - Reaktion
- ‚úÖ Location: `Assets/ThirdParty/Reaktion/` (Keijiro's audio analysis toolkit)
- ‚úÖ Features: Beat detection, frequency band analysis, audio-driven animations

**4. 3D Music Visualizer** - VU Meter System
- ‚úÖ Audio-driven particle effects in demo scenes
- ‚úÖ Proof-of-concept for audio reactive brushes

---

#### ‚ùå What's MISSING (20-30 hours remaining)

**1. H3MAudioAnalyzer (4-Band FFT)** - NOT IMPLEMENTED
- ‚ùå 4-band frequency separation (bass, mid, high, presence)
- ‚ùå Beat detection per frequency band
- ‚ùå Frequency band normalization for brush mapping
- **Needed for**: Per-frequency brush control (bass ‚Üí size, high ‚Üí color)

**2. BrushReactivity ScriptableObject System** - NOT IMPLEMENTED
- ‚ùå Scriptable brush reactivity profiles
- ‚ùå Mapping: frequency bands ‚Üí brush parameters (size, color, emission, VFX intensity)
- ‚ùå Per-brush reactivity configuration
- **Needed for**: Artist-friendly brush reactivity design

**3. iOS AVAudioSession Setup** - NOT IMPLEMENTED
- ‚ùå Proper iOS microphone permission handling
- ‚ùå Audio session category/mode configuration
- ‚ùå Dynamic device selection (currently hardcoded: `Microphone.devices[0]`, Mic.cs:59)
- **Needed for**: Robust iOS microphone input

**4. Brush Integration** - NOT IMPLEMENTED
- ‚ùå ParticleBrushManager integration (v2)
- ‚ùå EnchantedPaintbrush integration (v3)
- ‚ùå Real-time brush parameter modulation (size, color, emission)
- **Needed for**: Actual audio-reactive painting

**5. Advanced Effects** - NOT IMPLEMENTED
- ‚ùå Waveform Brush (3D audio waveform visualization)
- ‚ùå Spectrum Brush (frequency spectrum as vertical bars)
- ‚ùå Beat Pulse (particle burst on beat detection)
- ‚ùå Bassquake (VFX shake effect for bass frequencies)

---

#### üîÑ REMAINING WORK BREAKDOWN (20-30 hours)

| Task | Time | Status | Files |
|------|------|--------|-------|
| H3MAudioAnalyzer (4-band FFT) | 6-8h | ‚ùå NOT STARTED | `H3MAudioAnalyzer.cs` (new) |
| BrushReactivity ScriptableObject | 4-6h | ‚ùå NOT STARTED | `BrushReactivity.cs` (new) |
| iOS AVAudioSession Setup | 2-4h | ‚ùå NOT STARTED | Modify `Mic.cs` (lines 53-63) |
| ParticleBrushManager Integration | 4-6h | ‚ùå NOT STARTED | Modify `ParticleBrushManager.cs` |
| AudioReactivePaintbrush (v3) | 4-6h | ‚ùå NOT STARTED | `AudioReactivePaintbrush.cs` (new) |
| **TOTAL REMAINING** | **20-30h** | | |

---

### 4.1 TARGET ARCHITECTURE (Proposed)

```
Microphone / Audio Source
  ‚îÇ
  ‚îú‚îÄ> Mic.cs ‚úÖ EXISTS
  ‚îÇ     ‚îú‚îÄ> FFT Analysis (1024 samples) ‚úÖ
  ‚îÇ     ‚îú‚îÄ> RMS (volume level) ‚úÖ
  ‚îÇ     ‚îî‚îÄ> averageVolume (public API) ‚úÖ
  ‚îÇ
  ‚îú‚îÄ> H3MAudioAnalyzer.cs ‚ùå NEEDS IMPLEMENTATION
  ‚îÇ     ‚îú‚îÄ> 4-Band FFT (bass, mid, high, presence)
  ‚îÇ     ‚îú‚îÄ> Beat Detection per band
  ‚îÇ     ‚îî‚îÄ> Frequency normalization
  ‚îÇ
  ‚îú‚îÄ> BrushReactivity.cs (ScriptableObject) ‚ùå NEEDS IMPLEMENTATION
  ‚îÇ     ‚îú‚îÄ> Frequency ‚Üí Brush Size mapping curve
  ‚îÇ     ‚îú‚îÄ> Frequency ‚Üí Brush Color gradient
  ‚îÇ     ‚îú‚îÄ> Beat ‚Üí Particle Emission multiplier
  ‚îÇ     ‚îî‚îÄ> Bass ‚Üí VFX Intensity curve
  ‚îÇ
  ‚îî‚îÄ> ParticleBrushManager / EnchantedPaintbrush Integration ‚ùå NEEDS IMPLEMENTATION
        ‚îú‚îÄ> Real-time brush size modulation
        ‚îú‚îÄ> Real-time brush color modulation
        ‚îú‚îÄ> Particle emission rate control
        ‚îî‚îÄ> VFX intensity control
```

---

### 4.2 Implementation Steps (~20-30 hours remaining)

**Note**: Below are the REMAINING implementation steps. Mic.cs, MicHandle.cs, and Reaktion library are already complete.



#### **Phase 1: Audio Analysis** (6-8h) - H3MAudioAnalyzer

```
Microphone / Audio Source
  ‚îÇ
  ‚îú‚îÄ> AudioProcessor.cs
  ‚îÇ     ‚îú‚îÄ> FFT Analysis (1024 samples)
  ‚îÇ     ‚îú‚îÄ> Beat Detection (bass, mid, high)
  ‚îÇ     ‚îú‚îÄ> RMS (volume level)
  ‚îÇ     ‚îî‚îÄ> Frequency Bands (8 bands)
  ‚îÇ
  ‚îî‚îÄ> EnchantedPaintbrush Integration
        ‚îú‚îÄ> Brush size ‚Üí Audio volume
        ‚îú‚îÄ> Brush color ‚Üí Frequency spectrum
        ‚îú‚îÄ> Particle emission ‚Üí Beat detection
        ‚îî‚îÄ> VFX intensity ‚Üí Bass level
```


#### **Phase 1: Audio Analysis** (12-16h)

**Step 1**: Create AudioProcessor.cs (adapted from echovision)
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Audio/AudioProcessor.cs
using UnityEngine;

namespace H3M.Audio
{
    public class AudioProcessor : MonoBehaviour
    {
        [SerializeField] private AudioSource audioSource;
        [SerializeField] private int sampleSize = 1024;
        [SerializeField] private float beatThreshold = 1.5f;

        private float[] samples;
        private float[] freqBands = new float[8];
        private float[] bandBuffer = new float[8];
        private float[] bufferDecrease = new float[8];

        public float RMS { get; private set; }
        public float[] FrequencyBands => freqBands;
        public bool BeatDetected { get; private set; }

        void Start()
        {
            samples = new float[sampleSize];

            if (audioSource == null)
            {
                audioSource = gameObject.AddComponent<AudioSource>();
                audioSource.loop = true;
            }
        }

        void Update()
        {
            AnalyzeAudio();
        }

        void AnalyzeAudio()
        {
            // Get spectrum data
            audioSource.GetSpectrumData(samples, 0, FFTWindow.BlackmanHarris);

            // Calculate frequency bands
            MakeFrequencyBands();

            // Calculate RMS (volume)
            CalculateRMS();

            // Detect beats
            DetectBeat();
        }

        void MakeFrequencyBands()
        {
            int count = 0;

            // Divide spectrum into 8 frequency bands
            for (int i = 0; i < 8; i++)
            {
                float average = 0;
                int sampleCount = (int)Mathf.Pow(2, i) * 2;

                if (i == 7)
                {
                    sampleCount += 2; // Include remaining samples
                }

                for (int j = 0; j < sampleCount; j++)
                {
                    if (count < samples.Length)
                    {
                        average += samples[count] * (count + 1);
                        count++;
                    }
                }

                average /= count;
                freqBands[i] = average * 10; // Scale for visibility

                // Buffer system for smoother visualization
                if (freqBands[i] > bandBuffer[i])
                {
                    bandBuffer[i] = freqBands[i];
                    bufferDecrease[i] = 0.005f;
                }

                if (freqBands[i] < bandBuffer[i])
                {
                    bandBuffer[i] -= bufferDecrease[i];
                    bufferDecrease[i] *= 1.2f;
                }
            }
        }

        void CalculateRMS()
        {
            float sum = 0;
            for (int i = 0; i < samples.Length; i++)
            {
                sum += samples[i] * samples[i];
            }
            RMS = Mathf.Sqrt(sum / samples.Length);
        }

        void DetectBeat()
        {
            // Simple beat detection based on bass frequencies (band 0-1)
            float bassEnergy = freqBands[0] + freqBands[1];
            BeatDetected = bassEnergy > beatThreshold;
        }
    }
}
```

**Step 2**: Test audio analysis with music file

#### **Phase 2: Audio Reactive Brushes** (12-16h)

**Step 3**: Create AudioReactivePaintbrush.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/AudioReactivePaintbrush.cs
using UnityEngine;
using H3M.Audio;

namespace H3M
{
    public class AudioReactivePaintbrush : EnchantedPaintbrush
    {
        [SerializeField] private AudioProcessor audioProcessor;

        [Header("Audio Reactivity Settings")]
        [SerializeField] private AnimationCurve volumeToSize;
        [SerializeField] private Gradient frequencyToColor;
        [SerializeField] private bool enableBeatPulse = true;
        [SerializeField] private float beatPulseIntensity = 2.0f;

        private float baseRadius = 0.05f;

        protected override void AddPoint(Vector3 point)
        {
            if (audioProcessor == null) return;

            // Modulate brush size based on audio volume
            float audioVolume = audioProcessor.RMS;
            float brushSize = volumeToSize.Evaluate(audioVolume) * baseRadius;

            // Get base brush size from attributes
            baseRadius = GetFloat("Radius");
            if (baseRadius == 0) baseRadius = 0.05f;

            // Apply audio modulation
            SetFloat("Radius", brushSize);

            // Change color based on frequency spectrum
            float bassLevel = audioProcessor.FrequencyBands[0];
            float midLevel = audioProcessor.FrequencyBands[3];
            float highLevel = audioProcessor.FrequencyBands[7];

            // Map frequency to gradient position
            float gradientPos = Mathf.Clamp01((bassLevel + midLevel + highLevel) / 3f);
            Color audioColor = frequencyToColor.Evaluate(gradientPos);

            // Create material with audio-reactive color
            Material audioMaterial = new Material(GetMaterial("Line Material"));
            audioMaterial.color = audioColor;
            SetMaterial("Line Material", audioMaterial);

            // Add point with modified properties
            base.AddPoint(point);

            // Emit beat particles on beat detection
            if (enableBeatPulse && audioProcessor.BeatDetected)
            {
                EmitBeatParticles(point);
            }
        }

        private void EmitBeatParticles(Vector3 position)
        {
            GameObject beatParticle = GetParticleSystem("Particle");
            if (beatParticle != null)
            {
                GameObject instance = GameObject.Instantiate(beatParticle, transform);
                instance.transform.position = position;
                instance.transform.localScale = Vector3.one * beatPulseIntensity;
                Destroy(instance, 2.0f); // Clean up after 2 seconds
            }
        }

        protected override void HandleAttr(ContextualAttribute a)
        {
            RenderStrokes();
        }

        protected override void RenderStrokes()
        {
            // Use parent implementation
            // Each stroke already has audio-reactive properties baked in
        }
    }
}
```

**Step 4**: Test with music on iOS device

#### **Phase 3: Advanced Audio Effects** (8-12h)

**Step 5**: Implement effect variations
- **Waveform Brush** - Draws audio waveform in 3D space
- **Spectrum Brush** - Each stroke shows frequency spectrum as vertical bars
- **Beat Pulse** - Particles burst outward on beat detection
- **Bassquake** - VFX intensity tied to bass frequencies, creates shake effect

**Step 6**: Create WaveformBrush.cs for 3D audio visualization

### 4.3 Files to Create

**New Files**:
- `Assets/[H3M]/Portals/Code/v3/Audio/AudioProcessor.cs`
- `Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/AudioReactivePaintbrush.cs`
- `Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/WaveformBrush.cs`
- `Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/SpectrumBrush.cs`

### 4.4 Testing Checklist

- [ ] Audio analysis working (FFT, RMS, beat detection)
- [ ] Brush size reacts to volume in real-time
- [ ] Brush color changes with frequency spectrum
- [ ] Beat detection triggers particle bursts
- [ ] Works with microphone input
- [ ] Works with audio file playback
- [ ] No audio latency (<50ms)
- [ ] Frame rate stable ‚â•60 FPS

**Total Time**: ~32-48 hours

---

## 5. WHISPER.ICOSA SPEECH-TO-OBJECT INTEGRATION

**Goal**: "Speak a word ‚Üí Fetch 3D object from Icosa API ‚Üí Spawn in AR"

### 5.1 Prerequisites
- ‚úÖ **Whisper.Unity** package - Available in Whisper.Icosa project
- ‚úÖ **Icosa API Client** - Available in icosa-api-client-unity-main
- ‚ùå **glTFast 6.9.0** - **MUST ADD** to Portals_6

### 5.2 Architecture

```
User Speech
  ‚îÇ
  ‚îú‚îÄ> Whisper.Unity (On-device speech-to-text)
  ‚îÇ     ‚îú‚îÄ> Model: ggml-tiny.bin (39MB)
  ‚îÇ     ‚îú‚îÄ> Input: 16kHz audio, 10sec max
  ‚îÇ     ‚îî‚îÄ> Output: Transcribed text
  ‚îÇ
  ‚îú‚îÄ> Icosa API Client
  ‚îÇ     ‚îú‚îÄ> Query: GET https://poly.pizza/api/search?q={text}
  ‚îÇ     ‚îú‚îÄ> Filter: license=CC-BY, format=GLTF
  ‚îÇ     ‚îî‚îÄ> Response: List of 3D models (glTF/glb)
  ‚îÇ
  ‚îî‚îÄ> H3M EnchantedScenes Integration
        ‚îú‚îÄ> Download glTF model (glTFast 6.9.0)
        ‚îú‚îÄ> Spawn as EnchantedMedia object
        ‚îî‚îÄ> Place at AR raycast hit point
```

### 5.3 Implementation Steps (~60-80 hours)

#### **Phase 1: Package Integration** (12-16h)

**Step 1**: Copy Whisper.Unity package
```bash
# Copy package to Portals_6
cp -r "/Users/jamestunick/wkspaces/Whisper.Icosa/Whisper.Unity.Demo/Packages/com.whisper.unity" \
      "/Users/jamestunick/wkspaces/Portals_6/Packages/"

# Copy Whisper model files
mkdir -p "/Users/jamestunick/wkspaces/Portals_6/Assets/StreamingAssets/Whisper"
cp "/Users/jamestunick/wkspaces/Whisper.Icosa/Whisper.Unity.Demo/Assets/StreamingAssets/Whisper/ggml-tiny.bin" \
   "/Users/jamestunick/wkspaces/Portals_6/Assets/StreamingAssets/Whisper/"
```

**Step 2**: Copy Icosa API Client
```bash
cp -r "/Users/jamestunick/wkspaces/Whisper.Icosa/icosa-api-client-unity-main/Assets/IcosaApi" \
      "/Users/jamestunick/wkspaces/Portals_6/Assets/[H3M]/Portals/Code/v3/Services/IcosaApi"
```

**Step 3**: Update Packages/manifest.json
```json
{
  "dependencies": {
    "com.whisper.unity": "file:../Packages/com.whisper.unity",
    "com.unity.cloud.gltfast": "6.9.0"
  }
}
```

**Step 4**: Verify package installation in Unity Editor

#### **Phase 2: Speech Recognition Integration** (16-24h)

**Step 5**: Create H3MSpeechRecognizer.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Services/Speech/H3MSpeechRecognizer.cs
using System;
using System.Threading.Tasks;
using UnityEngine;
using Whisper;
using Whisper.Utils;

namespace H3M.Services
{
    public class H3MSpeechRecognizer : MonoBehaviour
    {
        [SerializeField] private WhisperManager whisperManager;
        [SerializeField] private MicrophoneRecord microphoneRecord;

        [Header("Recording Settings")]
        [SerializeField] private int recordingLength = 10; // seconds
        [SerializeField] private int sampleRate = 16000; // Hz

        public event Action<string> OnTranscriptionComplete;
        public event Action<string> OnTranscriptionError;

        public bool IsRecording { get; private set; }

        void Start()
        {
            // Find components if not assigned
            if (whisperManager == null)
                whisperManager = FindObjectOfType<WhisperManager>();
            if (microphoneRecord == null)
                microphoneRecord = FindObjectOfType<MicrophoneRecord>();

            if (whisperManager == null || microphoneRecord == null)
            {
                Debug.LogError("[H3MSpeechRecognizer] Missing required components!");
            }
        }

        public async void StartListening()
        {
            if (IsRecording)
            {
                Debug.LogWarning("[H3MSpeechRecognizer] Already recording!");
                return;
            }

            Debug.Log("[H3MSpeechRecognizer] Starting microphone recording...");
            IsRecording = true;

            try
            {
                // Start microphone recording
                microphoneRecord.StartRecord();

                // Wait for recording to finish
                await Task.Delay(recordingLength * 1000);

                // Stop recording and get audio clip
                var audioClip = microphoneRecord.StopRecord();

                if (audioClip == null)
                {
                    OnTranscriptionError?.Invoke("Failed to capture audio");
                    return;
                }

                Debug.Log("[H3MSpeechRecognizer] Processing audio with Whisper...");

                // Transcribe with Whisper
                var result = await whisperManager.GetTextAsync(audioClip);

                if (result != null && !string.IsNullOrEmpty(result.Result))
                {
                    Debug.Log($"[H3MSpeechRecognizer] Transcription: '{result.Result}'");
                    OnTranscriptionComplete?.Invoke(result.Result);
                }
                else
                {
                    OnTranscriptionError?.Invoke("No speech detected");
                }
            }
            catch (Exception ex)
            {
                Debug.LogError($"[H3MSpeechRecognizer] Error: {ex.Message}");
                OnTranscriptionError?.Invoke(ex.Message);
            }
            finally
            {
                IsRecording = false;
            }
        }

        public void StopListening()
        {
            if (IsRecording)
            {
                microphoneRecord.StopRecord();
                IsRecording = false;
            }
        }
    }
}
```

**Step 6**: Test speech recognition on iOS device

#### **Phase 3: Icosa API Integration** (16-24h)

**Step 7**: Create H3MIcosaService.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Services/IcosaApi/H3MIcosaService.cs
using System.Collections.Generic;
using System.Threading.Tasks;
using UnityEngine;
using IcosaApiClient;
using GLTFast;

namespace H3M.Services
{
    public class H3MIcosaService : MonoBehaviour
    {
        [SerializeField] private string apiKey; // Optional, for rate limit increases

        private IcosaApiClient.IcosaApi icosaApi;

        void Start()
        {
            icosaApi = new IcosaApiClient.IcosaApi();
        }

        public async Task<GameObject> SearchAndSpawnObject(string searchTerm)
        {
            Debug.Log($"[H3MIcosaService] Searching for: '{searchTerm}'");

            try
            {
                // Search Icosa API (poly.pizza)
                var searchResults = await icosaApi.SearchAssets(
                    query: searchTerm,
                    maxResults: 1,
                    license: "CC-BY",
                    format: "GLTF"
                );

                if (searchResults == null || searchResults.Count == 0)
                {
                    Debug.LogWarning($"[H3MIcosaService] No results found for: {searchTerm}");
                    return null;
                }

                // Get first result
                var asset = searchResults[0];
                Debug.Log($"[H3MIcosaService] Found: {asset.displayName} by {asset.authorName}");

                // Download and instantiate glTF model
                GameObject spawnedObject = await DownloadGltfModel(asset.formats.GLTF.url, searchTerm);

                return spawnedObject;
            }
            catch (System.Exception ex)
            {
                Debug.LogError($"[H3MIcosaService] Error: {ex.Message}");
                return null;
            }
        }

        private async Task<GameObject> DownloadGltfModel(string url, string modelName)
        {
            Debug.Log($"[H3MIcosaService] Downloading glTF from: {url}");

            var gltf = new GltfImport();
            bool success = await gltf.Load(url);

            if (success)
            {
                // Instantiate model
                var instantiator = new GameObjectInstantiator(gltf, transform);
                await gltf.InstantiateMainSceneAsync(instantiator);

                GameObject spawnedObject = instantiator.SceneInstance.Root;
                spawnedObject.name = $"Icosa_{modelName}";

                Debug.Log($"[H3MIcosaService] Model instantiated successfully");
                return spawnedObject;
            }
            else
            {
                Debug.LogError("[H3MIcosaService] Failed to load glTF model");
                return null;
            }
        }
    }
}
```

**Step 8**: Test Icosa API search and model download

#### **Phase 4: Voice-to-Object Pipeline** (12-16h)

**Step 9**: Create VoiceToObjectController.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/VoiceToObject/VoiceToObjectController.cs
using UnityEngine;
using UnityEngine.XR.ARFoundation;

namespace H3M.XR
{
    public class VoiceToObjectController : MonoBehaviour
    {
        [SerializeField] private Services.H3MSpeechRecognizer speechRecognizer;
        [SerializeField] private Services.H3MIcosaService icosaService;
        [SerializeField] private Transform spawnParent;
        [SerializeField] private ARRaycastManager raycastManager;

        [Header("Spawn Settings")]
        [SerializeField] private float defaultSpawnDistance = 1.5f; // meters in front of camera
        [SerializeField] private float defaultScale = 0.3f; // 30cm

        void OnEnable()
        {
            speechRecognizer.OnTranscriptionComplete += HandleTranscription;
        }

        void OnDisable()
        {
            speechRecognizer.OnTranscriptionComplete -= HandleTranscription;
        }

        public void StartVoiceCapture()
        {
            Debug.Log("[VoiceToObject] Starting voice capture...");
            speechRecognizer.StartListening();
        }

        async void HandleTranscription(string text)
        {
            Debug.Log($"[VoiceToObject] Heard: '{text}'");

            // Show loading UI (optional)
            ShowLoadingIndicator();

            // Search and spawn object
            GameObject spawnedObject = await icosaService.SearchAndSpawnObject(text);

            HideLoadingIndicator();

            if (spawnedObject != null)
            {
                // Place object in AR space
                PlaceObjectInAR(spawnedObject);

                // Convert to EnchantedMedia for save/load support
                ConvertToEnchantedMedia(spawnedObject);
            }
            else
            {
                Debug.LogWarning($"[VoiceToObject] Failed to spawn object for: {text}");
                // Show error UI
            }
        }

        void PlaceObjectInAR(GameObject obj)
        {
            // Try AR plane raycast first
            Vector2 screenCenter = new Vector2(Screen.width / 2f, Screen.height / 2f);

            if (raycastManager.Raycast(screenCenter, hits, UnityEngine.XR.ARSubsystems.TrackableType.Planes))
            {
                // Place on detected AR plane
                obj.transform.position = hits[0].pose.position;
                obj.transform.rotation = hits[0].pose.rotation;
            }
            else
            {
                // Fallback: Place in front of camera
                Vector3 forward = Camera.main.transform.forward;
                Vector3 spawnPosition = Camera.main.transform.position + forward * defaultSpawnDistance;

                obj.transform.position = spawnPosition;
                obj.transform.LookAt(Camera.main.transform);
            }

            // Scale to reasonable size
            obj.transform.localScale = Vector3.one * defaultScale;

            // Parent to scene
            if (spawnParent != null)
            {
                obj.transform.SetParent(spawnParent, true);
            }
        }

        void ConvertToEnchantedMedia(GameObject obj)
        {
            // Add EnchantedMedia component for H3M save/load system
            var enchantedMedia = obj.AddComponent<EnchantedMedia>();
            // Configure for persistence
        }

        private void ShowLoadingIndicator()
        {
            // TODO: Show UI loading spinner
        }

        private void HideLoadingIndicator()
        {
            // TODO: Hide UI loading spinner
        }
    }
}
```

**Step 10**: Add microphone button to H3M UI
- Create UI button in Composer flow
- Connect button to `VoiceToObjectController.StartVoiceCapture()`

**Step 11**: Test end-to-end on iOS device
- Speak "cat" ‚Üí Verify cat model spawns
- Speak "tree" ‚Üí Verify tree model spawns
- Test with various objects

#### **Phase 5: Multi-API Support** (8-12h)

**Step 12**: Create abstraction layer for multiple 3D model APIs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Services/3DModelAPIs/I3DModelAPI.cs
public interface I3DModelAPI
{
    Task<List<ModelSearchResult>> Search(string query, int maxResults = 10);
    Task<GameObject> DownloadModel(string modelUrl, string modelName);
    string ApiName { get; }
}

public class ModelSearchResult
{
    public string Id;
    public string DisplayName;
    public string AuthorName;
    public string ThumbnailUrl;
    public string ModelUrl;
    public string License;
}
```

**Step 13**: Implement API adapters
- `IcosaAPI.cs` - poly.pizza (already done)
- `SketchfabAPI.cs` - Sketchfab integration
- `TurboSquidAPI.cs` - TurboSquid integration (requires API key)

**Step 14**: Create API selector system
```csharp
public class ModelAPIManager : MonoBehaviour
{
    private List<I3DModelAPI> apis = new List<I3DModelAPI>();

    public async Task<GameObject> SearchAcrossAPIs(string query)
    {
        foreach (var api in apis)
        {
            var result = await api.Search(query, 1);
            if (result != null && result.Count > 0)
            {
                return await api.DownloadModel(result[0].ModelUrl, query);
            }
        }
        return null;
    }
}
```

### 5.4 Files to Create

**New Files**:
- `Assets/[H3M]/Portals/Code/v3/Services/Speech/H3MSpeechRecognizer.cs`
- `Assets/[H3M]/Portals/Code/v3/Services/IcosaApi/H3MIcosaService.cs`
- `Assets/[H3M]/Portals/Code/v3/XR/VoiceToObject/VoiceToObjectController.cs`
- `Assets/[H3M]/Portals/Code/v3/Services/3DModelAPIs/I3DModelAPI.cs`
- `Assets/[H3M]/Portals/Code/v3/Services/3DModelAPIs/IcosaAPI.cs`
- `Assets/[H3M]/Portals/Code/v3/Services/3DModelAPIs/SketchfabAPI.cs`
- `Assets/[H3M]/Portals/Code/v3/Services/3DModelAPIs/ModelAPIManager.cs`
- `Assets/StreamingAssets/Whisper/ggml-tiny.bin` (39MB model file)

### 5.5 Testing Checklist

- [ ] Whisper package installs successfully
- [ ] Speech recognition works (test with "hello world")
- [ ] Icosa API returns search results
- [ ] glTF models download and instantiate
- [ ] Models spawn at correct AR positions
- [ ] Models scale appropriately (visible but not huge)
- [ ] End-to-end: Speak ‚Üí Search ‚Üí Spawn works
- [ ] Works with various objects (animals, furniture, vehicles)
- [ ] Network errors handled gracefully
- [ ] Loading indicator shows during download

**Total Time**: ~60-80 hours

---

## 6. ECHOVISION FUNCTIONALITY INTEGRATION

**Goal**: LiDAR depth visualization with sound wave effects

### 6.1 Key Features

From echovision-main project:
- **LiDAR Depth Imaging** - Uses iOS LiDAR scanner (AROcclusionManager)
- **Sound Wave Visualization** - Audio-reactive depth effects
- **Mesh VFX** - VFX Graph particles follow depth mesh

### 6.2 Core Components

1. **DepthImageProcessor.cs** - Processes ARKit depth texture
2. **AudioProcessor.cs** - FFT analysis for audio reactivity
3. **MeshVFX.cs** - VFX Graph integration with depth mesh
4. **SoundWaveEmitter.cs** - Emits sound waves that interact with depth

### 6.3 Implementation Steps (~48-64 hours)

#### **Phase 1: LiDAR Depth Integration** (16-20h)

**Step 1**: Copy DepthImageProcessor.cs
```bash
cp "/Users/jamestunick/wkspaces/Hologrm.Demos/echovision-main/Assets/Scripts/DepthImageProcessor.cs" \
   "/Users/jamestunick/wkspaces/Portals_6/Assets/[H3M]/Portals/Code/v3/XR/DepthProcessing/"
```

**Step 2**: Integrate with H3M AR system
```
MainMultiPlayerScene-Tools.unity:
  XR Origin
    ‚îú‚îÄ> AR Camera Manager (already exists)
    ‚îú‚îÄ> AR Occlusion Manager (already exists)
    ‚îî‚îÄ> DepthImageProcessor (NEW)
          ‚îú‚îÄ> Reference: AR Camera Manager
          ‚îî‚îÄ> Reference: AR Occlusion Manager
```

**Step 3**: Create depth visualization VFX Graph
- Unity Editor: Create new VFX Graph
- Name: `DepthParticles.vfxgraph`
- Location: `Assets/[H3M]/Portals/Content/VFX/Depth/`

**VFX Graph Setup**:
```
Spawn:
- Position: From depth texture (sample XY, use depth as Z)
- Rate: 10,000 particles/sec
- Lifetime: 2 seconds

Update:
- Color based on distance:
  - Near (0-1m): Blue
  - Mid (1-3m): Green
  - Far (3-5m): Red
- Size: 0.005 - 0.02m

Output:
- Particle Quad
- Blending: Additive
```

**Step 4**: Test depth visualization on iPhone with LiDAR

#### **Phase 2: Sound Wave Effects** (16-24h)

**Step 5**: Adapt SoundWaveEmitter for H3M
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Audio/SoundWaveEmitter.cs
using UnityEngine;

namespace H3M.Audio
{
    public class SoundWaveEmitter : MonoBehaviour
    {
        [SerializeField] private AudioProcessor audioProcessor;
        [SerializeField] private DepthImageProcessor depthProcessor;

        [Header("Wave Settings")]
        [SerializeField] private float waveSpeed = 2.0f; // m/s
        [SerializeField] private float maxWaveRadius = 5.0f; // meters
        [SerializeField] private Material waveMaterial;

        private List<SoundWave> activeWaves = new List<SoundWave>();

        void Update()
        {
            // Emit wave on beat detection
            if (audioProcessor != null && audioProcessor.BeatDetected)
            {
                EmitWave(Camera.main.transform.position);
            }

            // Update active waves
            UpdateWaves();
        }

        public void EmitWave(Vector3 origin)
        {
            SoundWave wave = new SoundWave
            {
                origin = origin,
                currentRadius = 0f,
                lifetime = maxWaveRadius / waveSpeed
            };

            activeWaves.Add(wave);
        }

        private void UpdateWaves()
        {
            for (int i = activeWaves.Count - 1; i >= 0; i--)
            {
                var wave = activeWaves[i];
                wave.currentRadius += waveSpeed * Time.deltaTime;
                wave.lifetime -= Time.deltaTime;

                if (wave.lifetime <= 0 || wave.currentRadius >= maxWaveRadius)
                {
                    activeWaves.RemoveAt(i);
                }
                else
                {
                    // Interact with depth mesh
                    InteractWithDepth(wave);
                }
            }
        }

        private void InteractWithDepth(SoundWave wave)
        {
            // Use depth texture to reveal hidden depth contours
            // VFX particles spawn where wave intersects depth changes
        }

        private class SoundWave
        {
            public Vector3 origin;
            public float currentRadius;
            public float lifetime;
        }
    }
}
```

**Step 6**: Create composite effect
- User touches screen ‚Üí Emit sound wave
- Wave expands outward in circular pattern
- Wave reveals hidden depth contours (particle trail)
- VFX particles follow wave front

**Step 7**: Integrate with DepthImageProcessor

#### **Phase 3: Advanced Depth Effects** (16-20h)

**Step 8**: Implement effect variations
- **Depth Painting** - Paint with depth-aware brush (particles spawn at depth surface)
- **Depth Sculpting** - Modify virtual depth mesh with hand gestures
- **Depth Portals** - Create portals based on depth discontinuities (doorways, windows)

**Step 9**: Create DepthAwarePaintbrush.cs
```csharp
public class DepthAwarePaintbrush : EnchantedPaintbrush
{
    [SerializeField] private DepthImageProcessor depthProcessor;

    protected override void AddPoint(Vector3 point)
    {
        // Snap point to depth surface if nearby
        Vector3 depthCorrectedPoint = SnapToDepthSurface(point);
        base.AddPoint(depthCorrectedPoint);
    }

    private Vector3 SnapToDepthSurface(Vector3 point)
    {
        // Sample depth texture at point's screen position
        // Adjust point depth to match real-world surface
        return point;
    }
}
```

**Step 10**: Test all depth effects on iPhone with LiDAR

### 6.4 Files to Create

**New Files**:
- `Assets/[H3M]/Portals/Code/v3/XR/DepthProcessing/DepthImageProcessor.cs`
- `Assets/[H3M]/Portals/Code/v3/Audio/SoundWaveEmitter.cs`
- `Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/DepthAwarePaintbrush.cs`
- `Assets/[H3M]/Portals/Content/VFX/Depth/DepthParticles.vfxgraph`
- `Assets/[H3M]/Portals/Content/VFX/Depth/SoundWave.vfxgraph`

### 6.5 Testing Checklist

- [ ] Depth texture captured from LiDAR
- [ ] Depth visualization particles spawn correctly
- [ ] Color gradient shows distance accurately
- [ ] Sound waves emit on beat detection
- [ ] Waves interact with depth mesh
- [ ] Depth-aware painting works
- [ ] Frame rate ‚â•30 FPS
- [ ] Works in various lighting conditions

**Total Time**: ~48-64 hours

---

## 7. FUNGISYNC FUNCTIONALITY INTEGRATION

**Goal**: Multiplayer AR effects with hand gestures

### 7.1 Key Features

From fungisync-main project:
- **Multiplayer AR** - Unity Netcode for GameObjects
- **Hand Gesture Effects** - Spawn effects at gesture locations
- **Shared AR Meshing** - Synchronized world mesh across devices

### 7.2 Core Components

1. **Effect System** - 4 AR effects (Fungus, Crystal, Spike, Lantern)
2. **Network Synchronization** - Netcode for multiplayer
3. **Hand Gesture Recognition** - HoloKit hand gestures
4. **AR Meshing** - Shared mesh raycasting

### 7.3 Implementation Steps (~64-80 hours)

#### **Phase 1: Multiplayer Foundation** (20-28h)

**Step 1**: Adapt fungisync to Normcore architecture
- Portals_6 already uses Normcore 2.16.2
- Translate Netcode patterns to Normcore RealtimeComponents

**Step 2**: Create NetworkedEffectManager.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Network/Effects/NetworkedEffectManager.cs
using Normal.Realtime;
using UnityEngine;

namespace H3M.Network
{
    public class NetworkedEffectManager : RealtimeComponent<NetworkedEffectModel>
    {
        [SerializeField] private EffectBase[] effectPrefabs;

        public void SpawnEffect(EffectType type, Vector3 position, Quaternion rotation)
        {
            // Request effect spawn via model
            model.RequestSpawnEffect((int)type, position, rotation);
        }

        protected override void OnRealtimeModelReplaced(NetworkedEffectModel previousModel, NetworkedEffectModel currentModel)
        {
            if (previousModel != null)
            {
                previousModel.effectSpawned -= OnEffectSpawned;
            }

            if (currentModel != null)
            {
                currentModel.effectSpawned += OnEffectSpawned;
            }
        }

        private void OnEffectSpawned(int effectTypeInt, Vector3 position, Quaternion rotation)
        {
            EffectType effectType = (EffectType)effectTypeInt;

            // Instantiate effect locally for all clients
            EffectBase effectPrefab = effectPrefabs[(int)effectType];
            if (effectPrefab != null)
            {
                Instantiate(effectPrefab, position, rotation);
            }
        }
    }

    public enum EffectType
    {
        Fungus = 0,
        Crystal = 1,
        Spike = 2,
        Lantern = 3
    }
}
```

**Step 3**: Create NetworkedEffectModel
```csharp
// Create with Normcore Model Builder
// Fields:
// - int effectType
// - Vector3 spawnPosition
// - Quaternion spawnRotation
// Events:
// - effectSpawned(int, Vector3, Quaternion)
```

**Step 4**: Test multiplayer effect synchronization with 2 devices

#### **Phase 2: Hand Gesture Effects** (20-24h)

**Step 5**: Copy fungisync effect scripts
```bash
# Copy effect base classes
cp "/Users/jamestunick/wkspaces/Hologrm.Demos/fungisync-main/Assets/Scripts/Effect/EffectBase.cs" \
   "/Users/jamestunick/wkspaces/Portals_6/Assets/[H3M]/Portals/Code/v3/Effects/"

cp "/Users/jamestunick/wkspaces/Hologrm.Demos/fungisync-main/Assets/Scripts/Effect/Effect"*.cs \
   "/Users/jamestunick/wkspaces/Portals_6/Assets/[H3M]/Portals/Code/v3/Effects/"
```

**Step 6**: Integrate with HandGestureDetector (from Part 2)
```csharp
// Modify HandPaintTool.cs to support effect spawning
public class HandEffectTool : MonoBehaviour
{
    [SerializeField] private HandGestureDetector gestureDetector;
    [SerializeField] private NetworkedEffectManager effectManager;

    void HandleGesture(HandGestureDetector.GestureType type, XRHand hand)
    {
        if (type == HandGestureDetector.GestureType.Pinch)
        {
            // Get pinch position
            Vector3 position = GetPinchPosition(hand);
            Quaternion rotation = GetHandRotation(hand);

            // Spawn effect at pinch location
            effectManager.SpawnEffect(EffectType.Fungus, position, rotation);
        }
    }
}
```

**Step 7**: Adapt fungisync effects for H3M
- **Fungus Effect** - Grows mushrooms at pinch locations
- **Crystal Effect** - Spawns glowing crystals
- **Spike Effect** - Creates spiky protrusions from surfaces
- **Lantern Effect** - Floating lights that illuminate AR space

**Step 8**: Create effect prefabs in Unity Editor

#### **Phase 3: Shared AR Meshing** (16-20h)

**Step 9**: Synchronize AR mesh across devices
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Network/Mesh/SharedARMeshManager.cs
using Normal.Realtime;
using UnityEngine.XR.ARFoundation;

namespace H3M.Network
{
    public class SharedARMeshManager : RealtimeComponent<SharedARMeshModel>
    {
        [SerializeField] private ARMeshManager meshManager;

        private Dictionary<string, MeshData> sharedMeshes = new Dictionary<string, MeshData>();

        void Start()
        {
            meshManager.meshesChanged += OnMeshesChanged;
        }

        private void OnMeshesChanged(ARMeshesChangedEventArgs args)
        {
            // Send new/updated meshes to other clients
            foreach (var mesh in args.added)
            {
                BroadcastMeshData(mesh);
            }

            foreach (var mesh in args.updated)
            {
                BroadcastMeshData(mesh);
            }
        }

        private void BroadcastMeshData(MeshFilter mesh)
        {
            // Compress and send mesh data
            // Note: This is bandwidth-intensive, use sparingly
        }
    }
}
```

**Step 10**: Implement shared raycasting for effect placement
- All clients raycast against shared mesh
- Effects placed at consistent world positions

#### **Phase 4: Handshake Effect** (8-12h)

**Step 11**: Implement HandshakeEffect.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Effects/HandshakeEffect.cs
using UnityEngine;
using UnityEngine.XR.Hands;

namespace H3M.Effects
{
    public class HandshakeEffect : MonoBehaviour
    {
        [SerializeField] private HandGestureDetector localGestureDetector;
        [SerializeField] private GameObject handshakeVFX;
        [SerializeField] private float detectionRadius = 0.15f; // 15cm

        private List<Transform> remoteHandTransforms = new List<Transform>();

        void Update()
        {
            DetectHandshakes();
        }

        private void DetectHandshakes()
        {
            // Get local hand positions
            // Check distance to remote hand positions
            // If within radius ‚Üí Trigger handshake VFX
        }

        private void TriggerHandshakeVFX(Vector3 position)
        {
            GameObject vfx = Instantiate(handshakeVFX, position, Quaternion.identity);
            Destroy(vfx, 3.0f);
        }
    }
}
```

**Step 12**: Test handshake effect with 2 users

### 7.4 Files to Create

**New Files**:
- `Assets/[H3M]/Portals/Code/v3/Network/Effects/NetworkedEffectManager.cs`
- `Assets/[H3M]/Portals/Code/v3/Network/Effects/NetworkedEffectModel.cs`
- `Assets/[H3M]/Portals/Code/v3/Effects/EffectBase.cs`
- `Assets/[H3M]/Portals/Code/v3/Effects/Effect1_Fungus.cs`
- `Assets/[H3M]/Portals/Code/v3/Effects/Effect2_Crystal.cs`
- `Assets/[H3M]/Portals/Code/v3/Effects/Effect3_Spike.cs`
- `Assets/[H3M]/Portals/Code/v3/Effects/Effect4_Lantern.cs`
- `Assets/[H3M]/Portals/Code/v3/Effects/HandshakeEffect.cs`
- `Assets/[H3M]/Portals/Code/v3/Network/Mesh/SharedARMeshManager.cs`
- `Assets/[H3M]/Portals/Code/v3/XR/HandTracking/HandEffectTool.cs`

### 7.5 Testing Checklist

- [ ] Effects spawn at correct positions
- [ ] Effects synchronized across 2+ devices
- [ ] Hand gestures trigger effects reliably
- [ ] Handshake effect works between users
- [ ] Shared mesh raycasting consistent
- [ ] Frame rate ‚â•30 FPS with 2+ users
- [ ] Network latency acceptable (<200ms)
- [ ] No memory leaks during 15-minute session

**Total Time**: ~64-80 hours

---


## 8. H3M HOLOGRAM ROADMAP (CROSS-PLATFORM VOLUMETRIC VIDEO)

 **Priority**: P2 (Foundational Architecture)
 **Status**: üü° IN PROGRESS (Phase 1: Local Foundation)
 **Estimated Time**: Phased Rollout (Weeks 9-15+)
 **Dependencies**: AR Foundation 6.0, VFX Graph

 **Goal**: A simple, efficient mechanism for turning sparse video depth and audio from mobile devices, VR headsets, WebCams, and wearable devices into volumetric VFX graph or shader-based holograms.

 **Core Philosophy**:
 *   Infinitely Scalable (MMO style)
 *   Fidelity (Gaussian Splat-like)
 *   Reactivity (Audio, Physics)
 *   Efficiency (Fewest dependencies possible)

 **Phased Implementation Plan**:

 **Phase 1: Local Foundation (Current Focus)**
 *   **Target**: Single iOS Device (Local)
 *   **Input**: Local Camera + LiDAR Depth
 *   **Output**: On-screen Volumetric VFX (Rcam4 style)
 *   **Status**: Debugging "Empty Scene" on local build.

 **Phase 2: Peer-to-Peer Mobile**
 *   **Target**: Phone to Phone
 *   **Transport**: WebRTC (Unity WebRTC Package + Simple Signaling)
 *   **Data**: RGBD Video + Audio
 *   **Goal**: One-way or Two-way holographic streaming.

 **Phase 3: Web Integration**
 *   **Target**: Phone to WebGL
 *   **Goal**: View mobile holograms in a desktop/mobile browser.
 *   **Tech**: Unity WebRTC for WebGL (requires optimization).

 **Phase 4: Extended Inputs**
 *   **Target**: WebCam to Phone
 *   **Goal**: Desktop webcam depth estimation (BodyPix/Neural) streaming to mobile AR.

 **Phase 5: Full Web Interop**
 *   **Target**: Mobile Web Browser <-> iOS App
 *   **Goal**: Bi-directional streaming between native app and web client.

 **Phase 6: Conferencing**
 *   **Target**: >2 Users (Mesh Topology or SFU)
 *   **Goal**: Multi-user holographic chat.

 **Phase 7: VR/MR Integration**
 *   **Target**: Meta Quest (Passthrough)
 *   **Goal**: View holograms in mixed reality.
 *   **Tech**: Needle Engine (WebXR) or Native Unity Build.

 **Phase 8: Scale & Fidelity**
 *   **Target**: MMO Scale
 *   **Tech**: Gaussian Splats, Advanced Physics, Audio Reactivity.

 **Goal**: "RGBD capture ‚Üí 3D/4D/5D rendering with neural effects"

 **Key Components**:

 - Depth camera integration
 - Point cloud generation
 - Neural style transfer
 - Real-time depth effects

 **Reference Architectures Analyzed**:

 #### **Rcam4** (Keijiro, 2021)
 - **Purpose**: Real-time camera system for iOS LiDAR ‚Üí NDI streaming
 - **Key Tech**: ARCameraManager (RGB) + AROcclusionManager (depth) ‚Üí Shader multiplexer ‚Üí NDI output
 - **Metadata**: Camera position, rotation, projection matrix, depth range
 - **Streaming**: Klak.Ndi for low-latency NDI transmission
 - **Rendering**: Point cloud or VFX Graph particles on receiving client

 **File**: `/Users/jamestunick/Downloads/AI_Knowledge_Base_Setup/______VFX25/RCAMS/Rcam4-main 4/Rcam4Controller/Assets/Scripts/FrameEncoder.cs`

 **Critical Pattern**:
```csharp
void OnCameraFrameReceived(ARCameraFrameEventArgs args)
{
    // Extract Y/CbCr textures from AR Foundation
    for (var i = 0; i < args.textures.Count; i++)
    {
        var id = args.propertyNameIds[i];
        if (id == ShaderID.TextureY || id == ShaderID.TextureCbCr)
            _blitter.Material.SetTexture(id, args.textures[i]);
    }

    // Get projection matrix for depth unprojection
    if (args.projectionMatrix.HasValue)
    {
        _projMatrix = args.projectionMatrix.Value;
        // Aspect ratio compensation
        var texAspect = (float)tex1.width / tex1.height;
        var s = texAspect / _camera.aspect;
        _projMatrix[1, 1] *= s;
    }
}

void OnOcclusionFrameReceived(AROcclusionFrameEventArgs args)
{
    // Extract depth textures
    for (var i = 0; i < args.textures.Count; i++)
    {
        var id = args.propertyNameIds[i];
        if (id == ShaderID.HumanStencil ||
            id == ShaderID.EnvironmentDepth ||
            id == ShaderID.EnvironmentDepthConfidence)
            _blitter.Material.SetTexture(id, args.textures[i]);
    }
}
```

**Advantages**:
- ‚úÖ Event-driven (no polling)
- ‚úÖ Direct texture access (no CPU readback)
- ‚úÖ Shader-based multiplexing (GPU-efficient)
- ‚úÖ Metadata preservation for camera transform reconstruction

---

#### **Metavido** (Keijiro, 2023)
- **Purpose**: Record RGBD video to file, playback as holograms
- **Key Tech**: FrameEncoder (capture) + TextureDemuxer (playback) + Avfi VideoRecorder (H.265)
- **Format**: Multiplexed RGBA texture (RGB in RGB channels, Depth in Alpha)
- **Playback**: Instant preview via FrameFeeder (real-time demux while recording)

**File**: `/Users/jamestunick/Downloads/AI_Knowledge_Base_Setup/______VFX25/RCAMS/Metavido-main/Assets/Encoder/Runtime/AppController.cs`

**Critical Pattern**:
```csharp
void Start()
{
    // Recorder setup
    Recorder.source = (RenderTexture)_encoder.EncodedTexture;

    // Instant decoder setup (preview while recording)
    _feeder = new FrameFeeder(_decoder, _demuxer);
}

void Update()
{
    // Monitor update (show encoded frame in real-time)
    _feeder.AddFrame(_encoder.EncodedTexture);
    _feeder.Update();
}
```

**Advantages**:
- ‚úÖ File-based workflow (record RGBD, playback later)
- ‚úÖ Instant preview (see encoding results in real-time)
- ‚úÖ Standard video codecs (H.265 HEVC for compression)

---

#### **PeopleOcclusionVFXManager** (Portals Existing)
- **Purpose**: Extract human stencil + depth, feed to VFX Graph
- **Key Tech**: AROcclusionManager (humanStencilTexture, humanDepthTexture) + Compute Shader (depth‚Üíworld position) + VFX Graph
- **Output**: Position Map texture (world-space XYZ) + Color Map (RGB) + Stencil Map (binary mask)

**File**: `/Users/jamestunick/wkspaces/Portals_6/Assets/[H3M]/Portals/Content/__Paint_AR/_______VFX/_VFX/PeopleOcclusionVFXManager.cs`

**Critical Pattern**:
```csharp
void Update()
{
    Texture2D stencilTexture = m_OcclusionManager.humanStencilTexture;
    Texture2D depthTexture = m_OcclusionManager.humanDepthTexture;

    if (stencilTexture == null || depthTexture == null) return;

    // Compute world position from depth
    Matrix4x4 invVPMatrix = (m_Camera.projectionMatrix * m_Camera.transform.worldToLocalMatrix).inverse;

    m_ComputeShader.SetTexture(m_Kernel, "DepthTexture", depthTexture);
    m_ComputeShader.SetMatrix("InvVPMatrix", invVPMatrix);
    m_ComputeShader.Dispatch(m_Kernel, ...);

    // Feed to VFX Graph
    m_VfxInstance.SetTexture("Color Map", m_CaptureTexture);
    m_VfxInstance.SetTexture("Stencil Map", stencilTexture);
    m_VfxInstance.SetTexture("Position Map", m_PositionTexture); // Computed
}
```

**Compute Shader** (Depth ‚Üí World Position):
```hlsl
#pragma kernel GeneratePositionTexture

RWTexture2D<float4> PositionTexture;
Texture2D<float> DepthTexture;
float4x4 InvVPMatrix;

[numthreads(8,8,1)]
void GeneratePositionTexture(uint3 id : SV_DispatchThreadID)
{
    float depth = DepthTexture[id.xy].r;

    // Screen space to NDC
    float2 uv = id.xy / float2(width, height);
    float4 ndc = float4(uv * 2.0 - 1.0, depth, 1.0);

    // NDC to world space
    float4 worldPos = mul(InvVPMatrix, ndc);
    worldPos /= worldPos.w;

    PositionTexture[id.xy] = worldPos;
}
```

**Advantages**:
- ‚úÖ Already exists in Portals codebase
- ‚úÖ Compute shader approach (fast depth unprojection)
- ‚úÖ VFX Graph integration proven
- ‚ùå Only handles human segmentation (not full environment)

---

### 1.3 UNIVERSAL RGBD PIPELINE (PLUGIN ARCHITECTURE) ‚ö°

**Design Philosophy**: **Separation of Concerns** - Data capture/encoding is COMPLETELY decoupled from rendering

**Key Principle**: RGBD data is a **standardized interface** that ANY renderer can consume (VFX Graph, shaders, GSplat, mesh extrusion, raymarching, or future techniques)

---

#### **3-Layer Architecture** (Modular & Extensible)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 1: DATA SOURCES (Pluggable Inputs)                       ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ  Choose ONE or COMBINE multiple:                                ‚îÇ
‚îÇ  ‚îú‚îÄ Live iPhone Camera (ARCameraManager + AROcclusionManager)   ‚îÇ
‚îÇ  ‚îú‚îÄ Recorded RGBD Video (.mp4 H.265 with depth in alpha)        ‚îÇ
‚îÇ  ‚îú‚îÄ Synthetic RGBD (Unity Camera + Depth texture generation)    ‚îÇ
‚îÇ  ‚îú‚îÄ WebRTC Stream (Remote iPhone RGBD via network)              ‚îÇ
‚îÇ  ‚îî‚îÄ Image Sequence (PNG/EXR files with separate depth maps)     ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Output Format: RGB Texture + Depth Texture (separate OR muxed) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 2: STANDARDIZED RGBD FORMAT (Universal Data Contract)    ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ  H3MRGBDDataProvider (Abstract Interface)                        ‚îÇ
‚îÇ  ‚îú‚îÄ Properties (Read-Only):                                      ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ colorTexture : Texture (RGB or Y/CbCr)                   ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ depthTexture : Texture (Normalized 0-1 or meters)        ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ positionTexture : RenderTexture (World XYZ, optional)    ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ normalTexture : RenderTexture (World normals, optional)  ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ maskTexture : Texture (Binary stencil, optional)         ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ confidenceTexture : Texture (Depth confidence, optional) ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ metadata : RGBDMetadata (camera transform, depth range)  ‚îÇ
‚îÇ  ‚îÇ                                                                ‚îÇ
‚îÇ  ‚îú‚îÄ Methods:                                                     ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ bool IsDataReady() ‚Üí Check if textures valid this frame ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ void UpdateData() ‚Üí Refresh textures (called per frame)  ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ RGBDMetadata GetMetadata() ‚Üí Camera pose, depth range    ‚îÇ
‚îÇ  ‚îÇ                                                                ‚îÇ
‚îÇ  ‚îî‚îÄ Implementations (Swap-able):                                 ‚îÇ
‚îÇ      ‚îú‚îÄ LiveCameraRGBDProvider (AR Foundation)                   ‚îÇ
‚îÇ      ‚îú‚îÄ VideoFileRGBDProvider (H.265 playback)                   ‚îÇ
‚îÇ      ‚îú‚îÄ WebRTCRGBDProvider (Network stream)                      ‚îÇ
‚îÇ      ‚îî‚îÄ SyntheticRGBDProvider (Unity rendered)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 3: RENDERERS (Pluggable Outputs)                         ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ  ALL implement: IH3MHologramRenderer                             ‚îÇ
‚îÇ  ‚îú‚îÄ VFXGraphRenderer (Particle systems)                          ‚îÇ
‚îÇ  ‚îú‚îÄ PointCloudRenderer (Shader-based quads/billboards)           ‚îÇ
‚îÇ  ‚îú‚îÄ GSplatRenderer (Gaussian splatting - smooth volumetrics)     ‚îÇ
‚îÇ  ‚îú‚îÄ MeshExtrusionRenderer (Depth-based mesh generation)          ‚îÇ
‚îÇ  ‚îú‚îÄ RaymarchRenderer (Volumetric shader raymarching)             ‚îÇ
‚îÇ  ‚îú‚îÄ HolosphereRenderer (360¬∞ equirectangular mapping)            ‚îÇ
‚îÇ  ‚îî‚îÄ CustomRenderer (User-defined - just implement interface!)    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  HOW TO ADD NEW RENDERER: 3 simple steps (see section 1.4)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Advantage**: Add new rendering techniques WITHOUT touching data capture code!

**Example**: Want to try neural radiance fields (NeRF)? Just create `NeRFRenderer : IH3MHologramRenderer` - no changes to capture/encoding needed!

---

#### **Standard Data Contract** (RGBDMetadata)

```csharp
// Assets/[H3M]/Portals/Code/v3/XR/Holograms/RGBDMetadata.cs
using UnityEngine;

namespace H3M.Holograms
{
    /// <summary>
    /// Metadata for RGBD frame (camera transform + depth range)
    /// </summary>
    public struct RGBDMetadata
    {
        // Camera pose (world space)
        public Vector3 cameraPosition;
        public Quaternion cameraRotation;
        public Matrix4x4 projectionMatrix;
        public Matrix4x4 viewMatrix;

        // Depth range (meters)
        public float minDepth; // Typically 0.2m (too close = invalid)
        public float maxDepth; // User-adjustable (1-10m)

        // Texture dimensions
        public Vector2Int colorResolution; // e.g. 1920x1080
        public Vector2Int depthResolution; // e.g. 256x192

        // Timestamp (for synchronization)
        public double timestamp;

        // Optional: Segmentation type
        public SegmentationType segmentation; // None, Body, Hands, Custom
    }

    public enum SegmentationType
    {
        None,           // Full environment
        HumanBody,      // Human stencil mask
        Hands,          // Hand joint bounding box
        CustomMask      // User-provided mask
    }
}
```

---

#### **Universal Data Provider Interface**

```csharp
// Assets/[H3M]/Portals/Code/v3/XR/Holograms/IH3MRGBDDataProvider.cs
using UnityEngine;

namespace H3M.Holograms
{
    /// <summary>
    /// Universal interface for RGBD data sources
    /// Implement this to create new data providers (live camera, video file, network stream, etc.)
    /// </summary>
    public interface IH3MRGBDDataProvider
    {
        // ===== REQUIRED TEXTURES =====
        /// <summary>RGB color texture (may be Y/CbCr format - renderers must handle)</summary>
        Texture ColorTexture { get; }

        /// <summary>Depth texture (normalized 0-1 or raw meters - check metadata)</summary>
        Texture DepthTexture { get; }

        // ===== OPTIONAL TEXTURES (Computed on-demand) =====
        /// <summary>World-space position texture (XYZ). Null if not computed.</summary>
        RenderTexture PositionTexture { get; }

        /// <summary>World-space normal texture. Null if not computed.</summary>
        RenderTexture NormalTexture { get; }

        /// <summary>Binary mask texture (0 = discard, 1 = emit). Null if no segmentation.</summary>
        Texture MaskTexture { get; }

        /// <summary>Depth confidence texture (0-2: low/med/high). Null if not available.</summary>
        Texture ConfidenceTexture { get; }

        // ===== METADATA =====
        /// <summary>Camera transform, depth range, resolution, etc.</summary>
        RGBDMetadata Metadata { get; }

        // ===== LIFECYCLE =====
        /// <summary>Check if data is ready for current frame</summary>
        bool IsDataReady();

        /// <summary>Update textures (call once per frame in Update())</summary>
        void UpdateData();

        /// <summary>Cleanup resources</summary>
        void Dispose();
    }
}
```

**Why This Design?**
- ‚úÖ **Swap data sources** without changing renderer code
- ‚úÖ **Lazy computation** (Position/Normal textures only if renderer needs them)
- ‚úÖ **Optional features** (Mask/Confidence textures for advanced use cases)
- ‚úÖ **Metadata encapsulation** (Renderers get all context they need)

---

#### **Example: Live Camera Data Provider**

```csharp
// Assets/[H3M]/Portals/Code/v3/XR/Holograms/Providers/LiveCameraRGBDProvider.cs
using UnityEngine;
using UnityEngine.XR.ARFoundation;

namespace H3M.Holograms
{
    /// <summary>
    /// RGBD data from iPhone camera (AR Foundation)
    /// </summary>
    public class LiveCameraRGBDProvider : MonoBehaviour, IH3MRGBDDataProvider
    {
        [SerializeField] ARCameraManager cameraManager;
        [SerializeField] AROcclusionManager occlusionManager;
        [SerializeField] bool computePositionTexture = true;
        [SerializeField] ComputeShader depthToPositionShader;

        // Cached textures
        private Texture2D _colorY, _colorCbCr, _depth, _stencil, _confidence;
        private RenderTexture _position, _normal;
        private RGBDMetadata _metadata;
        private Camera _camera;

        // Interface implementation
        public Texture ColorTexture => _colorY; // YCbCr handled by renderers
        public Texture DepthTexture => _depth;
        public RenderTexture PositionTexture => _position;
        public RenderTexture NormalTexture => _normal;
        public Texture MaskTexture => _stencil;
        public Texture ConfidenceTexture => _confidence;
        public RGBDMetadata Metadata => _metadata;

        void OnEnable()
        {
            _camera = cameraManager.GetComponent<Camera>();
            cameraManager.frameReceived += OnCameraFrame;
            occlusionManager.frameReceived += OnOcclusionFrame;

            if (computePositionTexture)
            {
                _position = new RenderTexture(256, 192, 0, RenderTextureFormat.ARGBFloat);
                _position.enableRandomWrite = true;
                _position.Create();
            }
        }

        void OnDisable()
        {
            cameraManager.frameReceived -= OnCameraFrame;
            occlusionManager.frameReceived -= OnOcclusionFrame;
        }

        void OnCameraFrame(ARCameraFrameEventArgs args)
        {
            // Extract RGB textures (implementation from section 2)
            // ... (code omitted for brevity - see full section 2)

            // Update metadata
            _metadata.cameraPosition = _camera.transform.position;
            _metadata.cameraRotation = _camera.transform.rotation;
            if (args.projectionMatrix.HasValue)
            {
                _metadata.projectionMatrix = args.projectionMatrix.Value;
                _metadata.viewMatrix = _camera.worldToCameraMatrix;
            }
            _metadata.timestamp = Time.timeAsDouble;
        }

        void OnOcclusionFrame(AROcclusionFrameEventArgs args)
        {
            // Extract depth textures (implementation from section 2)
            // ... (code omitted for brevity)
        }

        public bool IsDataReady()
        {
            return _colorY != null && _depth != null;
        }

        public void UpdateData()
        {
            if (!IsDataReady()) return;

            // Optionally compute position texture from depth
            if (computePositionTexture && _position != null)
            {
                ComputePositionTexture();
            }
        }

        void ComputePositionTexture()
        {
            int kernel = depthToPositionShader.FindKernel("DepthToPosition");
            depthToPositionShader.SetTexture(kernel, "DepthTexture", _depth);
            depthToPositionShader.SetTexture(kernel, "PositionTexture", _position);
            depthToPositionShader.SetMatrix("InvVPMatrix", (_metadata.projectionMatrix * _metadata.viewMatrix).inverse);
            depthToPositionShader.Dispatch(kernel, 32, 24, 1);
        }

        public void Dispose()
        {
            if (_position != null) _position.Release();
            if (_normal != null) _normal.Release();
        }
    }
}
```

**Benefits**:
- ‚úÖ Clean separation (AR Foundation code isolated here)
- ‚úÖ Renderers don't need to know about ARCameraManager
- ‚úÖ Easy to swap for VideoFileRGBDProvider without changing renderers

---

### 1.4 RENDERER PLUGIN INTERFACE

**Goal**: ANY rendering technique can consume RGBD data by implementing simple interface

```csharp
// Assets/[H3M]/Portals/Code/v3/XR/Holograms/IH3MHologramRenderer.cs
using UnityEngine;

namespace H3M.Holograms
{
    /// <summary>
    /// Universal interface for hologram renderers
    /// Implement this to create new rendering techniques (VFX, shader, GSplat, etc.)
    /// </summary>
    public interface IH3MHologramRenderer
    {
        // ===== SETUP =====
        /// <summary>Set the RGBD data source for this renderer</summary>
        void SetDataProvider(IH3MRGBDDataProvider provider);

        /// <summary>Initialize renderer resources (called once)</summary>
        void Initialize();

        // ===== RENDERING =====
        /// <summary>Update rendering for current frame (called every frame)</summary>
        void UpdateRendering();

        /// <summary>Enable/disable rendering</summary>
        bool IsEnabled { get; set; }

        // ===== QUALITY SETTINGS =====
        /// <summary>Set quality level (0 = low, 1 = medium, 2 = high)</summary>
        void SetQualityLevel(int level);

        /// <summary>Get estimated particle/vertex count for current quality</summary>
        int GetEstimatedElementCount();

        // ===== CLEANUP =====
        /// <summary>Release resources</summary>
        void Dispose();
    }
}
```

---

#### **HOW TO ADD NEW RENDERER (3 Simple Steps)**

**Example**: Adding a hypothetical "Neural Radiance Field" renderer

**Step 1**: Create class implementing interface
```csharp
public class NeRFRenderer : MonoBehaviour, IH3MHologramRenderer
{
    private IH3MRGBDDataProvider _dataProvider;
    private NeRFModel _model; // Your custom NeRF implementation

    public void SetDataProvider(IH3MRGBDDataProvider provider)
    {
        _dataProvider = provider;
    }

    public void Initialize()
    {
        _model = new NeRFModel();
    }

    public void UpdateRendering()
    {
        if (!_dataProvider.IsDataReady()) return;

        // Feed RGBD data to NeRF model
        _model.Train(_dataProvider.ColorTexture, _dataProvider.DepthTexture);
        _model.Render();
    }

    // ... implement remaining interface methods
}
```

**Step 2**: Register renderer in H3MHologramManager
```csharp
H3MHologramManager manager = GetComponent<H3MHologramManager>();
manager.RegisterRenderer("NeRF", new NeRFRenderer());
```

**Step 3**: Select renderer at runtime
```csharp
manager.SetActiveRenderer("NeRF");
```

**That's it!** No changes to data capture, encoding, or other renderers.

---

### 1.5 3D vs 4D vs 5D RENDERING MODES

**Dimension Breakdown**:
- **3D (Spatial)**: Static volumetric data (single frame, no animation)
- **4D (Temporal)**: Animated volumetric data (video playback, temporal coherence)
- **5D (Interactive)**: Real-time interactive volumetrics (user input affects rendering)

---

#### **3D Mode (Static Holograms)**

**Use Case**: Capture single RGBD frame, render as frozen hologram

**Pipeline**:
```
Single RGBD Frame (photo)
  ‚Üí Encode to textures
  ‚Üí Render once (no updates)
  ‚Üí User can move around static hologram
```

**Implementation**:
```csharp
public class StaticHologramController : MonoBehaviour
{
    [SerializeField] IH3MRGBDDataProvider dataProvider;
    [SerializeField] IH3MHologramRenderer renderer;

    void Start()
    {
        // Capture single frame
        dataProvider.UpdateData();

        // Set to renderer (one-time)
        renderer.SetDataProvider(dataProvider);
        renderer.Initialize();
        renderer.UpdateRendering();

        // Freeze (no further updates)
        dataProvider.enabled = false;
    }
}
```

**Use Cases**:
- 3D selfies (capture person as hologram statue)
- Room scan snapshots (freeze spatial reconstruction)
- Holographic photos (RGBD still images)

---

#### **4D Mode (Animated Holograms - Temporal)**

**Use Case**: Playback recorded RGBD video, temporal coherence between frames

**Pipeline**:
```
RGBD Video File (.mp4 H.265)
  ‚Üí Decode frame-by-frame
  ‚Üí Update renderer every frame
  ‚Üí Smooth temporal transitions (optional)
```

**Implementation**:
```csharp
public class AnimatedHologramController : MonoBehaviour
{
    [SerializeField] VideoFileRGBDProvider videoProvider; // Plays .mp4
    [SerializeField] IH3MHologramRenderer renderer;
    [SerializeField] bool temporalSmoothing = true;

    void Update()
    {
        // Update video frame
        videoProvider.UpdateData();

        if (videoProvider.IsDataReady())
        {
            // Optional: Smooth transitions between frames
            if (temporalSmoothing)
            {
                ApplyTemporalFilter();
            }

            // Render updated frame
            renderer.UpdateRendering();
        }
    }

    void ApplyTemporalFilter()
    {
        // Blend with previous frame to reduce flicker
        // ... (implementation specific to renderer)
    }
}
```

**Use Cases**:
- Holographic recordings (record performance, playback later)
- Looping hologram animations (10-30 second loops)
- Cinematic holograms (pre-recorded sequences)

---

#### **5D Mode (Interactive Holograms - Real-time + User Input)**

**Use Case**: Live RGBD capture with user interaction (touch, gesture, audio reactivity)

**Pipeline**:
```
Live iPhone Camera (RGBD)
  ‚Üí Real-time encoding (60 FPS)
  ‚Üí User input modulates rendering
      ‚îú‚îÄ Touch: Change hologram color/size
      ‚îú‚îÄ Gesture: Trigger effects
      ‚îú‚îÄ Audio: Modulate particle emission
      ‚îî‚îÄ Hand tracking: Sculpt hologram
  ‚Üí Render with modulations
```

**Implementation**:
```csharp
public class InteractiveHologramController : MonoBehaviour
{
    [SerializeField] LiveCameraRGBDProvider liveProvider;
    [SerializeField] VFXGraphRenderer vfxRenderer; // Or any renderer
    [SerializeField] AudioReactiveModulator audioModulator; // Section 4
    [SerializeField] HandGestureDetector handGestures; // Section 2

    void Update()
    {
        // Update live RGBD data
        liveProvider.UpdateData();

        // Modulate rendering based on user input
        if (handGestures.IsPinching())
        {
            Vector3 pinchPos = handGestures.GetPinchPosition();
            vfxRenderer.SetInteractionPoint(pinchPos); // Custom method
        }

        if (audioModulator.IsBeatDetected())
        {
            vfxRenderer.TriggerPulse(); // Particles pulse with music
        }

        // Render with modulations
        vfxRenderer.UpdateRendering();
    }
}
```

**Use Cases**:
- Interactive hologram painting (Section 0 + Section 2 integration)
- Audio-reactive holograms (Section 4 - music visualizer)
- Gesture-controlled holograms (sculpt with hands)
- Multiplayer holograms (Section 10 - remote user interaction)

---

#### **Mode Comparison Table**

| Dimension | Data Source | Update Rate | Interaction | Use Case Examples |
|-----------|-------------|-------------|-------------|-------------------|
| **3D** | Single frame | 0 FPS (static) | None | Holographic photos, 3D selfies |
| **4D** | Recorded video | 30-60 FPS | None | Playback recordings, looping animations |
| **5D** | Live camera | 60 FPS | Real-time | Painting, music visualizer, multiplayer |

**Technical Difference**:
- **3D**: `UpdateRendering()` called once
- **4D**: `UpdateRendering()` called every frame, data from video file
- **5D**: `UpdateRendering()` called every frame, data from live camera + user input modulation

---

#### **Unified Controller (Supports All Modes)**

```csharp
// Assets/[H3M]/Portals/Code/v3/XR/Holograms/H3MHologramManager.cs
using UnityEngine;
using System.Collections.Generic;

namespace H3M.Holograms
{
    public enum HologramMode { Static3D, Animated4D, Interactive5D }

    public class H3MHologramManager : MonoBehaviour
    {
        [Header("Mode Selection")]
        [SerializeField] HologramMode mode = HologramMode.Interactive5D;

        [Header("Data Providers (Assign based on mode)")]
        [SerializeField] LiveCameraRGBDProvider liveProvider;      // For 5D
        [SerializeField] VideoFileRGBDProvider videoProvider;      // For 4D
        [SerializeField] StaticRGBDProvider staticProvider;        // For 3D

        [Header("Renderers (Swap-able)")]
        [SerializeField] List<GameObject> rendererObjects; // Each has IH3MHologramRenderer
        [SerializeField] int activeRendererIndex = 0;

        private IH3MRGBDDataProvider _activeProvider;
        private IH3MHologramRenderer _activeRenderer;

        void Start()
        {
            // Select data provider based on mode
            _activeProvider = mode switch
            {
                HologramMode.Static3D => staticProvider,
                HologramMode.Animated4D => videoProvider,
                HologramMode.Interactive5D => liveProvider,
                _ => liveProvider
            };

            // Select renderer
            _activeRenderer = rendererObjects[activeRendererIndex].GetComponent<IH3MHologramRenderer>();

            // Initialize
            _activeRenderer.SetDataProvider(_activeProvider);
            _activeRenderer.Initialize();

            // For 3D mode, render once and stop
            if (mode == HologramMode.Static3D)
            {
                _activeProvider.UpdateData();
                _activeRenderer.UpdateRendering();
                enabled = false; // Stop Update() calls
            }
        }

        void Update()
        {
            // 4D and 5D modes: Update every frame
            _activeProvider.UpdateData();

            if (_activeProvider.IsDataReady())
            {
                _activeRenderer.UpdateRendering();
            }
        }

        // Runtime renderer switching
        public void SwitchRenderer(int index)
        {
            _activeRenderer.Dispose();
            activeRendererIndex = index;
            _activeRenderer = rendererObjects[index].GetComponent<IH3MHologramRenderer>();
            _activeRenderer.SetDataProvider(_activeProvider);
            _activeRenderer.Initialize();
        }
    }
}
```

**Benefits**:
- ‚úÖ Single manager script for all modes
- ‚úÖ Runtime mode switching (3D ‚Üí 4D ‚Üí 5D)
- ‚úÖ Runtime renderer switching (VFX ‚Üí Shader ‚Üí GSplat)
- ‚úÖ No code changes needed to add new modes or renderers

---

**End of Section 1.3-1.5** - Universal RGBD Pipeline architecture complete!

**Key Takeaway**: RGBD data is the **universal interface**. Once you have it, you can render it ANY way you want (VFX, shader, GSplat, mesh, raymarching, etc.) without changing capture code.

---

### 8.2 AR FOUNDATION DATA EXTRACTION (RGBD CAPTURE)

### 2.1 ARCameraManager - RGB Extraction

**AR Foundation Callback Pattern**:
```csharp
using UnityEngine;
using UnityEngine.XR.ARFoundation;

public class H3MRGBDCapture : MonoBehaviour
{
    [SerializeField] ARCameraManager cameraManager;
    [SerializeField] AROcclusionManager occlusionManager;

    // Output textures (public for other systems to access)
    public Texture2D rgbTextureY { get; private set; }
    public Texture2D rgbTextureCbCr { get; private set; }
    public Texture2D depthTexture { get; private set; }
    public Texture2D stencilTexture { get; private set; }
    public Texture2D confidenceTexture { get; private set; }

    // Shader property IDs (cached for performance)
    static class ShaderIDs
    {
        public static readonly int TextureY = Shader.PropertyToID("_textureY");
        public static readonly int TextureCbCr = Shader.PropertyToID("_textureCbCr");
        public static readonly int HumanStencil = Shader.PropertyToID("_HumanStencil");
        public static readonly int HumanDepth = Shader.PropertyToID("_HumanDepth");
        public static readonly int EnvironmentDepth = Shader.PropertyToID("_EnvironmentDepth");
        public static readonly int EnvironmentDepthConfidence = Shader.PropertyToID("_EnvironmentDepthConfidence");
    }

    void OnEnable()
    {
        cameraManager.frameReceived += OnCameraFrameReceived;
        occlusionManager.frameReceived += OnOcclusionFrameReceived;
    }

    void OnDisable()
    {
        cameraManager.frameReceived -= OnCameraFrameReceived;
        occlusionManager.frameReceived -= OnOcclusionFrameReceived;
    }

    void OnCameraFrameReceived(ARCameraFrameEventArgs args)
    {
        // RGB textures are provided in Y/CbCr format (YUV420)
        // Y = Luminance (grayscale), CbCr = Chroma (color difference)
        // This format is used by iOS camera hardware for efficiency

        if (args.textures.Count == 0) return;

        for (int i = 0; i < args.textures.Count; i++)
        {
            int propertyID = args.propertyNameIds[i];

            if (propertyID == ShaderIDs.TextureY)
            {
                rgbTextureY = args.textures[i];
            }
            else if (propertyID == ShaderIDs.TextureCbCr)
            {
                rgbTextureCbCr = args.textures[i];
            }
        }

        // Projection matrix (needed for depth unprojection)
        if (args.projectionMatrix.HasValue)
        {
            projectionMatrix = args.projectionMatrix.Value;
        }
    }

    void OnOcclusionFrameReceived(AROcclusionFrameEventArgs args)
    {
        // Depth textures (LiDAR sensor data)

        if (args.textures.Count == 0) return;

        for (int i = 0; i < args.textures.Count; i++)
        {
            int propertyID = args.propertyNameIds[i];

            if (propertyID == ShaderIDs.HumanStencil)
            {
                // Binary mask: 255 = human, 0 = background
                stencilTexture = args.textures[i];
            }
            else if (propertyID == ShaderIDs.HumanDepth)
            {
                // Depth map of humans only (meters, 0-65535)
                // Only valid where stencil == 255
                depthTexture = args.textures[i];
            }
            else if (propertyID == ShaderIDs.EnvironmentDepth)
            {
                // Full environment depth (LiDAR, meters)
                depthTexture = args.textures[i];
            }
            else if (propertyID == ShaderIDs.EnvironmentDepthConfidence)
            {
                // Per-pixel confidence: 0=low, 1=medium, 2=high
                confidenceTexture = args.textures[i];
            }
        }
    }
}
```

**Key Insights**:
1. **Y/CbCr Format**: iOS camera outputs YUV420 (not RGB) for bandwidth efficiency
   - Y texture: 1920x1080 (luminance)
   - CbCr texture: 960x540 (chroma, half resolution)
   - Shader converts YCbCr ‚Üí RGB during rendering

2. **Depth Texture Resolution**: Environment depth is 256x192 (low-res but accurate)
   - LiDAR sensor limitation
   - Upscaling via shader interpolation recommended

3. **Stencil vs Depth**:
   - **HumanStencil**: Binary mask (fast, AI-generated)
   - **HumanDepth**: Depth only where stencil == 255 (sparse)
   - **EnvironmentDepth**: Full scene depth (dense, all pixels)

---

### 2.2 Depth Range & Normalization

**Problem**: LiDAR depth values are in meters (0.2m - 10m typical range), need normalized 0-1 for textures

**Solution**: Configurable depth range with clamping
```csharp
public class H3MDepthNormalizer
{
    public float minDepth = 0.2f; // 20cm minimum (too close = invalid)
    public float maxDepth = 5.0f; // 5 meters maximum (user-adjustable)

    public float NormalizeDepth(float depthMeters)
    {
        // Clamp to range
        float clamped = Mathf.Clamp(depthMeters, minDepth, maxDepth);

        // Normalize to 0-1
        return (clamped - minDepth) / (maxDepth - minDepth);
    }

    public float DenormalizeDepth(float depth01)
    {
        return depth01 * (maxDepth - minDepth) + minDepth;
    }
}
```

**Shader equivalent**:
```hlsl
float NormalizeDepth(float depthMeters, float2 depthRange)
{
    float minDepth = depthRange.x;
    float maxDepth = depthRange.y;
    float clamped = clamp(depthMeters, minDepth, maxDepth);
    return (clamped - minDepth) / (maxDepth - minDepth);
}
```

**UI Recommendation**: Slider for maxDepth (1m - 10m), automatically set minDepth = maxDepth / 50
- Close-up holograms: maxDepth = 2m
- Room-scale holograms: maxDepth = 5m
- Outdoor holograms: maxDepth = 10m

---

### 8.3 TEXTURE MULTIPLEXING & ENCODING (RGBD‚ÜíSingleTexture)

### 3.1 Why Multiplex?

**Problem**: Need to pass RGB (3 channels) + Depth (1 channel) = 4 channels total, but textures have 4 channels (RGBA)

**Solution**: Pack into single RGBA texture
- R: Red channel
- G: Green channel
- B: Blue channel
- A: Depth (normalized 0-1)

**Advantages**:
- ‚úÖ Single texture binding (faster than 2 separate textures)
- ‚úÖ Standard video codecs support RGBA (H.264, H.265)
- ‚úÖ WebRTC video streaming works out-of-box
- ‚úÖ GPU memory efficient (1 texture instead of 2)

---

### 3.2 YCbCr ‚Üí RGB Conversion Shader

**Shader** (`Assets/[H3M]/Portals/Code/v3/Shaders/YCbCrToRGB.shader`):
```hlsl
Shader "H3M/YCbCrToRGB"
{
    Properties
    {
        _TextureY("Y Texture", 2D) = "black" {}
        _TextureCbCr("CbCr Texture", 2D) = "gray" {}
    }

    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"

            sampler2D _TextureY;
            sampler2D _TextureCbCr;

            struct v2f
            {
                float4 pos : SV_POSITION;
                float2 uv : TEXCOORD0;
            };

            v2f vert(appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.uv = v.texcoord;
                return o;
            }

            float4 frag(v2f i) : SV_Target
            {
                // Sample Y (luminance) and CbCr (chroma)
                float y = tex2D(_TextureY, i.uv).r;
                float2 cbcr = tex2D(_TextureCbCr, i.uv).rg;

                // Convert YCbCr to RGB (ITU-R BT.709 standard)
                // https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.709_conversion
                float cb = cbcr.r - 0.5;
                float cr = cbcr.g - 0.5;

                float r = y + 1.5748 * cr;
                float g = y - 0.1873 * cb - 0.4681 * cr;
                float b = y + 1.8556 * cb;

                return float4(r, g, b, 1.0);
            }
            ENDCG
        }
    }
}
```

---

### 3.3 RGBD Multiplexer Shader

**Shader** (`Assets/[H3M]/Portals/Code/v3/Shaders/RGBDMultiplexer.shader`):
```hlsl
Shader "H3M/RGBDMultiplexer"
{
    Properties
    {
        _TextureY("Y Texture", 2D) = "black" {}
        _TextureCbCr("CbCr Texture", 2D) = "gray" {}
        _DepthTexture("Depth Texture", 2D) = "black" {}
        _DepthRange("Depth Range (min, max)", Vector) = (0.2, 5.0, 0, 0)
    }

    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"

            sampler2D _TextureY;
            sampler2D _TextureCbCr;
            sampler2D _DepthTexture;
            float2 _DepthRange;

            struct v2f
            {
                float4 pos : SV_POSITION;
                float2 uv : TEXCOORD0;
            };

            v2f vert(appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.uv = v.texcoord;
                return o;
            }

            float4 frag(v2f i) : SV_Target
            {
                // Convert YCbCr to RGB
                float y = tex2D(_TextureY, i.uv).r;
                float2 cbcr = tex2D(_TextureCbCr, i.uv).rg;

                float cb = cbcr.r - 0.5;
                float cr = cbcr.g - 0.5;

                float r = y + 1.5748 * cr;
                float g = y - 0.1873 * cb - 0.4681 * cr;
                float b = y + 1.8556 * cb;

                // Sample depth and normalize
                float depthMeters = tex2D(_DepthTexture, i.uv).r;
                float minDepth = _DepthRange.x;
                float maxDepth = _DepthRange.y;
                float depthNormalized = saturate((depthMeters - minDepth) / (maxDepth - minDepth));

                // Pack into RGBA (RGB = color, A = depth)
                return float4(r, g, b, depthNormalized);
            }
            ENDCG
        }
    }
}
```

**C# Usage**:
```csharp
public class H3MRGBDEncoder : MonoBehaviour
{
    [SerializeField] H3MRGBDCapture capture;
    [SerializeField] Material rgbdMultiplexerMaterial;
    [SerializeField] RenderTexture encodedOutput;

    void Update()
    {
        // Set shader inputs
        rgbdMultiplexerMaterial.SetTexture("_TextureY", capture.rgbTextureY);
        rgbdMultiplexerMaterial.SetTexture("_TextureCbCr", capture.rgbTextureCbCr);
        rgbdMultiplexerMaterial.SetTexture("_DepthTexture", capture.depthTexture);
        rgbdMultiplexerMaterial.SetVector("_DepthRange", new Vector2(minDepth, maxDepth));

        // Render to output texture
        Graphics.Blit(null, encodedOutput, rgbdMultiplexerMaterial);
    }
}
```

---

### 8.4 VFX GRAPH HOLOGRAM TECHNIQUES

### 4.1 Position Map Sampling Pattern

**VFX Graph**: Spawn particles from Position + Color textures

**Workflow**:
1. **Compute Shader**: Convert Depth texture ‚Üí Position texture (world-space XYZ)
2. **VFX Graph**: Sample Position texture to get particle spawn locations
3. **VFX Graph**: Sample Color texture (RGBD.rgb) to colorize particles

**Exposed Properties** (VFX Graph Blackboard):
```
Position Map (Texture2D) - World-space XYZ positions
Color Map (Texture2D) - RGB color
Mask Map (Texture2D, Optional) - Binary mask (0 = cull, 1 = emit)
Particle Count (int) - Number of particles to emit (default: 65536 = 256x256)
Particle Size (float) - Size multiplier (default: 0.01)
Depth Range (Vector2) - Min/max depth for culling
```

**VFX Graph Structure**:
```
Initialize Particle
‚îú‚îÄ Capacity: 65536 (256x256 texture = 65536 pixels)
‚îú‚îÄ Spawn: Burst (65536 particles on Start)
‚îî‚îÄ Set Position from Map (Custom HLSL)
    ‚îú‚îÄ Sample Position Map at UV = ParticleID / TextureSize
    ‚îú‚îÄ Set particle.position = sampledPosition
    ‚îî‚îÄ Cull if depth == 0 (no valid depth data)

Update Particle
‚îú‚îÄ Set Color from Map (Custom HLSL)
‚îÇ   ‚îú‚îÄ Sample Color Map at same UV as position
‚îÇ   ‚îî‚îÄ particle.color = sampledColor
‚îÇ
‚îî‚îÄ Cull by Mask (Optional)
    ‚îú‚îÄ Sample Mask Map
    ‚îî‚îÄ Kill particle if mask == 0

Output Particle Quad
‚îú‚îÄ Blend Mode: Additive or Alpha Blend
‚îú‚îÄ Size: Particle Size property
‚îî‚îÄ Color: From particle.color
```

**Custom HLSL** (Set Position from Map):
```hlsl
// VFX Graph Custom HLSL Block
Texture2D PositionMap;
SamplerState samplerPositionMap;
float2 TextureSize; // 256x192 for depth texture

void SetPositionFromMap(inout uint particleID, out float3 position, out float alpha)
{
    // Convert particle ID to texture UV
    uint x = particleID % uint(TextureSize.x);
    uint y = particleID / uint(TextureSize.x);
    float2 uv = float2(x, y) / TextureSize;

    // Sample position (world-space XYZ stored in RGB channels)
    float4 positionSample = PositionMap.SampleLevel(samplerPositionMap, uv, 0);
    position = positionSample.xyz;

    // Alpha channel can store depth for culling
    float depth = positionSample.w;
    alpha = (depth > 0.01) ? 1.0 : 0.0; // Cull if no valid depth
}
```

**Performance**: 65536 particles @ 60 FPS = ~4ms GPU time (acceptable on iPhone 13 Pro+)

---

### 4.2 Dynamic Particle Emission (Streaming Approach)

**Problem**: Static Burst emission doesn't update when RGBD textures change (live video)

**Solution**: Use **Periodic Burst** or **GPU Event** to respawn particles every frame

**VFX Graph Structure**:
```
Spawn Context
‚îú‚îÄ Spawn Mode: Periodic Burst
‚îú‚îÄ Rate: 60 bursts/sec (matches camera frame rate)
‚îî‚îÄ Count: 65536 particles per burst

Initialize Particle
‚îú‚îÄ Lifetime: 0.1 seconds (short lifetime = particles refresh quickly)
‚îî‚îÄ Set Position from Map (as above)
```

**Alternative**: **GPU Event** approach (more efficient)
```
GPU Event "UpdatePositions"
‚îú‚îÄ Trigger: Every frame (via C# script)
‚îî‚îÄ Update existing particles instead of respawning
    ‚îú‚îÄ Sample new Position Map
    ‚îî‚îÄ Update particle.position (smooth transition)
```

**C# GPU Event Trigger**:
```csharp
public class H3MVFXHologramRenderer : MonoBehaviour
{
    [SerializeField] VisualEffect vfx;
    [SerializeField] H3MRGBDEncoder encoder;

    void Update()
    {
        // Update VFX textures
        vfx.SetTexture("Position Map", encoder.positionTexture);
        vfx.SetTexture("Color Map", encoder.encodedOutput);

        // Trigger GPU event to update particle positions
        vfx.SendEvent("UpdatePositions");
    }
}
```

**Performance Comparison**:
| Method | GPU Time | Notes |
|--------|----------|-------|
| Static Burst | ~1ms | No updates, frozen hologram |
| Periodic Burst (60 Hz) | ~10ms | Respawns all particles each frame |
| GPU Event Update | ~3ms | Updates existing particles ‚úÖ RECOMMENDED |

---

### 8.5 SHADER-BASED VOLUMETRIC RENDERING

### 5.1 Point Cloud Mesh Approach

**Concept**: Generate quad mesh with 1 quad per pixel, position from Position texture, color from Color texture

**Mesh Generation** (C# Compute Shader approach):
```csharp
public class H3MPointCloudMeshGenerator : MonoBehaviour
{
    [SerializeField] ComputeShader meshGeneratorShader;
    [SerializeField] H3MRGBDEncoder encoder;

    private ComputeBuffer vertexBuffer;
    private ComputeBuffer indexBuffer;
    private Mesh pointCloudMesh;

    void Start()
    {
        int width = 256;
        int height = 192;
        int vertexCount = width * height * 4; // 4 vertices per quad
        int indexCount = width * height * 6; // 6 indices per quad (2 triangles)

        vertexBuffer = new ComputeBuffer(vertexCount, sizeof(float) * 10); // pos(3) + normal(3) + uv(2) + color(2)
        indexBuffer = new ComputeBuffer(indexCount, sizeof(int));

        pointCloudMesh = new Mesh();
        pointCloudMesh.indexFormat = UnityEngine.Rendering.IndexFormat.UInt32; // Support >65k vertices
    }

    void Update()
    {
        // Dispatch compute shader to generate vertices
        int kernel = meshGeneratorShader.FindKernel("GeneratePointCloudMesh");
        meshGeneratorShader.SetTexture(kernel, "PositionMap", encoder.positionTexture);
        meshGeneratorShader.SetTexture(kernel, "ColorMap", encoder.encodedOutput);
        meshGeneratorShader.SetBuffer(kernel, "VertexBuffer", vertexBuffer);
        meshGeneratorShader.SetBuffer(kernel, "IndexBuffer", indexBuffer);

        int threadGroups = Mathf.CeilToInt(256 / 8.0f);
        meshGeneratorShader.Dispatch(kernel, threadGroups, threadGroups, 1);

        // Update mesh from compute buffers
        pointCloudMesh.SetVertexBufferParams(vertexBuffer.count, /* layout */);
        pointCloudMesh.SetVertexBufferData(vertexBuffer, 0, 0, vertexBuffer.count);
        pointCloudMesh.SetIndexBufferParams(indexBuffer.count, UnityEngine.Rendering.IndexFormat.UInt32);
        pointCloudMesh.SetIndexBufferData(indexBuffer, 0, 0, indexBuffer.count);

        // Draw mesh
        Graphics.DrawMesh(pointCloudMesh, Matrix4x4.identity, hologramMaterial, 0);
    }
}
```

**Compute Shader** (`GeneratePointCloudMesh.compute`):
```hlsl
#pragma kernel GeneratePointCloudMesh

struct Vertex
{
    float3 position;
    float3 normal;
    float2 uv;
    float4 color;
};

RWStructuredBuffer<Vertex> VertexBuffer;
RWStructuredBuffer<int> IndexBuffer;
Texture2D<float4> PositionMap;
Texture2D<float4> ColorMap;
float ParticleSize;

[numthreads(8,8,1)]
void GeneratePointCloudMesh(uint3 id : SV_DispatchThreadID)
{
    uint width = 256;
    uint height = 192;

    if (id.x >= width || id.y >= height) return;

    uint pixelIndex = id.y * width + id.x;
    float2 uv = float2(id.x, id.y) / float2(width, height);

    // Sample position and color
    float4 positionSample = PositionMap[id.xy];
    float3 position = positionSample.xyz;
    float depth = positionSample.w;

    float4 color = ColorMap[id.xy];

    // Cull if no valid depth
    if (depth < 0.01)
    {
        // Write degenerate quad (zero-area triangles)
        uint baseVertexIndex = pixelIndex * 4;
        for (int i = 0; i < 4; i++)
        {
            VertexBuffer[baseVertexIndex + i].position = float3(0, 0, 0);
            VertexBuffer[baseVertexIndex + i].color = float4(0, 0, 0, 0);
        }
        return;
    }

    // Generate billboard quad facing camera
    float3 cameraPos = _WorldSpaceCameraPos;
    float3 toCamera = normalize(cameraPos - position);
    float3 right = normalize(cross(float3(0, 1, 0), toCamera)) * ParticleSize;
    float3 up = normalize(cross(toCamera, right)) * ParticleSize;

    // Quad vertices (billboard)
    uint baseVertexIndex = pixelIndex * 4;
    VertexBuffer[baseVertexIndex + 0].position = position - right - up; // Bottom-left
    VertexBuffer[baseVertexIndex + 1].position = position + right - up; // Bottom-right
    VertexBuffer[baseVertexIndex + 2].position = position + right + up; // Top-right
    VertexBuffer[baseVertexIndex + 3].position = position - right + up; // Top-left

    for (int i = 0; i < 4; i++)
    {
        VertexBuffer[baseVertexIndex + i].normal = toCamera;
        VertexBuffer[baseVertexIndex + i].color = color;
    }

    // Quad UVs
    VertexBuffer[baseVertexIndex + 0].uv = float2(0, 0);
    VertexBuffer[baseVertexIndex + 1].uv = float2(1, 0);
    VertexBuffer[baseVertexIndex + 2].uv = float2(1, 1);
    VertexBuffer[baseVertexIndex + 3].uv = float2(0, 1);

    // Indices (2 triangles per quad)
    uint baseIndexIndex = pixelIndex * 6;
    IndexBuffer[baseIndexIndex + 0] = baseVertexIndex + 0;
    IndexBuffer[baseIndexIndex + 1] = baseVertexIndex + 1;
    IndexBuffer[baseIndexIndex + 2] = baseVertexIndex + 2;
    IndexBuffer[baseIndexIndex + 3] = baseVertexIndex + 0;
    IndexBuffer[baseIndexIndex + 4] = baseVertexIndex + 2;
    IndexBuffer[baseIndexIndex + 5] = baseVertexIndex + 3;
}
```

**Performance**: 256x192 = 49,152 quads = 98,304 triangles @ 60 FPS = ~8ms GPU time

---

### 8.6 HUMAN BODY HOLOGRAMS (SEGMENTED CAPTURE)

### 6.1 Body Segmentation Pipeline

**Goal**: Capture only human body as hologram (exclude background)

**Pipeline**:
```
ARCameraManager (RGB) + AROcclusionManager (Depth + Stencil)
  ‚Üì
Human Stencil Texture (Binary mask: 255 = human, 0 = background)
  ‚Üì
Masked RGBD Encoder
  ‚îú‚îÄ If stencil == 0: discard pixel (set alpha = 0)
  ‚îî‚îÄ If stencil == 255: encode RGB + Depth
  ‚Üì
VFX Graph OR Shader Renderer
  ‚îî‚îÄ Emit particles only where alpha > 0
```

**Shader** (`RGBDMultiplexer_BodyMask.shader`):
```hlsl
Shader "H3M/RGBDMultiplexer_BodyMask"
{
    Properties
    {
        _TextureY("Y Texture", 2D) = "black" {}
        _TextureCbCr("CbCr Texture", 2D) = "gray" {}
        _DepthTexture("Depth Texture", 2D) = "black" {}
        _StencilTexture("Stencil Texture", 2D) = "black" {}
        _DepthRange("Depth Range", Vector) = (0.2, 5.0, 0, 0)
    }

    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            sampler2D _TextureY, _TextureCbCr, _DepthTexture, _StencilTexture;
            float2 _DepthRange;

            float4 frag(v2f i) : SV_Target
            {
                // Sample stencil (binary mask)
                float stencil = tex2D(_StencilTexture, i.uv).r;

                // Discard if not human
                if (stencil < 0.5) discard; // Background pixel

                // Convert YCbCr to RGB
                float y = tex2D(_TextureY, i.uv).r;
                float2 cbcr = tex2D(_TextureCbCr, i.uv).rg;
                float cb = cbcr.r - 0.5;
                float cr = cbcr.g - 0.5;

                float r = y + 1.5748 * cr;
                float g = y - 0.1873 * cb - 0.4681 * cr;
                float b = y + 1.8556 * cb;

                // Sample depth
                float depth = tex2D(_DepthTexture, i.uv).r;
                float depthNorm = saturate((depth - _DepthRange.x) / (_DepthRange.y - _DepthRange.x));

                return float4(r, g, b, depthNorm);
            }
            ENDCG
        }
    }
}
```

**Use Case**: Multiplayer human holograms (see Section 10 - Normcore + WebRTC integration)

---

### 8.7 HAND HOLOGRAMS (GESTURE VOLUMETRICS)

### 7.1 Hand Segmentation via XR Hands

**Goal**: Isolate hands from full-body hologram for gesture-based effects

**Pipeline**:
```
XR Hands (21 joints per hand)
  ‚Üì
Generate Hand Bounding Box (compute min/max XYZ of all joints)
  ‚Üì
Create Binary Mask Texture
  ‚îú‚îÄ For each pixel in depth texture:
  ‚îÇ   ‚îú‚îÄ Unproject to world position
  ‚îÇ   ‚îú‚îÄ Check if inside hand bounding box
  ‚îÇ   ‚îî‚îÄ Set mask = 1 if inside, 0 if outside
  ‚îî‚îÄ Output: Hand mask texture (256x192)
  ‚Üì
Masked RGBD Encoder (same as body hologram)
  ‚îî‚îÄ Emit particles only where mask == 1
```

**C# Implementation**:
```csharp
using UnityEngine.XR.Hands;

public class H3MHandMaskGenerator : MonoBehaviour
{
    [SerializeField] XRHandSubsystem handSubsystem;
    [SerializeField] ComputeShader handMaskShader;
    [SerializeField] H3MRGBDCapture capture;

    public RenderTexture handMaskTexture { get; private set; }

    void Start()
    {
        handMaskTexture = new RenderTexture(256, 192, 0, RenderTextureFormat.R8);
        handMaskTexture.enableRandomWrite = true;
        handMaskTexture.Create();
    }

    void Update()
    {
        // Get hand joint positions
        Vector3[] leftHandJoints = GetHandJoints(Handedness.Left);
        Vector3[] rightHandJoints = GetHandJoints(Handedness.Right);

        if (leftHandJoints == null && rightHandJoints == null) return;

        // Compute bounding boxes
        Bounds leftBounds = ComputeBounds(leftHandJoints);
        Bounds rightBounds = ComputeBounds(rightHandJoints);

        // Generate mask via compute shader
        int kernel = handMaskShader.FindKernel("GenerateHandMask");
        handMaskShader.SetTexture(kernel, "DepthTexture", capture.depthTexture);
        handMaskShader.SetTexture(kernel, "HandMaskOutput", handMaskTexture);
        handMaskShader.SetVector("LeftHandMin", leftBounds.min);
        handMaskShader.SetVector("LeftHandMax", leftBounds.max);
        handMaskShader.SetVector("RightHandMin", rightBounds.min);
        handMaskShader.SetVector("RightHandMax", rightBounds.max);

        handMaskShader.Dispatch(kernel, 32, 24, 1); // 256/8 = 32, 192/8 = 24
    }

    Vector3[] GetHandJoints(Handedness hand)
    {
        XRHand xrHand = (hand == Handedness.Left) ? handSubsystem.leftHand : handSubsystem.rightHand;
        if (!xrHand.isTracked) return null;

        Vector3[] joints = new Vector3[21];
        for (int i = 0; i < 21; i++)
        {
            XRHandJoint joint = xrHand.GetJoint((XRHandJointID)i);
            if (joint.TryGetPose(out Pose pose))
            {
                joints[i] = pose.position;
            }
        }
        return joints;
    }

    Bounds ComputeBounds(Vector3[] joints)
    {
        if (joints == null || joints.Length == 0) return new Bounds(Vector3.zero, Vector3.zero);

        Vector3 min = joints[0];
        Vector3 max = joints[0];

        foreach (Vector3 joint in joints)
        {
            min = Vector3.Min(min, joint);
            max = Vector3.Max(max, joint);
        }

        // Expand bounds slightly (5cm padding)
        min -= Vector3.one * 0.05f;
        max += Vector3.one * 0.05f;

        return new Bounds((min + max) / 2, max - min);
    }
}
```

**Compute Shader** (`GenerateHandMask.compute`):
```hlsl
#pragma kernel GenerateHandMask

Texture2D<float> DepthTexture;
RWTexture2D<float> HandMaskOutput;
float3 LeftHandMin, LeftHandMax;
float3 RightHandMin, RightHandMax;
float4x4 InvVPMatrix;

[numthreads(8,8,1)]
void GenerateHandMask(uint3 id : SV_DispatchThreadID)
{
    // Sample depth
    float depth = DepthTexture[id.xy];

    if (depth < 0.01)
    {
        HandMaskOutput[id.xy] = 0.0; // No depth data
        return;
    }

    // Unproject to world position
    float2 uv = id.xy / float2(256, 192);
    float4 ndc = float4(uv * 2.0 - 1.0, depth, 1.0);
    float4 worldPos = mul(InvVPMatrix, ndc);
    worldPos /= worldPos.w;

    // Check if inside left hand bounding box
    bool inLeftHand = all(worldPos.xyz >= LeftHandMin) && all(worldPos.xyz <= LeftHandMax);

    // Check if inside right hand bounding box
    bool inRightHand = all(worldPos.xyz >= RightHandMin) && all(worldPos.xyz <= RightHandMax);

    HandMaskOutput[id.xy] = (inLeftHand || inRightHand) ? 1.0 : 0.0;
}
```

**Use Case**: Hand gesture painting (Section 2 integration), hand-based VFX triggers (Section 4)

---

### 8.8 ENVIRONMENT HOLOGRAMS (SPATIAL RECONSTRUCTION)

#### 8.8.1 Full Environment Capture

**Goal**: Capture entire room/environment as volumetric hologram (no segmentation)

**Pipeline**:
```
ARCameraManager (RGB) + AROcclusionManager (Environment Depth)
  ‚Üì
RGBD Multiplexer (no masking)
  ‚îî‚îÄ Encode all pixels (RGB + full environment depth)
  ‚Üì
VFX Graph OR Shader Renderer
  ‚îî‚îÄ Emit particles for entire scene
```

**Shader**: Same as Section 3.3 (RGBDMultiplexer), but use `EnvironmentDepth` instead of `HumanDepth`

**Use Cases**:
- Room-scale spatial reconstruction
- Environment recording for later playback
- Mixed reality portals (see Echovision/Fungisync sections)

---

#### 8.8.2 Depth Confidence Filtering

**Problem**: LiDAR depth can have low-confidence pixels (reflective surfaces, transparent objects)

**Solution**: Filter by `EnvironmentDepthConfidence` texture

**Shader Addition**:
```hlsl
Texture2D _ConfidenceTexture;

float4 frag(v2f i) : SV_Target
{
    // Sample confidence (0=low, 1=medium, 2=high)
    float confidence = tex2D(_ConfidenceTexture, i.uv).r;

    // Only emit particles for medium/high confidence
    if (confidence < 1.0) discard;

    // ... rest of shader (RGB + Depth encoding)
}
```

**Performance Impact**: ~30% fewer particles emitted (higher quality, better performance)

---

### 8.9 HOLOSPHERES (360¬∞ RGBD ENVIRONMENTS)

#### 8.9.1 Equirectangular Mapping Concept

**Goal**: Convert RGBD video to 360¬∞ spherical environment (like 360 videos but volumetric)

**Pipeline**:
```
RGBD Video (recorded or live)
  ‚Üì
Reproject to Equirectangular Format (2:1 aspect ratio texture)
  ‚îú‚îÄ Depth ‚Üí Sphere radius
  ‚îî‚îÄ RGB ‚Üí Sphere color
  ‚Üì
Map to Sphere Mesh OR VFX Graph Sphere Emission
  ‚îî‚îÄ User stands inside sphere (immersive 360¬∞ hologram)
```

**Shader** (`RGBDToEquirectangular.shader`):
```hlsl
Shader "H3M/RGBDToEquirectangular"
{
    Properties
    {
        _RGBDTexture("RGBD Texture", 2D) = "black" {}
        _SphereRadius("Sphere Radius", Float) = 5.0
    }

    SubShader
    {
        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            sampler2D _RGBDTexture;
            float _SphereRadius;

            struct v2f
            {
                float4 pos : SV_POSITION;
                float3 viewDir : TEXCOORD0;
            };

            v2f vert(appdata_base v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.viewDir = normalize(v.vertex.xyz - _WorldSpaceCameraPos);
                return o;
            }

            float4 frag(v2f i) : SV_Target
            {
                // Convert view direction to spherical coordinates
                float3 dir = normalize(i.viewDir);
                float theta = atan2(dir.z, dir.x); // Azimuth (horizontal angle)
                float phi = asin(dir.y); // Elevation (vertical angle)

                // Map to equirectangular UV (0-1 range)
                float2 equiUV;
                equiUV.x = (theta / (2.0 * 3.14159)) + 0.5; // 0-1 horizontal
                equiUV.y = (phi / 3.14159) + 0.5; // 0-1 vertical

                // Sample RGBD texture
                float4 rgbd = tex2D(_RGBDTexture, equiUV);
                float3 color = rgbd.rgb;
                float depth = rgbd.a;

                // Depth modulates sphere radius
                float radius = _SphereRadius * depth;

                // Discard if too close or too far
                if (radius < 0.5 || radius > 10.0) discard;

                return float4(color, 1.0);
            }
            ENDCG
        }
    }
}
```

**Mesh Setup**: Create UV sphere mesh (subdivided icosphere, 500+ vertices)
- Assign RGBDToEquirectangular shader
- Set sphere radius = 5 meters (user at center)
- Play RGBD video texture on sphere

**Use Cases**:
- 360¬∞ holographic recordings (record room, playback as immersive sphere)
- Portals to other locations (live RGBD stream mapped to sphere)
- Music visualizers (audio-reactive RGBD spheres - Section 4 integration)

---

#### 8.9.2 VFX Graph Sphere Emission

**Alternative**: Emit particles on sphere surface instead of mesh

**VFX Graph Structure**:
```
Initialize Particle
‚îú‚îÄ Set Position: Sample equirectangular texture
‚îÇ   ‚îú‚îÄ Convert ParticleID ‚Üí Spherical coordinates (theta, phi)
‚îÇ   ‚îú‚îÄ radius = SampleRGBD(equiUV).a * SphereRadius
‚îÇ   ‚îî‚îÄ position = camera.position + SphericalToCartesian(theta, phi, radius)
‚îÇ
‚îî‚îÄ Set Color: Sample RGBD.rgb at same UV

Output Particle Quad
‚îî‚îÄ Billboard facing camera (inside-out sphere)
```

**Performance**: 360¬∞ sphere = ~100,000 particles @ 30 FPS (acceptable for cinematic mode)

---

### 8.10 WEBRTC RGBD STREAMING (MULTIPLAYER ARCHITECTURE)

#### 8.10.1 RGBD Video Encoding for WebRTC

**Goal**: Stream encoded RGBD texture as H.265 video track via WebRTC

**Pipeline**:
```
Local iPhone
‚îú‚îÄ H3MRGBDEncoder ‚Üí RGBA RenderTexture (1920x1080)
‚îî‚îÄ Unity WebRTC ‚Üí H.265 VideoStreamTrack
    ‚îî‚îÄ RTCPeerConnection ‚Üí Send to remote client
        ‚Üì
Remote Client (Another iPhone or Desktop)
‚îú‚îÄ Unity WebRTC ‚Üí Receive H.265 VideoStreamTrack
‚îú‚îÄ Decode to RenderTexture
‚îî‚îÄ H3MVFXHologramRenderer ‚Üí Display remote hologram
```

**C# Implementation** (Building on Section 9 - WebRTC Holograms):
```csharp
using Unity.WebRTC;

public class H3MWebRTCHologramSender : MonoBehaviour
{
    [SerializeField] H3MRGBDEncoder encoder;

    private RTCPeerConnection peerConnection;
    private VideoStreamTrack videoTrack;

    void Start()
    {
        // Create peer connection
        var config = new RTCConfiguration
        {
            iceServers = new[] { new RTCIceServer { urls = new[] { "stun:stun.l.google.com:19302" } } }
        };
        peerConnection = new RTCPeerConnection(ref config);

        // Create video track from RGBD texture
        videoTrack = new VideoStreamTrack(encoder.encodedOutput);
        peerConnection.AddTrack(videoTrack);

        // TODO: ICE candidate exchange, SDP offer/answer (see Section 9)
    }

    void OnDestroy()
    {
        videoTrack?.Dispose();
        peerConnection?.Dispose();
    }
}
```

**Receiver**:
```csharp
public class H3MWebRTCHologramReceiver : MonoBehaviour
{
    [SerializeField] H3MVFXHologramRenderer hologramRenderer;

    private RTCPeerConnection peerConnection;
    private RenderTexture receivedTexture;

    void Start()
    {
        receivedTexture = new RenderTexture(1920, 1080, 0, RenderTextureFormat.ARGB32);

        peerConnection = new RTCPeerConnection(/* config */);
        peerConnection.OnTrack = (RTCTrackEvent e) =>
        {
            if (e.Track is VideoStreamTrack videoTrack)
            {
                videoTrack.OnVideoReceived += (Texture texture) =>
                {
                    Graphics.Blit(texture, receivedTexture);
                    hologramRenderer.SetRGBDTexture(receivedTexture);
                };
            }
        };
    }
}
```

**Bandwidth**: 1920x1080 H.265 @ 30 FPS = ~5-10 Mbps (acceptable for WiFi/5G)

**Latency**: ~100-200ms (encoding + network + decoding)

---

#### 8.10.2 Normcore Integration (Section 10 + Section 8)

**Goal**: Combine Normcore room management (Section 11) with WebRTC RGBD streaming

**Architecture**:
```
Normcore Room
‚îú‚îÄ User A (iPhone) - Sends RGBD hologram
‚îÇ   ‚îú‚îÄ Normcore: Room presence + metadata
‚îÇ   ‚îî‚îÄ WebRTC: RGBD video stream
‚îÇ
‚îú‚îÄ User B (iPhone) - Receives hologram + Sends own
‚îÇ   ‚îú‚îÄ Normcore: Room presence + metadata
‚îÇ   ‚îî‚îÄ WebRTC: Bidirectional RGBD streams
‚îÇ
‚îî‚îÄ User C (Desktop) - Spectator (receives all holograms)
    ‚îú‚îÄ Normcore: Room presence
    ‚îî‚îÄ WebRTC: Receives multiple RGBD streams
```

**Signaling**: Use Normcore RealtimeModel for WebRTC SDP/ICE exchange (no separate signaling server needed!)

**See**: Section 10.9 - WebRTC Multiplayer Holograms (detailed implementation)

---

### 8.11 IMPLEMENTATION ROADMAP (150-220 HOURS)

### Phase 1: Core RGBD Capture & Encoding (40-60h)

**Week 1-2**:
- ‚úÖ H3MRGBDCapture (AR Foundation callbacks)
- ‚úÖ YCbCr ‚Üí RGB conversion shader
- ‚úÖ RGBD multiplexer shader
- ‚úÖ Depth normalization system
- ‚úÖ Testing on iPhone 13 Pro+ (verify textures)

**Deliverable**: Encoded RGBD RenderTexture updating at 60 FPS

---

### Phase 2: VFX Graph Hologram Rendering (30-45h)

**Week 3**:
- ‚úÖ Depth ‚Üí Position compute shader
- ‚úÖ VFX Graph particle emission from Position + Color maps
- ‚úÖ GPU Event update system (dynamic holograms)
- ‚úÖ Performance optimization (target: 60 FPS)

**Deliverable**: Live hologram particles following camera movement

---

### Phase 3: Shader-Based Point Cloud (20-30h)

**Week 4**:
- ‚úÖ Point cloud mesh generator (compute shader)
- ‚úÖ Billboard quad rendering
- ‚úÖ Comparison: VFX vs Shader performance

**Deliverable**: Alternative rendering method (user-selectable)

---

### Phase 4: Segmentation Holograms (30-45h)

**Week 5-6**:
- ‚úÖ Body hologram (human stencil mask)
- ‚úÖ Hand hologram (XR Hands bounding box)
- ‚úÖ Confidence filtering (environment depth)

**Deliverable**: Isolated body/hand holograms

---

### Phase 5: Holospheres (20-30h)

**Week 7**:
- ‚úÖ Equirectangular reprojection shader
- ‚úÖ Sphere mesh mapping
- ‚úÖ VFX Graph sphere emission (alternative)

**Deliverable**: 360¬∞ immersive holograms

---

### Phase 6: WebRTC Streaming (Optional, 30-50h)

**Week 8-9**:
- ‚úÖ WebRTC video track integration
- ‚úÖ Normcore signaling (SDP/ICE exchange)
- ‚úÖ Remote hologram rendering
- ‚úÖ Bandwidth optimization (adaptive bitrate)

**Deliverable**: Multiplayer hologram streaming

---

### 8.12 PERFORMANCE OPTIMIZATION & BEST PRACTICES

### 12.1 Texture Resolution Scaling

**Problem**: 1920x1080 RGBD encoding is expensive

**Solution**: Downscale to 960x540 or 640x480 for real-time holograms
```csharp
public RenderTexture CreateScaledTexture(int width, int height)
{
    return new RenderTexture(width, height, 0, RenderTextureFormat.ARGB32)
    {
        filterMode = FilterMode.Bilinear, // Smooth scaling
        wrapMode = TextureWrapMode.Clamp
    };
}
```

**Performance Gain**: 4x faster encoding @ 960x540 (negligible quality loss)

---

### 12.2 Particle Count Reduction

**VFX Graph**: Reduce particle count based on device capability
```csharp
int GetParticleCount()
{
    if (SystemInfo.graphicsMemorySize > 6000) // iPhone 14 Pro
        return 65536; // 256x256
    else if (SystemInfo.graphicsMemorySize > 4000) // iPhone 13
        return 32768; // 181x181
    else // iPhone 12
        return 16384; // 128x128
}
```

---

### 12.3 LOD System (Distance-Based Quality)

**Concept**: Reduce hologram quality when far from camera
```csharp
float distance = Vector3.Distance(Camera.main.transform.position, hologramCenter);

if (distance > 5f)
    vfx.SetInt("Particle Count", 16384); // Low quality
else if (distance > 2f)
    vfx.SetInt("Particle Count", 32768); // Medium quality
else
    vfx.SetInt("Particle Count", 65536); // High quality
```

---

### 12.4 Compute Shader Thread Optimization

**Best Practice**: Match thread group size to texture dimensions
```hlsl
// For 256x192 depth texture:
[numthreads(8,8,1)] // 8x8 = 64 threads per group
// Total groups: (256/8) x (192/8) = 32 x 24 = 768 groups
```

**Avoid**: Large thread groups (16x16) on small textures (wastes GPU)

---

### 12.5 Asynchronous GPU Readback (If CPU Access Needed)

**Problem**: `Texture2D.ReadPixels()` stalls GPU pipeline

**Solution**: Use `AsyncGPUReadback`
```csharp
using UnityEngine.Rendering;

void RequestDepthData()
{
    AsyncGPUReadback.Request(depthTexture, 0, TextureFormat.RFloat, (AsyncGPUReadbackRequest request) =>
    {
        if (!request.hasError)
        {
            NativeArray<float> data = request.GetData<float>();
            // Process depth data on CPU (rarely needed)
        }
    });
}
```

---

### 8.13 INTEGRATION WITH OTHER SYSTEMS

### 13.1 Particle Brush System (Section 0)

**Integration**: Use hologram position data to modulate particle brush emission

**Example**: Paint particles only on hologram surfaces
```csharp
Vector3 brushPosition = GetBrushPosition();
float depth = SampleDepthAt(brushPosition);

if (depth > 0.01f) // Valid hologram surface
{
    brushManager.StartPainting();
}
```

---

**Immediate v3 action plan (verified 2025-11-02)**

1. **ParticlePainter pooling refactor**
   - Replace per-point Instantiate/Destroy with pooled stroke entries, indexed per stroke for reuse
   - Remove debug spam; gate `HandleAttr` logic by attribute name to avoid full rebuilds on unrelated changes
   - **Execution Blueprint**
     1. Document current runtime cost: profile long strokes and attribute toggles to capture GC allocations, child counts, and console noise.
     2. Design pooled data schema: align stroke data (`List<List<Vector3>>`) with pooled GameObject lists or struct wrappers; ensure compatibility with `EnchantedPaintbrushData`.
     3. Lifecycle mapping: storyboard how `AddPoint`, `RenderStrokes`, and undo/redo flows interact once pooling replaces instantiation.
     4. Attribute response table: map contextual attributes (`Particle`, `Size`, `Color`, future params) to targeted updates on pooled instances.
     5. Test matrix: plan editor tests (500+ point stroke, rapid size swap, load/save roundtrip) and profiler checkpoints (GC allocs, frame timing) pre/post refactor.
2. **Contextual attribute cloning**
   - Fix `EnchantedPaintbrush.Save/Load` to deep-clone attributes before subscribing, eliminating duplicate entries and stale references
   - **Execution Blueprint**
     1. Trace existing load/save flow to document where duplication occurs and how `EnsureAndSubscribe` contributes.
     2. Choose cloning implementation (reuse `ContextualAttribute.Clone()` vs. manual copy) ensuring nested data (buttonValues, color) preserved.
     3. Resubscribe lifecycle diagram‚Äîclarify when `EnsureAndSubscribe` runs post-load to avoid double listeners.
     4. Regression scenarios: plan tests (load saved brush, duplicate, undo/redo) to confirm no shared references remain.
     5. Update documentation to reflect serialization expectations once behaviour verified.
3. **VFX Graph bindings**
   - Wire `Size`, `Color`, and additional floats to each VFX Graph brush via `VisualEffect.SetFloat/SetVector4`
   - Ensure prefab swaps triggered by the `Particle` attribute rebuild the pool cleanly
   - **Execution Blueprint**
     1. Catalogue VFX prefabs and exposed parameters (size, color, emission rate) under the EnchantedPaintbrush bundle.
     2. Map contextual attributes to VFX property names/IDs and define default ranges.
     3. Integrate bindings into pooling lifecycle (initial spawn, attribute change, prefab swap) without unnecessary `Reinit`.
     4. Validate with both VFX and Shuriken brushes, confirming attribute updates without respawn.
     5. Profile to ensure minimal overhead (no per-frame property reassignments).
4. **Brush catalogue reconciliation**
   - Audit the 13 headline particle brushes versus the assets under `Assets/Downloadable/Content/Bundles/EnchantedPaintbrush/`
   - Restore missing prefabs or update documentation/menus to match what actually ships
   - **Execution Blueprint**
     1. Generate authoritative prefab list (AssetDatabase or scripted export) and compare against documentation/UI references.
     2. Flag missing/broken entries for recreation or removal; note dependencies (materials, scripts) requiring fixes.
     3. Update documentation and brush selection UI to reflect verified inventory.
     4. Create GUID/reference table to support pooling and addressable loading if needed.
     5. Schedule recurring audit (e.g., monthly) to keep list accurate.
5. **Audio-reactive support**
   - Provide a shared `AudioVisualizer.AudioSampler` prefab (mic permission flow) for v3 scenes or temporarily hide microphone-driven brushes
   - **Execution Blueprint**
     1. Inventory current AudioVisualizer usage (AudioSampler, MicHandle) and platform-specific requirements.
     2. Decide on inclusion strategy: global singleton vs. conditional brush gating until audio pipeline ready.
     3. Outline iOS microphone permission flow and integration points in the v3 boot sequence.
     4. Identify Web/Needle bridge gaps for Phase 2 documentation.
     5. Draft QA checklist covering mic access, latency, background audio behaviour.
6. **Documentation + UX updates**
   - Refresh `_BRUSH_REFERENCE.md`, `.claude/_SECTION_0_QUICK_START.md`, and in-app brush listings with the new pooling workflow and validated brush list
   - **Execution Blueprint**
     1. Gather confirmed behaviour after tasks 1-5 complete; capture diagrams/screenshots as needed.
     2. Structure doc updates (architecture overview, attribute table, troubleshooting).
     3. Cross-link to Paint-AR reference implementations and knowledge base entries.
     4. Note revision metadata (date, owner) for audit trail.
     5. Communicate updates to team channels once published.

---

### 13.2 Hand Tracking Painting (Section 2)

**Integration**: Hand hologram + pinch gesture = volumetric hand painting
```csharp
if (HandGestureDetector.IsPinching())
{
    // Enable hand hologram
    handHologramRenderer.SetActive(true);

    // Emit particles at pinch point
    Vector3 pinchPosition = HandGestureDetector.GetPinchPosition();
    brushManager.AddStrokePoint(pinchPosition);
}
```

---

### 13.3 Normcore Multiplayer (Section 10)

**Integration**: Sync hologram metadata via Normcore, stream RGBD via WebRTC

**RealtimeModel**:
```csharp
[RealtimeModel]
public partial class HologramMetadataModel
{
    [RealtimeProperty(1, reliable: true)]
    private Vector3 _cameraPosition;

    [RealtimeProperty(2, reliable: true)]
    private Quaternion _cameraRotation;

    [RealtimeProperty(3, reliable: true)]
    private Vector2 _depthRange;

    [RealtimeProperty(4, reliable: true)]
    private bool _isStreaming;
}
```

---

## CONCLUSION

**XRAI 5D Holograms** provides a scalable, AR Foundation-based system for volumetric hologram capture and rendering on iOS devices. By combining Keijiro's proven Rcam architecture with Portals' existing VFX capabilities, this system enables:

- ‚úÖ Real-time local holograms (60 FPS)
- ‚úÖ Body/hand segmentation for focused captures
- ‚úÖ 360¬∞ holospheres for immersive environments
- ‚úÖ WebRTC streaming for multiplayer experiences
- ‚úÖ VFX Graph + Shader dual rendering paths
- ‚úÖ Scalable performance across iPhone 12-14 Pro

**Total Time**: 150-220 hours (4-6 weeks, 1-2 developers)

**Recommended Start**: Phase 1 (Core RGBD Capture) - Foundational system for all other features

---

**END OF XRAI 5D HOLOGRAMS DEEP ANALYSIS**

**Document Version**: 1.0
**Created**: 2025-10-25
**Last Updated**: 2025-10-25
**Status**: Ready for Implementation

## 9. RCAM4 LOCAL LIDAR VFX HOLOGRAMS

**Goal**: LiDAR+RGB ‚Üí VFX Graph holograms (local device visualization)

### 9.1 Rcam System Overview

Rcam (Real-time Camera effects by Keijiro):
- Captures iOS LiDAR depth + RGB camera feed
- Processes in real-time (30-60 FPS)
- Renders as VFX Graph point clouds or particle effects
- Used for artistic/creative AR visualization

**Source Location**: `/Users/jamestunick/Downloads/AI_Knowledge_Base_Setup/______VFX25/RCAMS/Rcam4-main 4`

### 9.2 Architecture

```
iOS Device
  ‚îÇ
  ‚îú‚îÄ> LiDAR Scanner (AROcclusionManager)
  ‚îÇ     ‚îî‚îÄ> Depth texture (256x192, 60 FPS)
  ‚îÇ
  ‚îú‚îÄ> RGB Camera (ARCameraManager)
  ‚îÇ     ‚îî‚îÄ> Color texture (1920x1080, 30 FPS)
  ‚îÇ
  ‚îú‚îÄ> Frame Capture (H3MLiDARCapture.cs)
  ‚îÇ     ‚îú‚îÄ> Synchronize depth + RGB
  ‚îÇ     ‚îî‚îÄ> Package camera metadata (pose, FOV, timestamp)
  ‚îÇ
  ‚îî‚îÄ> VFX Graph Hologram Rendering
        ‚îú‚îÄ> Point cloud mode (raw LiDAR points)
        ‚îú‚îÄ> Mesh mode (reconstructed surface)
        ‚îú‚îÄ> Particle mode (artistic visualization)
        ‚îî‚îÄ> Splat mode (Gaussian splatting)
```

### 9.3 Implementation Steps (~80-100 hours)

#### **Phase 1: Rcam4 Analysis & Setup** (8-12h)

**Step 1**: Study Rcam4 architecture
```bash
# Key files to analyze:
# - Rcam4Common/Runtime/InputHandle.cs
# - Rcam4Common/Runtime/Metadata.cs
# - Rcam4Visualizer/Assets/Scripts/FrameDecoder.cs
# - Rcam4Visualizer/Assets/Scripts/AppController.cs
```

**Step 2**: Copy Rcam4Common to Portals_6
```bash
mkdir -p "/Users/jamestunick/wkspaces/Portals_6/Packages/com.h3m.rcam4common"

cp -r "/Users/jamestunick/Downloads/AI_Knowledge_Base_Setup/______VFX25/RCAMS/Rcam4-main 4/Rcam4Common/Runtime" \
      "/Users/jamestunick/wkspaces/Portals_6/Packages/com.h3m.rcam4common/"

# Create package.json for Rcam4Common
cat > "/Users/jamestunick/wkspaces/Portals_6/Packages/com.h3m.rcam4common/package.json" << 'EOF'
{
  "name": "com.h3m.rcam4common",
  "version": "1.0.0",
  "displayName": "H3M Rcam4 Common",
  "description": "Rcam4 data structures for LiDAR hologram rendering",
  "unity": "2022.3"
}
EOF
```

**Step 3**: Add to manifest.json
```json
{
  "dependencies": {
    "com.h3m.rcam4common": "file:../Packages/com.h3m.rcam4common"
  }
}
```

#### **Phase 2: iOS LiDAR Capture** (16-24h)

**Step 4**: Create H3MLiDARCapture.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/LiDAR/H3MLiDARCapture.cs
using UnityEngine;
using UnityEngine.XR.ARFoundation;
using UnityEngine.XR.ARSubsystems;
using Rcam4;

namespace H3M.XR
{
    public class H3MLiDARCapture : MonoBehaviour
    {
        [SerializeField] private AROcclusionManager occlusionManager;
        [SerializeField] private ARCameraManager cameraManager;

        [Header("Capture Settings")]
        [SerializeField] private bool captureDepth = true;
        [SerializeField] private bool captureColor = true;

        public Texture2D DepthTexture { get; private set; }
        public Texture2D ColorTexture { get; private set; }
        public Metadata CameraMetadata { get; private set; }

        public bool IsCapturing { get; private set; }

        void Start()
        {
            if (occlusionManager == null)
                occlusionManager = FindObjectOfType<AROcclusionManager>();
            if (cameraManager == null)
                cameraManager = FindObjectOfType<ARCameraManager>();
        }

        void Update()
        {
            if (IsCapturing)
            {
                CaptureFrame();
            }
        }

        public void StartCapture()
        {
            IsCapturing = true;
        }

        public void StopCapture()
        {
            IsCapturing = false;
        }

        void CaptureFrame()
        {
            // Capture depth texture from LiDAR
            if (captureDepth)
            {
                var depthTex = occlusionManager.environmentDepthTexture;
                if (depthTex != null)
                {
                    DepthTexture = ConvertRenderTextureToTexture2D(depthTex as RenderTexture);
                }
            }

            // Capture RGB texture from camera
            if (captureColor)
            {
                if (cameraManager.TryAcquireLatestCpuImage(out var cpuImage))
                {
                    ColorTexture = ConvertXRCpuImageToTexture2D(cpuImage);
                    cpuImage.Dispose();
                }
            }

            // Capture camera metadata
            CameraMetadata = new Metadata
            {
                cameraPosition = Camera.main.transform.position,
                cameraRotation = Camera.main.transform.rotation,
                fov = Camera.main.fieldOfView,
                timestamp = Time.time
            };
        }

        private Texture2D ConvertRenderTextureToTexture2D(RenderTexture rt)
        {
            RenderTexture.active = rt;
            Texture2D tex = new Texture2D(rt.width, rt.height, TextureFormat.RFloat, false);
            tex.ReadPixels(new Rect(0, 0, rt.width, rt.height), 0, 0);
            tex.Apply();
            RenderTexture.active = null;
            return tex;
        }

        private Texture2D ConvertXRCpuImageToTexture2D(XRCpuImage cpuImage)
        {
            var conversionParams = new XRCpuImage.ConversionParams
            {
                inputRect = new RectInt(0, 0, cpuImage.width, cpuImage.height),
                outputDimensions = new Vector2Int(cpuImage.width, cpuImage.height),
                outputFormat = TextureFormat.RGB24,
                transformation = XRCpuImage.Transformation.MirrorY
            };

            int size = cpuImage.GetConvertedDataSize(conversionParams);
            var buffer = new NativeArray<byte>(size, Allocator.Temp);
            cpuImage.Convert(conversionParams, new IntPtr(buffer.GetUnsafePtr()), buffer.Length);

            Texture2D texture = new Texture2D(
                conversionParams.outputDimensions.x,
                conversionParams.outputDimensions.y,
                conversionParams.outputFormat,
                false);

            texture.LoadRawTextureData(buffer);
            texture.Apply();
            buffer.Dispose();

            return texture;
        }
    }
}
```

**Step 5**: Test LiDAR capture on iPhone 12 Pro+

#### **Phase 3: VFX Graph Integration** (24-32h)

**Step 6**: Create VFX Graphs based on Rcam4 techniques

**NOTE**: Rcam4 does NOT have .vfxgraph files in Assets. Use Keijiro's VFX packages (already installed) and create new VFX Graphs.

**Create PointCloudHologram.vfxgraph**:
```
Initialize Context:
- Capacity: 100,000 particles
- Bounds: 10m cube

Spawn System:
- Custom C# spawner (AttributeBinder)
- Spawn from depth texture
- Rate: Per-pixel spawn (once per depth point)
- Position: Sample depth texture ‚Üí Convert to 3D world space

Update System:
- No velocity (static particles)
- Fade over distance

Output:
- Particle Quad
- Size: 0.01m
- Color: From RGB texture (UV lookup)
- Blending: Opaque
```

**Step 7**: Create H3MHologramRenderer.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/XR/LiDAR/H3MHologramRenderer.cs
using UnityEngine;
using UnityEngine.VFX;
using Rcam4;

namespace H3M.XR
{
    public class H3MHologramRenderer : MonoBehaviour
    {
        [SerializeField] private H3MLiDARCapture lidarCapture;
        [SerializeField] private VisualEffect hologramVFX;

        [Header("Rendering Modes")]
        [SerializeField] private RenderMode renderMode = RenderMode.PointCloud;

        private static readonly int DepthMapProperty = Shader.PropertyToID("DepthMap");
        private static readonly int ColorMapProperty = Shader.PropertyToID("ColorMap");
        private static readonly int CameraPositionProperty = Shader.PropertyToID("CameraPosition");
        private static readonly int CameraRotationProperty = Shader.PropertyToID("CameraRotation");

        void Start()
        {
            if (lidarCapture != null)
            {
                lidarCapture.StartCapture();
            }
        }

        void Update()
        {
            if (lidarCapture == null || hologramVFX == null) return;

            if (lidarCapture.DepthTexture != null)
            {
                // Send textures to VFX Graph
                hologramVFX.SetTexture(DepthMapProperty, lidarCapture.DepthTexture);

                if (lidarCapture.ColorTexture != null)
                {
                    hologramVFX.SetTexture(ColorMapProperty, lidarCapture.ColorTexture);
                }

                // Send camera metadata
                if (lidarCapture.CameraMetadata != null)
                {
                    hologramVFX.SetVector3(CameraPositionProperty,
                        lidarCapture.CameraMetadata.cameraPosition);
                    hologramVFX.SetVector4(CameraRotationProperty,
                        new Vector4(
                            lidarCapture.CameraMetadata.cameraRotation.x,
                            lidarCapture.CameraMetadata.cameraRotation.y,
                            lidarCapture.CameraMetadata.cameraRotation.z,
                            lidarCapture.CameraMetadata.cameraRotation.w
                        ));
                }
            }
        }

        public void SetRenderMode(RenderMode mode)
        {
            renderMode = mode;
            // Switch VFX Graph based on mode
        }

        public enum RenderMode
        {
            PointCloud,   // Raw LiDAR points
            Mesh,         // Reconstructed surface
            Particles,    // Artistic particle effect
            Splat         // Gaussian splatting
        }
    }
}
```

**Step 8**: Test point cloud rendering on iOS device

#### **Phase 4: Advanced Hologram Modes** (16-20h)

**Step 9**: Create additional VFX Graphs
- **MeshHologram.vfxgraph** - Surface reconstruction
- **ParticleHologram.vfxgraph** - Flowing particles
- **SplatHologram.vfxgraph** - Gaussian splat effect

**Step 10**: Implement mode switching UI

#### **Phase 5: Optimization & Testing** (16-20h)

**Step 11**: Optimize for iOS performance
- Reduce particle count based on device (iPhone 12: 50K, iPhone 13+: 100K)
- LOD system: Reduce density at distance
- Culling: Don't render offscreen particles
- GPU instancing for particles

**Step 12**: iOS device testing
- Test on iPhone 12 Pro (oldest LiDAR device)
- Profile with Xcode GPU Frame Capture
- Measure frame rate (target: 30 FPS)
- Test in various environments (indoor, outdoor, bright, dark)

### 9.4 Files to Create

**New Files**:
- `Packages/com.h3m.rcam4common/` (entire package)
- `Assets/[H3M]/Portals/Code/v3/XR/LiDAR/H3MLiDARCapture.cs`
- `Assets/[H3M]/Portals/Code/v3/XR/LiDAR/H3MHologramRenderer.cs`
- `Assets/[H3M]/Portals/Content/VFX/Holograms/PointCloudHologram.vfxgraph`
- `Assets/[H3M]/Portals/Content/VFX/Holograms/MeshHologram.vfxgraph`
- `Assets/[H3M]/Portals/Content/VFX/Holograms/ParticleHologram.vfxgraph`
- `Assets/[H3M]/Portals/Content/VFX/Holograms/SplatHologram.vfxgraph`

### 9.5 Testing Checklist

- [ ] LiDAR depth captured successfully
- [ ] RGB camera feed captured
- [ ] Depth + color synchronized
- [ ] Point cloud renders correctly
- [ ] Colors match RGB feed
- [ ] Camera metadata accurate
- [ ] Frame rate ‚â•30 FPS
- [ ] Mode switching works
- [ ] No memory leaks
- [ ] Works indoors and outdoors

**Total Time**: ~80-100 hours

---

## 10. WEBRTC MULTIPLAYER HOLOGRAMS

**Goal**: Stream LiDAR+RGB holograms to multiple devices via WebRTC

### 10.1 Architecture

```
SENDER DEVICE (iPhone A)
  ‚îÇ
  ‚îú‚îÄ> iOS LiDAR + RGB Capture (from Part 8)
  ‚îÇ
  ‚îú‚îÄ> Frame Encoder
  ‚îÇ     ‚îú‚îÄ> Compress depth (HEVC/H.265)
  ‚îÇ     ‚îú‚îÄ> Compress RGB (H.264)
  ‚îÇ     ‚îî‚îÄ> Package metadata (pose, FOV)
  ‚îÇ
  ‚îî‚îÄ> WebRTC Streaming
        ‚îú‚îÄ> Peer connection setup
        ‚îú‚îÄ> Video tracks (depth + color)
        ‚îî‚îÄ> Data channel (metadata)

RECEIVER DEVICES (iPhone B, C, D...)
  ‚îÇ
  ‚îú‚îÄ> WebRTC Stream Reception
  ‚îÇ     ‚îî‚îÄ> Decode video streams
  ‚îÇ
  ‚îú‚îÄ> Frame Decoder
  ‚îÇ     ‚îú‚îÄ> Reconstruct depth texture
  ‚îÇ     ‚îú‚îÄ> Reconstruct RGB texture
  ‚îÇ     ‚îî‚îÄ> Parse metadata
  ‚îÇ
  ‚îî‚îÄ> Remote Hologram Rendering
        ‚îú‚îÄ> VFX Graph (from Part 8)
        ‚îî‚îÄ> Position in shared AR space
```

### 10.2 Implementation Steps (~120-180 hours)

#### **Phase 1: WebRTC Infrastructure** (32-48h)

**Step 1**: Verify Unity WebRTC package
```json
// Already installed: "com.unity.webrtc": "3.0.0-pre.8"
```

**Step 2**: Setup WebRTC signaling
**Option A: Use Normcore for signaling** (Recommended)
```csharp
// Use Normcore RPC for WebRTC offer/answer exchange
public class NormcoreWebRTCSignaler : RealtimeComponent<WebRTCSignalingModel>
{
    public void SendOffer(string offer, int targetClientId)
    {
        model.BroadcastOffer(offer, targetClientId);
    }

    public void SendAnswer(string answer, int targetClientId)
    {
        model.BroadcastAnswer(answer, targetClientId);
    }

    public void SendICECandidate(string candidate, int targetClientId)
    {
        model.BroadcastICECandidate(candidate, targetClientId);
    }
}
```

**Option B: Deploy Node.js signaling server**
```bash
# Deploy to Heroku/Vercel/AWS
# Or use Firebase Realtime Database as signaling channel
```

**Step 3**: Create H3MWebRTCManager.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Network/WebRTC/H3MWebRTCManager.cs
using Unity.WebRTC;
using UnityEngine;

namespace H3M.Network
{
    public class H3MWebRTCManager : MonoBehaviour
    {
        [SerializeField] private NormcoreWebRTCSignaler signaler;

        private RTCPeerConnection peerConnection;
        private MediaStream sendStream;
        private Dictionary<int, RTCPeerConnection> remotePeers = new Dictionary<int, RTCPeerConnection>();

        void Start()
        {
            // Initialize WebRTC
            WebRTC.Initialize();
        }

        void OnDestroy()
        {
            // Cleanup
            peerConnection?.Dispose();
            WebRTC.Dispose();
        }

        public async void StartStreaming(Texture2D depthTexture, Texture2D colorTexture)
        {
            // Create peer connection
            RTCConfiguration config = new RTCConfiguration
            {
                iceServers = new[]
                {
                    new RTCIceServer { urls = new[] { "stun:stun.l.google.com:19302" } }
                }
            };

            peerConnection = new RTCPeerConnection(ref config);

            // Handle ICE candidates
            peerConnection.OnIceCandidate = candidate =>
            {
                signaler.SendICECandidate(candidate.Candidate, targetClientId);
            };

            // Create video tracks from textures
            var depthTrack = new VideoStreamTrack("depth", depthTexture, UnityEngine.RenderTextureFormat.RFloat);
            peerConnection.AddTrack(depthTrack);

            var colorTrack = new VideoStreamTrack("color", colorTexture, UnityEngine.RenderTextureFormat.BGRA32);
            peerConnection.AddTrack(colorTrack);

            // Create data channel for metadata
            var metadataChannel = peerConnection.CreateDataChannel("metadata");

            // Create offer
            var offer = await peerConnection.CreateOffer();
            await peerConnection.SetLocalDescription(ref offer);

            // Send offer via signaling
            signaler.SendOffer(offer.sdp, targetClientId);
        }

        public async void ReceiveOffer(string offerSdp, int senderClientId)
        {
            // Create peer connection for remote peer
            RTCConfiguration config = new RTCConfiguration
            {
                iceServers = new[]
                {
                    new RTCIceServer { urls = new[] { "stun:stun.l.google.com:19302" } }
                }
            };

            var remotePeer = new RTCPeerConnection(ref config);
            remotePeers[senderClientId] = remotePeer;

            // Handle incoming tracks
            remotePeer.OnTrack = evt =>
            {
                OnRemoteTrackReceived(evt.Track, senderClientId);
            };

            // Set remote description (offer)
            RTCSessionDescription offer = new RTCSessionDescription
            {
                type = RTCSdpType.Offer,
                sdp = offerSdp
            };
            await remotePeer.SetRemoteDescription(ref offer);

            // Create answer
            var answer = await remotePeer.CreateAnswer();
            await remotePeer.SetLocalDescription(ref answer);

            // Send answer via signaling
            signaler.SendAnswer(answer.sdp, senderClientId);
        }

        private void OnRemoteTrackReceived(MediaStreamTrack track, int senderClientId)
        {
            if (track is VideoStreamTrack videoTrack)
            {
                // Get texture from video track
                Texture remoteTexture = videoTrack.Texture;

                // Pass to hologram renderer
                // (will be implemented in Phase 4)
            }
        }
    }
}
```

**Step 4**: Test WebRTC connection between 2 devices (no video yet, just connection)

#### **Phase 2: Frame Encoding/Decoding** (24-32h)

**Step 5**: Create H3MFrameEncoder.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Network/WebRTC/H3MFrameEncoder.cs
using UnityEngine;
using Rcam4;

namespace H3M.Network
{
    public class H3MFrameEncoder : MonoBehaviour
    {
        [Header("Compression Settings")]
        [SerializeField] private int depthWidth = 256;
        [SerializeField] private int depthHeight = 192;
        [SerializeField] private int colorWidth = 640;
        [SerializeField] private int colorHeight = 480;
        [SerializeField] private int targetBitrate = 2000000; // 2 Mbps

        public (Texture2D depth, Texture2D color) PrepareFrameForStreaming(
            Texture2D originalDepth,
            Texture2D originalColor,
            Metadata metadata)
        {
            // Downscale textures for bandwidth efficiency
            Texture2D depthScaled = ScaleTexture(originalDepth, depthWidth, depthHeight);
            Texture2D colorScaled = ScaleTexture(originalColor, colorWidth, colorHeight);

            return (depthScaled, colorScaled);
        }

        private Texture2D ScaleTexture(Texture2D source, int targetWidth, int targetHeight)
        {
            RenderTexture rt = RenderTexture.GetTemporary(targetWidth, targetHeight);
            rt.filterMode = FilterMode.Bilinear;

            RenderTexture.active = rt;
            Graphics.Blit(source, rt);

            Texture2D result = new Texture2D(targetWidth, targetHeight);
            result.ReadPixels(new Rect(0, 0, targetWidth, targetHeight), 0, 0);
            result.Apply();

            RenderTexture.active = null;
            RenderTexture.ReleaseTemporary(rt);

            return result;
        }

        public byte[] EncodeMetadata(Metadata metadata)
        {
            // Serialize metadata to byte array
            // Use MessagePack or Protocol Buffers for efficiency
            return System.Text.Encoding.UTF8.GetBytes(JsonUtility.ToJson(metadata));
        }
    }
}
```

**Step 6**: Create H3MFrameDecoder.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Network/WebRTC/H3MFrameDecoder.cs
using UnityEngine;
using Rcam4;

namespace H3M.Network
{
    public class H3MFrameDecoder : MonoBehaviour
    {
        public Texture2D DepthTexture { get; private set; }
        public Texture2D ColorTexture { get; private set; }
        public Metadata CameraMetadata { get; private set; }

        public void DecodeFrame(Texture2D receivedDepth, Texture2D receivedColor, byte[] metadataBytes)
        {
            DepthTexture = receivedDepth;
            ColorTexture = receivedColor;

            // Deserialize metadata
            string metadataJson = System.Text.Encoding.UTF8.GetString(metadataBytes);
            CameraMetadata = JsonUtility.FromJson<Metadata>(metadataJson);
        }
    }
}
```

**Step 7**: Test encoding and decoding

#### **Phase 3: Network Synchronization** (32-48h)

**Step 8**: Integrate with Normcore
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Network/WebRTC/H3MHologramSynchronizer.cs
using Normal.Realtime;
using UnityEngine;

namespace H3M.Network
{
    public class H3MHologramSynchronizer : RealtimeComponent<HologramModel>
    {
        [SerializeField] private H3MLiDARCapture lidarCapture;
        [SerializeField] private H3MWebRTCManager webrtcManager;
        [SerializeField] private H3MFrameEncoder encoder;

        private bool isSending = false;
        private float sendInterval = 1f / 15f; // 15 FPS for hologram streaming
        private float lastSendTime = 0f;

        void Update()
        {
            if (isSending && Time.time - lastSendTime >= sendInterval)
            {
                StreamHologramFrame();
                lastSendTime = Time.time;
            }
        }

        public void StartSending()
        {
            isSending = true;
            lidarCapture.StartCapture();

            // Notify other clients that this player is streaming
            model.isStreaming = true;
        }

        public void StopSending()
        {
            isSending = false;
            lidarCapture.StopCapture();
            model.isStreaming = false;
        }

        private void StreamHologramFrame()
        {
            if (lidarCapture.DepthTexture == null || lidarCapture.ColorTexture == null)
                return;

            // Prepare frame for streaming
            var (depthScaled, colorScaled) = encoder.PrepareFrameForStreaming(
                lidarCapture.DepthTexture,
                lidarCapture.ColorTexture,
                lidarCapture.CameraMetadata
            );

            // Stream via WebRTC
            webrtcManager.UpdateStreamTextures(depthScaled, colorScaled);

            // Send metadata via data channel
            byte[] metadataBytes = encoder.EncodeMetadata(lidarCapture.CameraMetadata);
            webrtcManager.SendMetadata(metadataBytes);
        }

        protected override void OnRealtimeModelReplaced(HologramModel previousModel, HologramModel currentModel)
        {
            if (previousModel != null)
            {
                previousModel.isStreamingDidChange -= OnStreamingStateChanged;
            }

            if (currentModel != null)
            {
                currentModel.isStreamingDidChange += OnStreamingStateChanged;
            }
        }

        private void OnStreamingStateChanged(HologramModel model, bool isStreaming)
        {
            if (isStreaming && !model.isOwner)
            {
                // Another player started streaming, set up receiver
                SetupReceiver(model.ownerIDInHierarchy);
            }
        }

        private void SetupReceiver(int senderClientId)
        {
            // Request WebRTC connection to sender
            webrtcManager.RequestConnection(senderClientId);
        }
    }
}
```

**Step 9**: Create HologramModel with Normcore Model Builder
- Fields: `bool isStreaming`, `int clientId`
- Events: `isStreamingDidChange`

**Step 10**: Test with 2 devices (sender + receiver)

#### **Phase 4: Remote Hologram Rendering** (24-32h)

**Step 11**: Create RemoteHologramRenderer.cs
```csharp
// Location: Assets/[H3M]/Portals/Code/v3/Network/WebRTC/RemoteHologramRenderer.cs
using UnityEngine;
using UnityEngine.VFX;
using Rcam4;

namespace H3M.Network
{
    public class RemoteHologramRenderer : MonoBehaviour
    {
        [SerializeField] private H3MFrameDecoder decoder;
        [SerializeField] private VisualEffect remoteHologramVFX;

        [Header("Spatial Anchoring")]
        [SerializeField] private Transform remotePlayerAnchor;

        private static readonly int DepthMapProperty = Shader.PropertyToID("DepthMap");
        private static readonly int ColorMapProperty = Shader.PropertyToID("ColorMap");
        private static readonly int CameraPositionProperty = Shader.PropertyToID("CameraPosition");

        public void Initialize(int remoteClientId)
        {
            // Find or create anchor for this remote player
            remotePlayerAnchor = GetOrCreatePlayerAnchor(remoteClientId);
        }

        void Update()
        {
            if (decoder.DepthTexture != null && remoteHologramVFX != null)
            {
                // Update VFX Graph with remote hologram data
                remoteHologramVFX.SetTexture(DepthMapProperty, decoder.DepthTexture);
                remoteHologramVFX.SetTexture(ColorMapProperty, decoder.ColorTexture);

                // Position hologram in shared AR space
                if (decoder.CameraMetadata != null && remotePlayerAnchor != null)
                {
                    // Transform from remote player's space to local AR space
                    Vector3 localPosition = TransformToLocalSpace(
                        decoder.CameraMetadata.cameraPosition,
                        remotePlayerAnchor
                    );

                    remoteHologramVFX.SetVector3(CameraPositionProperty, localPosition);
                }
            }
        }

        private Transform GetOrCreatePlayerAnchor(int clientId)
        {
            // Find existing anchor or create new one
            GameObject anchorObj = GameObject.Find($"PlayerAnchor_{clientId}");
            if (anchorObj == null)
            {
                anchorObj = new GameObject($"PlayerAnchor_{clientId}");
            }
            return anchorObj.transform;
        }

        private Vector3 TransformToLocalSpace(Vector3 remotePosition, Transform anchor)
        {
            // Convert from remote player's coordinate system to local
            // This requires shared AR anchors or relative positioning
            return anchor.TransformPoint(remotePosition);
        }
    }
}
```

**Step 12**: Implement multi-user hologram system
- Support up to 4 simultaneous remote holograms
- LOD system based on distance
- Cull offscreen holograms

#### **Phase 5: AR Spatial Anchoring** (16-24h)

**Step 13**: Implement shared AR space alignment
**Option A: AR Cloud Anchors** (Requires cloud service)
```csharp
// Use Azure Spatial Anchors or similar
// All players anchor to same real-world location
```

**Option B: Relative Positioning** (Simpler, no cloud)
```csharp
// All players start at room origin
// Track relative positions via Normcore RealtimeTransform
```

**Step 14**: Test spatial alignment with 2+ devices

#### **Phase 6: Optimization & Testing** (8-20h)

**Step 15**: Network optimization
- Adaptive bitrate: Reduce quality based on bandwidth
- Frame rate throttling: 15 FPS for holograms (vs 30 FPS local)
- Resolution scaling: Lower resolution at distance
- Bandwidth monitoring

**Step 16**: Multi-device testing
- Test with 2-4 iPhones simultaneously
- Verify hologram synchronization
- Measure latency (target: <200ms)
- Test network resilience (WiFi, 5G)
- Profile battery usage

### 10.3 Files to Create

**New Files**:
- `Assets/[H3M]/Portals/Code/v3/Network/WebRTC/H3MWebRTCManager.cs`
- `Assets/[H3M]/Portals/Code/v3/Network/WebRTC/NormcoreWebRTCSignaler.cs`
- `Assets/[H3M]/Portals/Code/v3/Network/WebRTC/WebRTCSignalingModel.cs`
- `Assets/[H3M]/Portals/Code/v3/Network/WebRTC/H3MFrameEncoder.cs`
- `Assets/[H3M]/Portals/Code/v3/Network/WebRTC/H3MFrameDecoder.cs`
- `Assets/[H3M]/Portals/Code/v3/Network/WebRTC/H3MHologramSynchronizer.cs`
- `Assets/[H3M]/Portals/Code/v3/Network/WebRTC/HologramModel.cs`
- `Assets/[H3M]/Portals/Code/v3/Network/WebRTC/RemoteHologramRenderer.cs`

### 10.4 Testing Checklist

- [ ] WebRTC connection established between 2 devices
- [ ] Depth texture streams successfully
- [ ] RGB texture streams successfully
- [ ] Metadata synchronized
- [ ] Remote hologram renders correctly
- [ ] Spatial alignment accurate
- [ ] Works with 2+ users simultaneously
- [ ] Latency <200ms
- [ ] Frame rate ‚â•15 FPS for remote holograms
- [ ] Network errors handled gracefully
- [ ] Bandwidth usage acceptable (5-10 Mbps)

**Total Time**: ~120-180 hours

---

## 11. NORMCORE MULTIPLAYER XR SCENE COMPOSITION

**Status**: ‚ö° **OPTIONAL - HIGH VALUE, COST-OPTIMIZED**
**Priority**: P1 (Optional multiplayer mode for collaborative creation)
**Time Estimate**: 48-72 hours (1-2 weeks, 1 developer)
**Goal**: Enable multiple users to compose XR scenes together remotely using Normcore networking

**Cost Focus**: Minimize Normcore CCU (Concurrent User) costs via efficient state synchronization

---

### 10.1 CURRENT STATE ANALYSIS

#### **Portals_6 Existing Setup**

**Already Installed** ([Packages/manifest.json](file:///Users/jamestunick/wkspaces/Portals_6/Packages/manifest.json)):
```json
{
  "com.normalvr.normcore": "2.16.2"
}
```

‚úÖ **Normcore 2.16.2 is already integrated** - No package installation required!

**What's Missing**:
- ‚ùå No multiplayer scene composition (painting strokes, object placement)
- ‚ùå No RealtimeModel definitions for synchronized state
- ‚ùå No room management UI
- ‚ùå No AR spectator view for mixed device sessions
- ‚ùå No cost optimization strategies implemented

---

### 10.2 NORMCORE COST OPTIMIZATION STRATEGIES

**Critical**: Normcore pricing based on **CCU (Concurrent Connected Users)** - Minimize connection time!

#### **Cost Tiers** (2024 Pricing):
| Plan | CCU | Price/Month | Notes |
|------|-----|-------------|-------|
| Personal | 20 CCU | $9 | Development/testing |
| Indie | 50 CCU | $50 | Small studios |
| Studio | 100 CCU | $300 | Production apps |
| Enterprise | 500+ CCU | Custom | Large scale |

**Target**: Stay within Indie tier (50 CCU) for production launch

#### **Optimization Pattern #1: Lazy Connection**

**DON'T**: Connect on app launch
```csharp
// ‚ùå BAD - User pays CCU cost immediately
void Start() {
    _realtime.Connect();
}
```

**DO**: Connect only when entering multiplayer mode
```csharp
// ‚úÖ GOOD - User only charged when actively collaborating
public void OnMultiplayerButtonClicked() {
    _realtime.Connect();
}
```

**Savings**: 50-80% reduction in CCU hours if users spend most time in single-player mode

---

#### **Optimization Pattern #2: Auto-Disconnect on Idle**

```csharp
// Assets/[H3M]/Portals/Code/v3/Multiplayer/IdleDisconnectManager.cs
using UnityEngine;
using Normal.Realtime;

namespace H3M.Multiplayer
{
    public class IdleDisconnectManager : MonoBehaviour
    {
        public Realtime realtime;
        public float idleTimeout = 300f; // 5 minutes

        private float lastActivityTime;

        void Update()
        {
            if (!realtime.connected) return;

            // Check for user activity
            if (Input.anyKey || Input.touchCount > 0)
            {
                lastActivityTime = Time.time;
            }

            // Disconnect if idle too long
            if (Time.time - lastActivityTime > idleTimeout)
            {
                Debug.Log("Auto-disconnect due to inactivity");
                realtime.Disconnect();
                // Show reconnect prompt
                ShowReconnectDialog();
            }
        }

        public void RegisterActivity()
        {
            lastActivityTime = Time.time;
        }

        private void ShowReconnectDialog()
        {
            // UI prompt: "You've been disconnected due to inactivity. Reconnect?"
        }
    }
}
```

**Savings**: 30-50% reduction by disconnecting inactive users

---

#### **Optimization Pattern #3: Efficient State Synchronization**

**DON'T**: Sync every frame
```csharp
// ‚ùå BAD - Excessive bandwidth and server load
void Update() {
    if (realtimeView.isOwnedLocallySelf) {
        model.brushPosition = transform.position; // Syncs every frame!
    }
}
```

**DO**: Use distance thresholds and reliable sync for completed strokes
```csharp
// ‚úÖ GOOD - Only sync when meaningful change occurs
void Update() {
    if (!realtimeView.isOwnedLocallySelf) return;

    Vector3 currentPos = transform.position;
    if (Vector3.Distance(lastSyncedPosition, currentPos) > 0.01f) {
        model.brushPosition = currentPos;
        lastSyncedPosition = currentPos;
    }
}
```

**Best Practice from Normcore Docs**:
> "Whenever I add multiplayer support to a prototype, I like to think about the state that needs to be shared between clients."

**Rule**: Only sync **completed actions** (finished strokes, placed objects), not **continuous motion** (cursor position)

---

#### **Optimization Pattern #4: Room Size Limits**

```csharp
// Limit max users per room to control bandwidth
public class RoomManager : MonoBehaviour
{
    public Realtime realtime;
    public int maxUsersPerRoom = 4; // Keep rooms small!

    void OnClientConnected(Realtime.ClientConnectedEvent evt)
    {
        if (realtime.clientIDs.Count > maxUsersPerRoom)
        {
            Debug.LogWarning("Room full, redirecting to new room");
            string newRoomName = $"Room_{Random.Range(1000, 9999)}";
            realtime.Connect(newRoomName);
        }
    }
}
```

**Recommendation**: **2-4 users per room max** for collaborative painting

**Bandwidth per user**:
- 2 users: ~50-100 KB/s per user
- 4 users: ~100-200 KB/s per user
- 8 users: ~300-500 KB/s per user (avoid!)

---

### 10.3 ARCHITECTURE: MULTIPLAYER PAINTING & OBJECT PLACEMENT

Based on [Normcore Multiplayer Drawing Guide](https://docs.normcore.io/guides/creating-a-multiplayer-drawing-app)

#### **State Synchronization Model**

```
H3M Multiplayer Architecture
‚îú‚îÄ RealtimeModels (State definitions)
‚îÇ   ‚îú‚îÄ BrushStrokeModel : RealtimeModel
‚îÇ   ‚îÇ   ‚îú‚îÄ strokePoints: RealtimeArray<Vector3>
‚îÇ   ‚îÇ   ‚îú‚îÄ brushType: string
‚îÇ   ‚îÇ   ‚îú‚îÄ color: Color
‚îÇ   ‚îÇ   ‚îú‚îÄ isFinalized: bool
‚îÇ   ‚îÇ   ‚îî‚îÄ ownerID: int
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ PlacedObjectModel : RealtimeModel
‚îÇ   ‚îÇ   ‚îú‚îÄ objectID: string (from Icosa API)
‚îÇ   ‚îÇ   ‚îú‚îÄ position: Vector3
‚îÇ   ‚îÇ   ‚îú‚îÄ rotation: Quaternion
‚îÇ   ‚îÇ   ‚îú‚îÄ scale: Vector3
‚îÇ   ‚îÇ   ‚îî‚îÄ ownerID: int
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ ParticleBrushStrokeModel : RealtimeModel (P0 integration)
‚îÇ       ‚îú‚îÄ brushName: string
‚îÇ       ‚îú‚îÄ strokePoints: RealtimeArray<Vector3>
‚îÇ       ‚îú‚îÄ emissionTimestamps: RealtimeArray<float>
‚îÇ       ‚îî‚îÄ isFinalized: bool
‚îÇ
‚îú‚îÄ RealtimeComponents (Network sync)
‚îÇ   ‚îú‚îÄ NetworkedBrushStroke : RealtimeComponent<BrushStrokeModel>
‚îÇ   ‚îú‚îÄ NetworkedPlacedObject : RealtimeComponent<PlacedObjectModel>
‚îÇ   ‚îî‚îÄ NetworkedParticleBrush : RealtimeComponent<ParticleBrushStrokeModel>
‚îÇ
‚îî‚îÄ Managers
    ‚îú‚îÄ MultiplayerSceneManager (Room join/leave)
    ‚îú‚îÄ NetworkedPaintingManager (Stroke creation)
    ‚îî‚îÄ NetworkedObjectManager (Object placement)
```

---

### 10.4 IMPLEMENTATION PLAN (48-72 hours)

#### **Phase 1: Realtime Models** (8-12h)

**Step 1**: Create BrushStrokeModel
```csharp
// Assets/[H3M]/Portals/Code/v3/Multiplayer/Models/BrushStrokeModel.cs
using Normal.Realtime;
using Normal.Realtime.Serialization;
using UnityEngine;

namespace H3M.Multiplayer
{
    [RealtimeModel]
    public partial class BrushStrokeModel
    {
        [RealtimeProperty(1, reliable: true)]
        private string _brushType;

        [RealtimeProperty(2, reliable: true)]
        private Color _color;

        [RealtimeProperty(3, reliable: true)]
        private bool _isFinalized;

        [RealtimeProperty(4, reliable: true)]
        private int _ownerID;

        [RealtimeProperty(5, reliable: true)]
        private RealtimeArray<Vector3Model> _strokePoints;
    }

    // Helper model for Vector3 serialization
    [RealtimeModel]
    public partial class Vector3Model
    {
        [RealtimeProperty(1, reliable: true)]
        private float _x;

        [RealtimeProperty(2, reliable: true)]
        private float _y;

        [RealtimeProperty(3, reliable: true)]
        private float _z;

        public Vector3 ToVector3() => new Vector3(_x, _y, _z);
        public static Vector3Model FromVector3(Vector3 v) => new Vector3Model { _x = v.x, _y = v.y, _z = v.z };
    }
}
```

**Step 2**: Create PlacedObjectModel
```csharp
// Assets/[H3M]/Portals/Code/v3/Multiplayer/Models/PlacedObjectModel.cs
using Normal.Realtime;
using Normal.Realtime.Serialization;
using UnityEngine;

namespace H3M.Multiplayer
{
    [RealtimeModel]
    public partial class PlacedObjectModel
    {
        [RealtimeProperty(1, reliable: true)]
        private string _objectID; // Icosa asset ID

        [RealtimeProperty(2, reliable: true)]
        private Vector3Model _position;

        [RealtimeProperty(3, reliable: true)]
        private QuaternionModel _rotation;

        [RealtimeProperty(4, reliable: true)]
        private Vector3Model _scale;

        [RealtimeProperty(5, reliable: true)]
        private int _ownerID;
    }

    [RealtimeModel]
    public partial class QuaternionModel
    {
        [RealtimeProperty(1, reliable: true)] private float _x;
        [RealtimeProperty(2, reliable: true)] private float _y;
        [RealtimeProperty(3, reliable: true)] private float _z;
        [RealtimeProperty(4, reliable: true)] private float _w;

        public Quaternion ToQuaternion() => new Quaternion(_x, _y, _z, _w);
        public static QuaternionModel FromQuaternion(Quaternion q) => new QuaternionModel { _x = q.x, _y = q.y, _z = q.z, _w = q.w };
    }
}
```

**Step 3**: Compile models
- Unity automatically generates RealtimeModel implementation code
- Check `Assets/[H3M]/Portals/Code/v3/Multiplayer/Models/*.Generated.cs`
- Fix any compilation errors

---

#### **Phase 2: Networked Components** (16-24h)

**Step 4**: Create NetworkedBrushStroke component
```csharp
// Assets/[H3M]/Portals/Code/v3/Multiplayer/NetworkedBrushStroke.cs
using Normal.Realtime;
using UnityEngine;
using System.Collections.Generic;
using H3M.Painting;

namespace H3M.Multiplayer
{
    public class NetworkedBrushStroke : RealtimeComponent<BrushStrokeModel>
    {
        private LineRenderer lineRenderer;
        private List<Vector3> localPoints = new List<Vector3>();

        private void Awake()
        {
            lineRenderer = GetComponent<LineRenderer>();
            if (lineRenderer == null)
            {
                lineRenderer = gameObject.AddComponent<LineRenderer>();
            }
        }

        protected override void OnRealtimeModelReplaced(BrushStrokeModel previousModel, BrushStrokeModel currentModel)
        {
            if (previousModel != null)
            {
                // Unregister old events
                previousModel.strokePoints.modelAdded -= OnStrokePointAdded;
            }

            if (currentModel != null)
            {
                // Rebuild stroke from existing points (late-join scenario)
                localPoints.Clear();
                foreach (Vector3Model pointModel in currentModel.strokePoints)
                {
                    localPoints.Add(pointModel.ToVector3());
                }
                UpdateLineRenderer();

                // Register new events
                currentModel.strokePoints.modelAdded += OnStrokePointAdded;

                // Apply brush properties
                ApplyBrushProperties();
            }
        }

        private void OnStrokePointAdded(BrushStrokeModel model, Vector3Model pointModel, int index)
        {
            localPoints.Add(pointModel.ToVector3());
            UpdateLineRenderer();
        }

        private void UpdateLineRenderer()
        {
            lineRenderer.positionCount = localPoints.Count;
            lineRenderer.SetPositions(localPoints.ToArray());
        }

        private void ApplyBrushProperties()
        {
            // Apply color, width, material based on model.brushType
            lineRenderer.startColor = model.color;
            lineRenderer.endColor = model.color;
            lineRenderer.startWidth = 0.01f;
            lineRenderer.endWidth = 0.01f;
            // TODO: Load material based on model.brushType
        }

        // Called by local painting system to add points
        public void AddPoint(Vector3 worldPosition)
        {
            if (!realtimeView.isOwnedLocallySelf) return;

            Vector3Model pointModel = Vector3Model.FromVector3(worldPosition);
            model.strokePoints.Add(pointModel);
        }

        public void FinalizeStroke()
        {
            if (!realtimeView.isOwnedLocallySelf) return;
            model.isFinalized = true;
        }
    }
}
```

**Step 5**: Create NetworkedPaintingManager
```csharp
// Assets/[H3M]/Portals/Code/v3/Multiplayer/NetworkedPaintingManager.cs
using Normal.Realtime;
using UnityEngine;
using H3M.Painting;

namespace H3M.Multiplayer
{
    public class NetworkedPaintingManager : MonoBehaviour
    {
        public Realtime realtime;
        public GameObject brushStrokePrefab; // Must be in Resources folder!

        private NetworkedBrushStroke currentStroke;

        public void StartStroke(string brushType, Color color)
        {
            if (!realtime.connected)
            {
                Debug.LogWarning("Not connected to Normcore, painting locally only");
                return;
            }

            // Instantiate networked stroke prefab
            GameObject strokeObj = Realtime.Instantiate(
                prefabName: "NetworkedBrushStroke",
                ownedByClient: true,
                preventOwnershipTakeover: true,
                useInstance: realtime
            );

            currentStroke = strokeObj.GetComponent<NetworkedBrushStroke>();
            currentStroke.model.brushType = brushType;
            currentStroke.model.color = color;
            currentStroke.model.ownerID = realtime.clientID;
        }

        public void AddStrokePoint(Vector3 worldPosition)
        {
            if (currentStroke == null) return;
            currentStroke.AddPoint(worldPosition);
        }

        public void EndStroke()
        {
            if (currentStroke == null) return;
            currentStroke.FinalizeStroke();
            currentStroke = null;
        }
    }
}
```

---

#### **Phase 3: Integration with EnchantedPaintbrush** (12-18h)

**Step 6**: Create EnchantedMultiplayerPaintbrush
```csharp
// Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/EnchantedMultiplayerPaintbrush.cs
using UnityEngine;
using H3M.Multiplayer;
using Normal.Realtime;

namespace H3M
{
    public class EnchantedMultiplayerPaintbrush : EnchantedPaintbrush
    {
        [Header("Multiplayer")]
        public NetworkedPaintingManager networkManager;
        public Realtime realtime;

        [Header("Multiplayer Mode")]
        public bool multiplayerEnabled = false;

        private bool isPainting = false;

        void Update()
        {
            bool inputActive = ((Touchscreen.current != null &&
                                Touchscreen.current.primaryTouch.press.isPressed) ||
                               (Mouse.current != null && Mouse.current.leftButton.isPressed)) &&
                              H3M.XR.activeObjectManipulator.currentItem == gameObject;

            // Start painting
            if (inputActive && !isPainting)
            {
                isPainting = true;
                strokes.Add(new List<Vector3>());

                // Start networked stroke if multiplayer enabled
                if (multiplayerEnabled && realtime != null && realtime.connected)
                {
                    string brushType = GetString("BrushName");
                    Color color = GetMaterial("BrushMaterial").color;
                    networkManager.StartStroke(brushType, color);
                }
            }

            // Continue painting
            if (inputActive && isPainting)
            {
                Vector3 newPoint = H3M.XR.XRCamera.transform.position +
                                   H3M.XR.XRCamera.transform.forward * recordingDistance;

                List<Vector3> currentStroke = strokes[strokes.Count - 1];

                if (currentStroke.Count == 0 ||
                    Vector3.Distance(currentStroke[currentStroke.Count - 1], newPoint) >= minDistanceBetweenPoints)
                {
                    currentStroke.Add(newPoint);
                    AddPoint(newPoint);

                    // Add point to networked stroke
                    if (multiplayerEnabled && networkManager != null)
                    {
                        networkManager.AddStrokePoint(newPoint);
                    }
                }
            }

            // Stop painting
            if (!inputActive && isPainting)
            {
                isPainting = false;

                // Finalize networked stroke
                if (multiplayerEnabled && networkManager != null)
                {
                    networkManager.EndStroke();
                }
            }
        }

        protected override void AddPoint(Vector3 point)
        {
            // Local rendering (particles, effects)
        }

        protected override void RenderStrokes()
        {
            // Local stroke visualization
        }

        protected override void HandleAttr(ContextualAttribute a)
        {
            // Attribute changes
        }
    }
}
```

---

#### **Phase 4: Room Management & UI** (12-18h)

**Step 7**: Create MultiplayerRoomManager
```csharp
// Assets/[H3M]/Portals/Code/v3/Multiplayer/MultiplayerRoomManager.cs
using Normal.Realtime;
using UnityEngine;
using UnityEngine.UI;

namespace H3M.Multiplayer
{
    public class MultiplayerRoomManager : MonoBehaviour
    {
        [Header("Normcore")]
        public Realtime realtime;

        [Header("UI")]
        public InputField roomNameInput;
        public Button joinButton;
        public Button leaveButton;
        public Text statusText;

        [Header("Cost Optimization")]
        public IdleDisconnectManager idleManager;

        void Start()
        {
            joinButton.onClick.AddListener(OnJoinClicked);
            leaveButton.onClick.AddListener(OnLeaveClicked);

            realtime.didConnectToRoom += OnConnectedToRoom;
            realtime.didDisconnectFromRoom += OnDisconnectedFromRoom;

            UpdateUI();
        }

        private void OnJoinClicked()
        {
            string roomName = roomNameInput.text;
            if (string.IsNullOrEmpty(roomName))
            {
                roomName = $"Room_{Random.Range(1000, 9999)}";
            }

            statusText.text = $"Connecting to {roomName}...";
            realtime.Connect(roomName);
        }

        private void OnLeaveClicked()
        {
            statusText.text = "Disconnecting...";
            realtime.Disconnect();
        }

        private void OnConnectedToRoom(Realtime room)
        {
            statusText.text = $"Connected to {room.room.name} ({realtime.clientIDs.Count} users)";
            UpdateUI();

            // Start idle disconnect timer
            if (idleManager != null)
            {
                idleManager.enabled = true;
            }
        }

        private void OnDisconnectedFromRoom(Realtime room)
        {
            statusText.text = "Disconnected";
            UpdateUI();

            // Stop idle disconnect timer
            if (idleManager != null)
            {
                idleManager.enabled = false;
            }
        }

        private void UpdateUI()
        {
            bool connected = realtime != null && realtime.connected;
            joinButton.interactable = !connected;
            leaveButton.interactable = connected;
            roomNameInput.interactable = !connected;
        }
    }
}
```

**Step 8**: Create multiplayer UI Canvas
- Room join/leave buttons
- User count display
- Connection status indicator
- "Multiplayer Mode" toggle

---

### 10.5 AR SPECTATOR VIEW (OPTIONAL ADVANCED FEATURE)

**Use Case**: Mix AR (iPhone/iPad) and VR (Quest) users in same room

Based on [Normcore AR Spectator Guide](https://docs.normcore.io/guides/using-ar-as-a-spectator-view)

**Architecture**:
```
Multiplayer Session (Same Normcore Room)
‚îú‚îÄ VR Users (Quest headset)
‚îÇ   ‚îî‚îÄ Standard XR Rig + RealtimeAvatarManager
‚îÇ
‚îî‚îÄ AR Users (iPhone/iPad)
    ‚îî‚îÄ AR Foundation Camera Rig + RealtimeAvatarManager
        ‚îú‚îÄ AR plane detection
        ‚îú‚îÄ World origin placement
        ‚îî‚îÄ Camera synced to avatar head position
```

**Implementation** (16-24h):
1. Create AR-specific scene variant
2. Replace camera rig with AR Foundation prefab
3. Assign AR Camera to RealtimeAvatarManager.head slot
4. Disable virtual floor in AR scene
5. Test cross-platform session joining

**Benefits**:
- AR users can "spectate" VR experiences without headset
- Voice chat between AR and VR users
- Shared spatial canvas for collaborative creation

**Cost Impact**: No additional CCU cost (AR users = VR users in pricing)

---

### 10.6 PARTICLE BRUSH MULTIPLAYER INTEGRATION (P0 + P1)

**Goal**: Sync particle brush strokes from Section 0 across network

**Step 9**: Create ParticleBrushStrokeModel
```csharp
[RealtimeModel]
public partial class ParticleBrushStrokeModel
{
    [RealtimeProperty(1, reliable: true)]
    private string _brushName;

    [RealtimeProperty(2, reliable: true)]
    private RealtimeArray<Vector3Model> _strokePoints;

    [RealtimeProperty(3, reliable: true)]
    private RealtimeArray<FloatModel> _emissionTimestamps;

    [RealtimeProperty(4, reliable: true)]
    private bool _isFinalized;
}

[RealtimeModel]
public partial class FloatModel
{
    [RealtimeProperty(1, reliable: true)] private float _value;
    public float Value { get => _value; set => _value = value; }
}
```

**Step 10**: Extend NetworkedPaintingManager for particles
```csharp
public void StartParticleStroke(string brushName)
{
    GameObject strokeObj = Realtime.Instantiate(
        prefabName: "NetworkedParticleBrushStroke",
        ownedByClient: true,
        useInstance: realtime
    );

    NetworkedParticleBrushStroke stroke = strokeObj.GetComponent<NetworkedParticleBrushStroke>();
    stroke.model.brushName = brushName;

    // Instantiate local particle brush via H3MParticleBrushManager
    brushManager.SelectBrush(brushName);
    brushManager.StartPainting();
}
```

**Challenge**: Particle systems are **continuous emission**, not discrete strokes
**Solution**: Only sync **stroke paths** (Vector3 points), let each client render particles locally

**Bandwidth Savings**: ~95% compared to syncing particle positions

---

### 10.7 TESTING REQUIREMENTS

**Mandatory Tests**:
1. ‚úÖ 2-4 users painting simultaneously
2. ‚úÖ Strokes appear on all clients
3. ‚úÖ Late-joining users see existing strokes
4. ‚úÖ Idle disconnect works (5 min timeout)
5. ‚úÖ Connection lost ‚Üí Reconnect flow
6. ‚úÖ Room full ‚Üí Redirect to new room
7. ‚úÖ Frame rate ‚â•60 FPS (local) + ‚â•30 FPS (remote strokes)
8. ‚úÖ Latency <200ms (stroke appears on remote clients)
9. ‚úÖ Bandwidth <500 KB/s per user (4-user room)
10. ‚úÖ Memory usage <600MB total

**Performance Benchmarks**:
| Metric | Target | Acceptable | Notes |
|--------|--------|------------|-------|
| Latency (stroke sync) | <100ms | <200ms | From local draw to remote render |
| Bandwidth (per user) | <200 KB/s | <500 KB/s | 4-user room |
| CCU cost (5-hour session) | <0.25 CCU | <0.5 CCU | With idle disconnect |
| Frame time (local + remote) | <16ms | <33ms | 60 FPS / 30 FPS |

**Cost Testing**:
- **Scenario 1**: User paints for 30 min, idles for 30 min
  - Expected: 0.5h CCU (auto-disconnect after 5 min idle)
  - Max cost: $0.10 per user (Indie tier)

- **Scenario 2**: 4 users collaborate for 1 hour
  - Expected: 4.0h CCU
  - Max cost: $0.80 total

- **Scenario 3**: 50 daily active users (30 min avg session)
  - Expected: 25h CCU per day = 750h/month = ~31 CCU peak
  - Fits within: Indie tier (50 CCU) = $50/month

---

### 10.8 FILES TO CREATE/MODIFY

**New Files**:
```
Assets/[H3M]/Portals/Code/v3/Multiplayer/
  ‚îú‚îÄ Models/
  ‚îÇ   ‚îú‚îÄ BrushStrokeModel.cs
  ‚îÇ   ‚îú‚îÄ PlacedObjectModel.cs
  ‚îÇ   ‚îú‚îÄ ParticleBrushStrokeModel.cs
  ‚îÇ   ‚îú‚îÄ Vector3Model.cs
  ‚îÇ   ‚îî‚îÄ QuaternionModel.cs
  ‚îÇ
  ‚îú‚îÄ NetworkedBrushStroke.cs
  ‚îú‚îÄ NetworkedParticleBrushStroke.cs
  ‚îú‚îÄ NetworkedPlacedObject.cs
  ‚îú‚îÄ NetworkedPaintingManager.cs
  ‚îú‚îÄ MultiplayerRoomManager.cs
  ‚îî‚îÄ IdleDisconnectManager.cs

Assets/[H3M]/Portals/Code/v3/XR/EnchantedScenes/EnchantedPaintbrush/
  ‚îî‚îÄ EnchantedMultiplayerPaintbrush.cs

Assets/[H3M]/Portals/Resources/
  ‚îú‚îÄ NetworkedBrushStroke.prefab (MUST be in Resources!)
  ‚îî‚îÄ NetworkedParticleBrushStroke.prefab
```

**Modified Files**:
```
Assets/[H3M]/Portals/Scenes/
  ‚îî‚îÄ MainMultiPlayerScene-Tools.unity (Add Realtime + UI)
```

---

### 10.9 TIMELINE & COST ANALYSIS

**Implementation Timeline** (1 developer):
- Week 1: Phases 1-2 (Models + Components) - 24-36h
- Week 2: Phases 3-4 (Integration + UI) - 24-36h

**Total**: 48-72 hours (1-2 weeks)

**Estimated Monthly Costs** (Indie Tier $50/month):

| User Pattern | Daily Sessions | Avg Duration | Monthly CCU | Cost |
|--------------|----------------|--------------|-------------|------|
| Casual | 10 users | 15 min | ~8 CCU peak | $9/month (Personal tier) |
| Regular | 30 users | 30 min | ~25 CCU peak | $50/month (Indie tier) |
| Active | 50 users | 45 min | ~40 CCU peak | $50/month (Indie tier) |
| Heavy | 80 users | 60 min | ~65 CCU peak | $300/month (Studio tier) ‚ö†Ô∏è |

**Recommendation**: **Start with Personal tier ($9/month)** for testing, upgrade to Indie ($50/month) at launch

**With Optimizations Applied**:
- Idle disconnect: **-40% CCU cost**
- Lazy connection: **-60% CCU cost**
- Combined savings: **~70% reduction**

Example: 50 active users without optimization = 100 CCU ($300/month)
Example: 50 active users WITH optimization = 30 CCU ($50/month)

**ROI**: Optimization implementation (48-72h) pays for itself in **3-6 months** of server costs

---

### 10.10 NORMCORE BEST PRACTICES (KNOWLEDGE BASE)

**From Official Documentation Analysis**:

1. **State-First Design**: "Think about the state that needs to be shared between clients" - Design RealtimeModels before writing component logic

2. **Ownership Pattern**: Use `realtimeView.isOwnedLocallySelf` to gate writes - Only owner modifies model, everyone reads

3. **Reliable vs Unreliable**:
   - Reliable (true): Completed strokes, object placement, finalization flags
   - Unreliable (false): Cursor position, brush tip position (frequent updates)

4. **Event-Driven Updates**: Use `modelAdded` events instead of polling - React to network changes, don't poll models

5. **Late-Join Handling**: Always rebuild from model state in `OnRealtimeModelReplaced` - New users reconstruct scene from existing models

6. **Prefab Location**: Networked prefabs MUST be in `Resources/` folder - Required for `Realtime.Instantiate()` name-based loading

7. **Lazy Connection**: Don't connect on app launch - Only connect when entering multiplayer mode

8. **Idle Disconnect**: Implement 5-10 minute timeout - Disconnect inactive users to save CCU costs

9. **Room Size Limits**: Keep rooms small (2-4 users) - More users = more bandwidth per user

10. **Sync Completed Actions Only**: Don't sync continuous motion - Sync discrete events (stroke finished, object placed)

**Anti-Patterns to Avoid**:
- ‚ùå Syncing cursor position every frame (use unreliable + distance threshold)
- ‚ùå Large rooms (8+ users) without bandwidth optimization
- ‚ùå Connecting on app launch (users pay even in solo mode)
- ‚ùå No idle disconnect (users stay connected when AFK)
- ‚ùå Syncing particle positions (sync stroke path instead)

---

### 10.11 INTEGRATION WITH OTHER FEATURES

**Particle Brush System (Section 0)**:
- Sync brush type + stroke path
- Let each client render particles locally
- 95% bandwidth savings vs syncing particles

**Hand Tracking Painting (Section 2)**:
- Sync hand gesture strokes same as touch strokes
- No special handling needed

**Whisper.Icosa Objects (Section 5)**:
- Sync placed object ID + transform
- Each client loads glTF from Icosa API locally
- Bandwidth: ~100 bytes per object (just metadata!)

**Body Tracking VFX (Section 3)**:
- DON'T sync body tracking data (privacy + bandwidth)
- Each user sees their own body VFX locally only

**Fungisync Effects (Section 7)**:
- Already multiplayer-ready (uses Normcore)
- Can share NetworkedEffectManager implementation

---

**RECOMMENDATION**: Implement multiplayer support for **painting ONLY** first (48h), add object placement later (24h)

---

## 12. IMPLEMENTATION TIMELINE & ROADMAP

### Recommended Phasing (2 developers, full-time)

| Phase | Features | Duration | Priority |
|-------|----------|----------|----------|
| **Phase 1** | Hand Tracking Painting<br/>Audio Reactive Brushes | 4-6 weeks | üî• HIGH |
| **Phase 2** | Whisper.Icosa Voice-to-Object<br/>VFX Graph Body Tracking | 6-8 weeks | üî• HIGH |
| **Phase 3** | Rcam4 Local Holograms<br/>Echovision Depth Effects | 8-10 weeks | ‚ö†Ô∏è MEDIUM |
| **Phase 4** | Fungisync Multiplayer<br/>WebRTC Holograms | 12-16 weeks | ‚ö†Ô∏è MEDIUM |

**TOTAL**: 30-40 weeks (7-10 months)

### Weekly Sprint Breakdown (Example: Phase 1)

**Week 1-2: Hand Tracking Painting**
- Days 1-3: Hand gesture detection implementation
- Days 4-7: EnchantedPaintbrush integration
- Days 8-10: iOS testing and refinement

**Week 3-4: Audio Reactive Brushes**
- Days 1-3: Audio analysis system
- Days 4-6: Audio reactive paintbrush
- Days 7-10: Advanced effects and testing

---

## 13. TESTING REQUIREMENTS

### Per Feature (MANDATORY)

Every feature MUST pass these checks before moving to next:

- [ ] ‚úÖ Compiles with zero warnings
- [ ] ‚úÖ Works in Unity Editor (Mac Standalone)
- [ ] ‚úÖ **Works on iOS device** (iPhone 12 Pro+, iOS 16+)
- [ ] ‚úÖ Frame rate ‚â•30 FPS (60 FPS for painting/UI)
- [ ] ‚úÖ No memory leaks (Xcode Instruments)
- [ ] ‚úÖ Save/Load works correctly (if applicable)
- [ ] ‚úÖ No console errors during 10-minute session
- [ ] ‚úÖ Battery drain acceptable (<20% per hour)

### iOS-Specific Testing

**ALWAYS start ADB logging on iOS testing** (if needed):
```bash
# For Quest/Android testing only
adb logcat -v color -s Unity
```

**For iOS testing**:
```bash
# Connect iPhone to Mac, open Xcode
# Window ‚Üí Devices and Simulators ‚Üí Select device ‚Üí View Console
# OR use Unity Remote Log Viewer
```

**Performance Profiling**:
- Open Xcode ‚Üí Menu ‚Üí Xcode ‚Üí Open Developer Tool ‚Üí Instruments
- Select "Metal System Trace" for GPU profiling
- Select "Time Profiler" for CPU profiling
- Record during gameplay session
- Analyze frame drops and bottlenecks

### Test Device Requirements

**Minimum Devices Needed**:
- ‚úÖ **iPhone 12 Pro** (oldest LiDAR device) - Test baseline performance
- ‚úÖ **iPhone 13 Pro** (better GPU) - Test optimal performance
- ‚úÖ **iPhone 14 Pro+** (optional) - Test latest hardware

**Features Requiring Specific Hardware**:
- LiDAR features: iPhone 12 Pro+ only
- Hand tracking: iPhone 12+ (works on all ARKit hand tracking devices)
- WebRTC holograms: Any iPhone 12+ (2+ devices required for testing)

---

## 14. RISK MITIGATION

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **iOS Performance Issues** | HIGH | HIGH | Test on oldest device (iPhone 12) frequently, profile early |
| **Network Bandwidth Limitations** | MEDIUM | HIGH | Implement adaptive bitrate, test on 5G and WiFi |
| **LiDAR Availability** | LOW | HIGH | Only iPhone 12 Pro+ has LiDAR - document clearly |
| **API Rate Limits (Icosa)** | MEDIUM | MEDIUM | Implement caching, add fallback APIs |
| **Complexity Overload** | HIGH | HIGH | Start with Phase 1, validate before moving to Phase 2-4 |
| **Battery Drain** | HIGH | MEDIUM | Optimize VFX particle counts, throttle update rates |
| **Spatial Alignment Errors** | MEDIUM | HIGH | Test shared AR anchors thoroughly, provide manual alignment |
| **WebRTC Connection Failures** | MEDIUM | HIGH | Implement reconnection logic, fallback to 4G if WiFi fails |
| **Unity Package Conflicts** | LOW | MEDIUM | Test package upgrades in separate branch first |
| **ML Model Size (Whisper)** | LOW | MEDIUM | Use tiny model (39MB), preload on app start |

### Critical Success Factors

1. **Test on iOS devices early and often** - Don't wait until end of development
2. **Profile performance continuously** - Fix bottlenecks as they appear
3. **Start simple, iterate** - Get basic version working before adding complexity
4. **User feedback loops** - Test with real users after Phase 1 and 2
5. **Backup plans** - Have fallbacks for risky features (WebRTC, AR anchors)

---

## PACKAGE VERSION CORRECTIONS

**CRITICAL**: The following package versions in this plan were incorrect. Actual versions from Portals_6:

| Package | Plan Version | Actual Version | Status |
|---------|--------------|----------------|--------|
| XR Hands | ~~1.7.0~~ | **1.5.0** | ‚úÖ Unity 6 compatible |
| XR Interaction Toolkit | ~~3.1.2~~ | **3.1.1** | ‚úÖ Latest stable |
| WebRTC | ~~3.0.0-pre.7~~ | **3.0.0-pre.8** | ‚úÖ Already newer |
| glTFast | Not installed | **MUST ADD 6.9.0** | ‚ö†Ô∏è REQUIRED |

**Update Packages/manifest.json**:
```json
{
  "dependencies": {
    "com.unity.cloud.gltfast": "6.9.0"
  }
}
```

---

## DOCUMENT VERSION

**Version**: 1.0
**Created**: 2025-10-25
**Last Updated**: 2025-10-25
**Author**: Claude Code Deep Analysis
**Status**: Ready for Implementation

---

## APPENDIX A: AUTHORITATIVE SOURCES - GITHUB REPOSITORIES

**Last Updated**: 2025-10-25
**Purpose**: Curated list of authoritative GitHub repositories for RGBD holograms, WebRTC streaming, and multiplayer XR

---

### üéØ PRIORITY REFERENCES

#### Record3D (RGBD Hologram Capture & Streaming) ‚≠ê PRIMARY REFERENCE

**Author**: [marek-simonik](https://github.com/marek-simonik)

**Core Projects**:
1. **[record3d-simple-wifi-streaming-demo](https://github.com/marek-simonik/record3d-simple-wifi-streaming-demo)**
   - Simple WiFi streaming of RGBD data
   - iOS LiDAR ‚Üí WiFi ‚Üí Unity receiver
   - Minimal example for learning architecture
   - **Use Case**: Foundation for Section 8 (XRAI Local Holograms)

2. **[record3d_unity_streaming](https://github.com/marek-simonik/record3d_unity_streaming)**
   - Production-ready Unity streaming integration
   - Real-time RGBD point cloud rendering
   - iOS companion app + Unity receiver
   - **Use Case**: Direct integration pattern for Portals

3. **[record3d_offline_unity_demo](https://github.com/marek-simonik/record3d_offline_unity_demo)**
   - Offline RGBD video playback in Unity
   - MP4 format handling (H.265 with depth in alpha)
   - **Use Case**: Section 8.4 (4D Animated Holograms)

4. **[record3d-wifi-streaming-and-rgbd-mp4-3d-video-demo](https://github.com/marek-simonik/record3d-wifi-streaming-and-rgbd-mp4-3d-video-demo)**
   - Comprehensive demo: Streaming + Recording + Playback
   - Full pipeline: Capture ‚Üí Stream ‚Üí Record ‚Üí Playback
   - **Use Case**: Complete reference for Sections 8-10

**Why Record3D is Authoritative**:
- ‚úÖ iOS LiDAR + RGB capture (same as our stack)
- ‚úÖ Real-time WiFi streaming (proven architecture)
- ‚úÖ Unity integration (direct compatibility)
- ‚úÖ MP4 RGBD format (H.265 video standard)
- ‚úÖ Production app on App Store (battle-tested)
- ‚úÖ Active maintenance (2024 updates)
- ‚úÖ Simple, elegant architecture (minimal dependencies)

**Architecture Pattern** (from Record3D):
```
iOS App (Record3D)
  ‚îú‚îÄ ARKit LiDAR + RGB capture
  ‚îú‚îÄ RGBD encoding (depth in luma channel)
  ‚îî‚îÄ WiFi UDP streaming ‚Üí Unity

Unity Receiver
  ‚îú‚îÄ RGBD decoding
  ‚îú‚îÄ Point cloud generation (compute shader)
  ‚îî‚îÄ VFX Graph rendering
```

**Integration Notes for Portals**:
- Record3D uses custom UDP protocol ‚Üí Adapt to WebRTC for Section 10
- Point cloud shader ‚Üí Extend to VFX Graph + GSplat rendering (Section 8.4-8.5)
- Offline playback ‚Üí Add to Section 8.4 (4D mode)

---

#### Needle Engine (WebRTC Multiplayer & Cross-Platform) ‚≠ê ALTERNATIVE REFERENCE

**Author**: [needle-tools](https://github.com/needle-tools)

**Core Projects**:
1. **[needle-engine-support](https://github.com/needle-tools/needle-engine-support)**
   - WebRTC-based multiplayer framework
   - Unity ‚Üí Web export with automatic networking
   - Quest, visionOS, mobile, desktop support
   - **Use Case**: Section 10-11 (WebRTC Multiplayer, cross-platform)

2. **[needle-console](https://github.com/needle-tools/needle-console)**
   - Remote debugging for XR devices
   - WebRTC-based console streaming
   - **Use Case**: Development workflow optimization

3. **[UnityGLTF](https://github.com/needle-tools/UnityGLTF)**
   - Fast glTF export/import for Unity
   - Runtime asset loading
   - **Use Case**: Asset sharing in multiplayer scenes

4. **[needle-engine-modules](https://github.com/needle-tools/needle-engine-modules)**
   - Modular components for XR experiences
   - Networking, physics, UI, audio modules
   - **Use Case**: Reference for modular architecture (Section 8.3 - Universal RGBD Pipeline)

**Why Needle Engine is Authoritative**:
- ‚úÖ WebRTC multiplayer (proven cross-platform)
- ‚úÖ Unity + Web + Quest + visionOS (complete stack)
- ‚úÖ Production deployments (used by studios)
- ‚úÖ Active development (2024-2025)
- ‚úÖ Modular architecture (best practice patterns)
- ‚úÖ Open source + commercial support

**Architecture Pattern** (from Needle Engine):
```
Unity Scene
  ‚îú‚îÄ Needle components (auto-detected)
  ‚îî‚îÄ Export to glTF + JS

Web Client
  ‚îú‚îÄ Three.js renderer
  ‚îú‚îÄ WebRTC networking (automatic)
  ‚îî‚îÄ XR API support (WebXR)

Quest/visionOS Native
  ‚îú‚îÄ Unity runtime OR web view
  ‚îî‚îÄ WebRTC peer connection
```

**Integration Notes for Portals**:
- Needle's WebRTC architecture ‚Üí Adapt for RGBD streaming (Section 10)
- glTF export ‚Üí Alternative to Normcore for asset sync (Section 11)
- Cross-platform approach ‚Üí Expand Portals to visionOS (future)

---

#### Cross-Platform Publish Strategy (Needle + Normcore + Icosa) ‚úÖ Research (2025-11-02)

**Goal**: Achieve ‚Äúedit once, publish everywhere‚Äù by aligning native (Normcore), web (Needle), and asset (Icosa/Open Brush) stacks for Weeks 17-20 deliverables.

**Validated References**
- **Open Brush & Icosa**
  - [Open Brush README](https://github.com/icosa-foundation/open-brush) ‚Üí Shipping builds for Quest, Steam, Rift, Viveport, Pico, Windows (Unity 2022.3.34f1).
  - [Open Brush File Format](https://github.com/icosa-foundation/open-brush-docs/blob/master/developer-notes/open-brush-file-format.md) ‚Üí `.tilt` = zip header + `data.sketch` strokes + `metadata.json`.
  - [Open Brush Toolkit](https://github.com/icosa-foundation/open-brush-toolkit) ‚Üí Unity SDK + Python tools for glTF/FBX/OBJ exports (URP branch available).
  - [Icosa Gallery README](https://github.com/icosa-foundation/icosa-gallery) + [Icosa Toolkit Unity](https://github.com/icosa-foundation/icosa-toolkit-unity) ‚Üí REST API, editor/runtime import, glTF2/OBJ support, plugins for Blender/Godot/Web.
- **Needle Engine**
  - [Feature Overview](https://engine.needle.tools/docs/features-overview.html) + [XR docs](https://engine.needle.tools/docs/xr.html) ‚Üí Desktop, mobile, WebXR, Quest, Pico, HoloLens, Vision Pro, Android Chrome/Firefox, iOS QuickLook (via USDZ exporter).
  - [Everywhere Actions](https://engine.needle.tools/docs/everywhere-actions.html) ‚Üí No-code interactions, iOS QuickLook pipeline (USDZExporter + WebXR).
  - [Networking](https://engine.needle.tools/docs/networking.html) ‚Üí WebSocket rooms, JSON + Flatbuffers, `SyncedRoom`, `SyncedTransform`, `VoIP`, view-only IDs.
- **Normcore**
  - [Platforms Overview](https://github.com/NormalVR/Normcore-Documentation/blob/main/docs/platforms/readme.md) ‚Üí iOS, Android, tvOS*, Windows, macOS, Linux, WebGL, PS4/5, Xbox, Switch*, Quest, PSVR, ARKit, ARCore, HoloLens*, Magic Leap, SteamVR.
  - [WebGL Guide](https://github.com/NormalVR/Normcore-Documentation/blob/main/docs/platforms/webgl.md) ‚Üí Browser build path with parity networking (FMOD voice chat limitation).
  - [AR Spectator Guide](https://github.com/NormalVR/Normcore-Documentation/blob/main/docs/guides/using-ar-as-a-spectator-view.md) ‚Üí AR Foundation spectator app hooked to Normcore room (shared app key).
  - [Multiplayer Drawing Guide](https://github.com/NormalVR/Normcore-Documentation/blob/main/docs/guides/creating-a-multiplayer-drawing-app.md) ‚Üí Authoritative brush sync pattern.

**Action Plan**
1. **Normcore Cross-Platform Sprint**
   - Import WebGL preview package, build Hoverbird sample, validate multi-client (desktop/Quest/WebGL).
   - Document hosting/TLS requirements, audio caveats, matcher scaling.
2. **Needle Export Prototype**
   - Export Portals mini-scene via Needle ExportInfo; enable `WebXR` + `USDZExporter`; confirm WebXR + QuickLook paths; exercise `SyncedRoom` sample.
   - Evaluate bridge feasibility (Node relay vs. migrating networking to Needle) using JSON room state.
3. **Icosa Asset Flow**
   - Wire `icosa-toolkit-unity` for editor/runtime import; ensure `.tilt ‚Üí glTF` pipeline using Open Brush Toolkit; map metadata fields into Portals brush presets.
   - Prototype runtime fetch from Icosa REST and instantiate within EnchantedPaintbrush.
4. **Architecture RFC (Week 17 deliverable)**
   - Define shared asset format (glTF + Open Brush metadata).
   - Outline synchronization layer (Normcore RealtimeModel ‚Üî Needle JSON).
   - Decide on deployment flow (native builds, Needle exports, Icosa CDN) and hosting requirements.
5. **Milestone Checks**
   - Normcore WebGL build with cross-device parity.
   - Needle WebXR build accessible on desktop/Quest + QuickLook preview.
   - Icosa asset imported in both native and web clients.
   - Cross-session smoke test: brush stroke drawn in Normcore client renders in Needle viewer.

**Outputs Required**
- Updated timeline (Weeks 17-20) reflecting cross-platform tasks.
- Documentation updates (priorities doc + knowledge base).
- Prototype builds + architecture RFC.

---

### üîß EXISTING KEIJIRO REFERENCES (Already in Use)

#### Keijiro Takahashi (VFX & Graphics Pioneer)

**Profile**: [github.com/keijiro](https://github.com/keijiro)

**Core Projects for XRAI**:
1. **[Rcam4](https://github.com/keijiro/Rcam4)**
   - Real-time LiDAR camera system (iOS ‚Üí NDI streaming)
   - Reference for Section 8.1-8.2 (XRAI Core Architecture)

2. **[Metavido](https://github.com/keijiro/Metavido)**
   - RGBD video recording + playback
   - Reference for Section 8.4 (4D Holograms)

3. **[Bibcam](https://github.com/keijiro/Bibcam)**
   - Earlier version of Rcam (still relevant patterns)
   - Reference for compute shader techniques

4. **[SMRVFX](https://github.com/keijiro/SMRVFX)**
   - VFX Graph utilities for point clouds
   - Reference for Section 8.4 (VFX Graph techniques)

5. **[MetaMesh](https://github.com/keijiro/MetaMesh)**
   - Mesh-based hologram rendering
   - Reference for Section 8.6 (Mesh Extrusion)

6. **[MetaWire](https://github.com/keijiro/MetaWire)**
   - Wireframe hologram effects
   - Reference for alternative rendering styles

**Why Keijiro is Authoritative**:
- ‚úÖ 300+ repositories (VFX, graphics, audio, AR)
- ‚úÖ Unity Technologies collaborator
- ‚úÖ Production-proven techniques
- ‚úÖ Cutting-edge research (2024-2025 updates)
- ‚úÖ Clean, minimal code (best practices)
- ‚úÖ Active community support

---

### üé® UNITY OFFICIAL EXAMPLES

#### Unity Technologies

**Profile**: [github.com/Unity-Technologies](https://github.com/Unity-Technologies)

**Key Projects**:
1. **[XR Interaction Toolkit Examples](https://github.com/Unity-Technologies/XR-Interaction-Toolkit-Examples)**
   - Official XRI 3.1.2 examples
   - Reference for hand tracking, interactables, locomotion

2. **[AR Foundation Samples](https://github.com/Unity-Technologies/arfoundation-samples)**
   - Official AR Foundation examples (6.1.0)
   - Reference for Section 8.2 (ARCameraManager, AROcclusionManager)

3. **[MR Example - Meta OpenXR](https://github.com/Unity-Technologies/mr-example-meta-openxr)**
   - Mixed reality example using Meta OpenXR SDK
   - Reference for Quest integration patterns

---

### üåê COMMUNITY PROJECTS

#### Multiplayer & Networking

1. **[NormalVR/Normcore-Samples](https://github.com/NormalVR/Normcore-Samples)**
   - Official Normcore examples
   - Reference for Section 11 (Normcore Multiplayer)

2. **[absurd-joy/Quest-hands-for-Normcore](https://github.com/absurd-joy/Quest-hands-for-Normcore)**
   - Hand tracking + Normcore integration
   - Reference for Section 2 (Hand Tracking) + Section 11

3. **[calebcram/Passthrough-Online-MRTK_Quest-Sample](https://github.com/calebcram/Passthrough-Online-MRTK_Quest---Sample)**
   - Multiplayer hand tracking example
   - Reference for Quest passthrough + networking

#### VFX & Particle Systems

1. **[icosa-foundation/open-brush](https://github.com/icosa-foundation/open-brush)**
   - Open source Tilt Brush (Google)
   - Reference for Section 0 (Particle Brush System)
   - **Architecture**: GeniusParticlesBrush (10-50x faster than Unity ParticleSystem)

2. **[dilmerv/MixedRealityUtilityKitDemos](https://github.com/dilmerv/MixedRealityUtilityKitDemos)**
   - Meta MRUK demos
   - Reference for spatial mapping, AR effects

#### Testing & CI/CD

1. **[needle-mirror/com.unity.test-framework](https://github.com/needle-mirror/com.unity.test-framework)**
   - Unity Test Framework mirror
   - Reference for automated testing

2. **[GameCI](https://game.ci/docs/github/test-runner/)**
   - GitHub Actions for Unity
   - Reference for CI/CD pipelines

---

### üìä QUICK REFERENCE TABLE

| Use Case | Primary Reference | Alternative | Notes |
|----------|-------------------|-------------|-------|
| **RGBD Capture** | Record3D (marek-simonik) | Keijiro Rcam4 | Record3D = simpler, Rcam4 = more flexible |
| **WebRTC Streaming** | Needle Engine | Unity WebRTC Package | Needle = full stack, Unity pkg = lower level |
| **VFX Holograms** | Keijiro SMRVFX | Unity VFX Samples | Keijiro = production patterns |
| **Particle Brushes** | Open Brush | Paint-AR (existing) | Open Brush = 10-50x faster |
| **Multiplayer Sync** | Normcore Samples | Needle Engine | Normcore = specialized, Needle = general |
| **Hand Tracking** | Unity XRI Examples | Quest-hands-for-Normcore | XRI = official, Quest-hands = practical |
| **Cross-Platform** | Needle Engine | Unity WebGL + XR API | Needle = turnkey, WebGL = manual |

---

### üîó INTEGRATION STRATEGY

#### For Section 8 (XRAI Local Holograms):
1. **Base Architecture**: Record3D offline demo (RGBD capture pattern)
2. **VFX Rendering**: Keijiro SMRVFX (particle techniques)
3. **Compute Shaders**: Keijiro Rcam4 (depth unprojection)

#### For Section 9-10 (WebRTC Streaming):
1. **Streaming Protocol**: Record3D WiFi demo (adapt UDP ‚Üí WebRTC)
2. **WebRTC Framework**: Unity WebRTC Package (official)
3. **Cross-Platform Reference**: Needle Engine (architecture patterns)

#### For Section 11 (Normcore Multiplayer):
1. **Base Integration**: Normcore Samples (official patterns)
2. **Hand Tracking Sync**: Quest-hands-for-Normcore (proven approach)
3. **Alternative**: Needle Engine (if cross-platform expansion needed)

---

### üìù MAINTENANCE NOTES

**Update Frequency**: Check for new releases quarterly
- Record3D: Check App Store updates (iOS app changes may affect API)
- Needle Engine: Active development (weekly releases)
- Keijiro: Monthly updates typical
- Unity Official: Tied to Unity LTS releases

**Deprecation Watch**:
- None currently - all projects actively maintained (2024-2025)

**Contact**:
- Record3D: Support via App Store contact
- Needle Engine: GitHub issues + Discord
- Keijiro: GitHub issues only (no Discord)
- Unity: Forums + GitHub issues

---

**END OF APPENDIX A**

---

## 15. UNIVERSAL SPATIAL MEDIA FORMAT RESEARCH (XRAI/VNMF)

**Priority**: P3 (Long-term Research, ongoing)
**Status**: ‚úÖ RESEARCH PHASE ACTIVE (Documentation created 2025-11-02)
**Estimated Time**: Ongoing research (100+ hours total over 6-12 months)
**Dependencies**: None (parallel research track)

---

### 15.1 STRATEGIC VISION & GOALS

**Goal**: "Identify or create single lightweight spatial media format that is optimal for the future of open source AI model sharing & cross platform multiplayer XR dynamic world sharing - the DNA of rich interactive immersive spatial media"

**Guiding Principles**:
- **Generative, procedural approach**: "From simple rules comes infinite complexity" (Stephen Wolfram)
- **Perception-first** vs geometry-first or frame-first design
- **Hypergraph with provenance** (Tim Berners-Lee, Ted Nelson)
- **Build on success factors** of Linux, JPEG, glTF (open, simple, universal)
- **Improve upon USD/USDZ** (too heavy, no neural fields, no AI)

---

### 15.2 RESEARCH TRACKS

#### Track 1: XRAI Format (eXtended Reality AI)

**Status**: Specification phase, not yet implemented
**Source**: `/Users/jamestunick/Applications/WarpJobs/.archive/example-scripts/vis/xrai-format`

**Architecture**:
- Binary container with 11 section types (Magic: "XRAI", 16-byte header)
- Hybrid geometry: Traditional meshes + Gaussian Splats + NeRF + point clouds
- AI-native: ONNX neural networks, adaptation rules, behavior models
- VFX Graph system for dynamic effects
- Roadmap: 2025-2026 (core) ‚Üí 2026-2027 (advanced) ‚Üí 2027-2030 (neural interfaces)

**Key Innovations**:
1. **Hybrid Geometry**: Mesh (compatibility) + Splats (500k @ 30fps) + NeRF (photorealistic)
2. **AI Components** (Section ID 6): ONNX weights, adaptation rules, behavior models
3. **VFX Systems** (Section ID 7): Particle systems, shader programs, VFX Graph nodes
4. **Progressive Fidelity**: Content adapts to device capabilities (Quest 2 ‚Üí Vision Pro)

**Comparison with glTF/USD**:
| Feature | XRAI | glTF | USD |
|---------|------|------|-----|
| Neural Fields | ‚úì | ‚úó | ‚úó |
| Gaussian Splats | ‚úì | ‚àº* | ‚úó |
| AI Components | ‚úì | ‚úó | ‚úó |
| Real-time Adaptation | ‚úì | ‚úó | ‚úó |
| Collaboration | ‚úì | ‚úó | ‚àº |

*Through extensions

---

#### Track 2: VNMF (Volumetric Neural Media Format)

**Status**: Prototype phase (encoder stubs, Unity/WebXR loaders)
**Source**: `/Users/jamestunick/Applications/WarpJobs/.archive/example-scripts/vis/UNIVERSAL SPATIAL FILE FORMAT/vnmf-full`

**Core Philosophy**: **"Perception-First"**
- Traditional Video: Frame-first (sequence of 2D images)
- Traditional 3D: Geometry-first (meshes, vertices, triangles)
- **VNMF**: Perception-first (neural fields responding to viewer position/context in real-time)

**Architecture**:
```
my-object.vnmf/ (ZIP or binary bundle)
‚îú‚îÄ‚îÄ manifest.json              # Orchestrates all layers
‚îú‚îÄ‚îÄ lightfield/                # Visual layer (3DGS/NeRF)
‚îú‚îÄ‚îÄ audiofield/                # Spatial audio layer (ambisonics)
‚îú‚îÄ‚îÄ interaction/               # Behavior layer (ONNX)
‚îú‚îÄ‚îÄ semantic/                  # Metadata layer (labels)
‚îú‚îÄ‚îÄ environment/               # Lighting/integration layer
‚îî‚îÄ‚îÄ fallback/                  # Compatibility layer
    ‚îú‚îÄ‚îÄ preview.jpg            # Static preview
    ‚îî‚îÄ‚îÄ mesh.gltf              # Always-compatible fallback
```

**Key Innovations**:
1. **Layered Perception**: 6 independent layers (visual, audio, interaction, semantic, environment, fallback)
2. **Always-Compatible Fallback**: Every VNMF object includes `fallback/mesh.gltf`
3. **ONNX Interaction Proxy**: Neural behavior models (gaze, tap, proximity)
4. **Spatial Audio Fields**: Ambisonic audio tied to viewer position

**Demo**: "Memory Vase" - Interactive VNMF object in Unity + WebXR

**Roadmap**:
- [x] Lightfield splat encoder (stubbed)
- [x] Audiofield encoder (stubbed)
- [x] Interaction graph encoder (stubbed)
- [x] Unity & WebXR fallback loaders
- [ ] Publish sample VNMF asset & build viewer launcher

---

#### Track 3: Neural Rendering Technologies

**Gaussian Splatting (3DGS)**:
- 500k-2M ellipsoidal splats @ 1080p 30fps
- Real-time on mobile (Quest 3, iPhone 15 Pro)
- Fast training: 5-30 minutes from multi-view images
- Smaller models: 20-80MB

**NeRF (Neural Radiance Fields)**:
- Photorealistic but slow (not real-time on mobile)
- Research: Jon Barron (Google) - Mip-NeRF, Zip-NeRF, SMERF
- **SMERF** (2024): Streamable partitioned NeRF for large scenes

**Instant-NGP** (NVIDIA):
- Hash-grid encoding, 5-10 second training
- Potential for real-time XR integration

---

#### Track 4: Standards Body Coordination

**Key Organizations**:
- **W3C** Immersive Web Working Group - WebXR, WebGPU
- **Khronos Group** - glTF, OpenXR, WebGL, Vulkan
- **Metaverse Standards Forum** - 3D Web Interoperability
- **Web3D Consortium** - X3D Working Group, Semantic Working Group
- **OGC** (Open Geospatial Consortium) - 3D Tiles, CityGML
- **IEEE** - Metaverse Working Group, 3D Body Processing
- **MPEG** (ISO/IEC SC29) - MPEG-I Scene Description

**3D Web Interoperability Goals** (Aug 2023 Charter):
1. Asset Interoperability - Standardize 3D asset formats
2. Tooling & Workflows - Improve DCC tools integration
3. Browser Capabilities - Extend WebXR for advanced spatial features
4. Networking & Synchronization - Standardize state sync protocols

---

#### Track 5: Hugging Face AI Integration

**Current Ecosystem**:
- 500K+ pretrained models
- Text-to-3D: Shap-E, Point-E, DreamFusion
- Image-to-3D: TripoSR, Wonder3D
- ONNX export ‚Üí XRAI/VNMF containers

**Integration Workflow**:
```
Hugging Face Model ‚Üí ONNX Export ‚Üí XRAI/VNMF Container ‚Üí Unity/WebXR Runtime
```

**Use Cases**:
1. Generative 3D Models - Text/image-to-3D asset creation
2. Neural Field Models - Pre-trained NeRF/3DGS models
3. Behavior Models - Character AI (dialogue, movement)
4. Style Transfer - Apply artistic styles to 3D content

---

### 15.3 IMPLICATIONS FOR PORTALS_6

**Near-term** (2025 Q1-Q2):
- Gaussian Splatting for portal interiors (real-time, mobile-friendly)
- ONNX Runtime integration for behavior models
- Prototype VNMF-style fallback system (gltf compatibility)

**Medium-term** (2025 Q3-Q4):
- ONNX behavior models from Hugging Face
- Procedural environment generation
- Adaptive VFX based on user preferences

**Long-term** (2026+):
- XRAI/VNMF hybrid format for Normcore multiplayer portals
- Neural field streaming over Normcore
- AI-driven content generation
- Cross-platform spatial media sharing

---

### 15.4 CONCEPTUAL FRAMEWORKS

#### Stephen Wolfram - Computational Universe

**Key Concepts**:
- Wolfram diagrams & hypergraphs - From simple rules comes infinite complexity
- Computational irreducibility - Emergent behavior cannot be predicted
- Hypergraph rewriting as universal computation

**Application to Spatial Media**:
- Procedural generation from simple rule sets
- Complex worlds from minimal initial conditions
- Generative systems that evolve over time
- Compression via simple rules + computation

---

#### Tim Berners-Lee & Ted Nelson - Hypergraph with Provenance

**Linked Data** (Berners-Lee):
- Everything has a URI
- HTTP for dereferencing
- RDF for semantic relationships
- SPARQL for queries

**Xanadu & Transclusion** (Ted Nelson):
- Every document has provenance
- Bi-directional links
- Version control built-in

**Application to Spatial Media**:
- Spatial objects as linked data (URI-addressable)
- Provenance tracking for AI model sharing
- Attribution for procedural generators
- Versioning and remix culture

---

#### Jon Barron - Neural Rendering Research

**Key Papers** (Google Research):
- NeRF: Neural Radiance Fields (2020)
- Mip-NeRF: Anti-Aliasing for NeRF (2021)
- Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields (2022)
- Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields (2023)
- **SMERF** (2024): Streamable Memory-Efficient Radiance Fields

**Implications**:
- Neural fields will become practical for XR (2-3 years)
- Streaming neural scenes over networks
- Hybrid explicit/implicit representations

---

### 15.5 SUCCESS FACTORS ANALYSIS

#### Why Linux Succeeded:
- Open source - Anyone can contribute, fork, modify
- Unix philosophy - Do one thing well, composability
- POSIX compliance - Standardized APIs
- Community-driven - Meritocracy, no single vendor lock-in
- Lightweight core - Kernel + modular components

#### Why JPEG Succeeded:
- Simple to implement - Widely accessible algorithms
- Good enough quality - 10:1 compression with acceptable loss
- Hardware acceleration - Easy to optimize
- Universal support - Every device, every browser
- ISO standard - Official blessing, stability

#### Why glTF Succeeded:
- Right timing - Filled gap between USD (too heavy) and OBJ (too simple)
- Web-first - Designed for HTTP, JavaScript, WebGL
- Khronos legitimacy - Same group as OpenGL, WebGL, OpenXR
- Extensibility - Can add features without breaking parsers
- Industry adoption - Unity, Unreal, Babylon.js, Three.js, Sketchfab

**Lessons for Spatial Media**:
- Open standards beat proprietary (even if initially inferior)
- Modularity enables evolution
- Community > single vendor
- Lightweight core + extensions
- Target the web platform
- "Good enough" beats "perfect"

---

### 15.6 IMPROVING UPON USD/USDZ

**USD Strengths to Preserve**:
- ‚úÖ Hierarchical composition (layers, variants, overrides)
- ‚úÖ Rich scene graph
- ‚úÖ Non-destructive editing
- ‚úÖ Strong ecosystem (Omniverse, Apple USDZ)

**USD Weaknesses to Address**:
- ‚ùå **Too heavy for real-time XR**: XRAI/VNMF are lightweight
- ‚ùå **Complex to implement**: 100K+ lines of code
- ‚ùå **Not web-friendly**: ASCII format (USDA) is verbose
- ‚ùå **No neural fields**: Designed for traditional CG
- ‚ùå **No AI integration**: No adaptation, no procedural AI

**Proposed Improvements**:
1. Binary-first design (like XRAI) - Efficient streaming, faster parsing
2. Neural field support (like XRAI/VNMF) - NeRF, Gaussian Splatting as first-class citizens
3. AI-native (like XRAI) - ONNX neural networks, adaptation rules
4. Perception-first (like VNMF) - View-dependent rendering, context-aware content
5. Always-compatible fallback (like VNMF) - Include traditional mesh representation

---

### 15.7 NEXT ACTIONS (ONGOING)

**Research Tasks**:
- [ ] Study Jon Barron's latest papers (SMERF, Zip-NeRF)
- [ ] Explore Instant-NGP Unity integration
- [ ] Benchmark Gaussian Splatting on Quest 3
- [ ] Test ONNX Runtime in Unity with Hugging Face models
- [ ] Prototype VNMF-style fallback system for Portals_6

**Documentation Tasks**:
- [x] Create comprehensive knowledgebase document (2025-11-02)
- [ ] Track W3C Immersive Web Working Group progress
- [ ] Monitor Metaverse Standards Forum deliverables
- [ ] Document Gaussian Splatting best practices
- [ ] Compile ONNX model conversion workflows

**Implementation Tasks**:
- [ ] Prototype hybrid mesh + neural field portal
- [ ] Test Normcore synchronization with neural content
- [ ] Implement adaptive LOD for Portals_6
- [ ] Explore procedural environment generation

---

### 15.8 DOCUMENTATION

**Primary Reference**:
`/Users/jamestunick/Documents/GitHub/Unity-XR-AI/KnowledgeBase/_WEB_INTEROPERABILITY_STANDARDS.md`

**Contents** (32KB comprehensive analysis):
- XRAI format specification summary (binary format, 11 sections, roadmap)
- VNMF format specification summary (6-layer architecture, perception-first)
- glTF/USD/VRM/MPEG-I comparative analysis
- Neural rendering technologies (NeRF, Gaussian Splatting, ONNX)
- Standards body coordination (W3C, Khronos, Metaverse Standards Forum)
- Hugging Face AI integration workflows
- Conceptual frameworks (Wolfram, Berners-Lee, Nelson, Barron)
- Success factors analysis (Linux, JPEG, glTF)
- USD/USDZ improvement proposals
- Recommendations for Unity-XR-AI development

**Updated**: 2025-11-02
**Status**: Active research phase

---

**END OF SECTION 15**

---

**END OF APPENDIX A**

---

**END OF IMPLEMENTATION PLAN**

Total Document Length: ~38,000 words
Total Implementation Time: 400-600 hours (7-10 months, 2 developers)
Total Features: 15 major integrations across 12 sections (including long-term research)
