# Unity Sentis ML Inference Guide

**Package**: `com.unity.ai.inference` (formerly com.unity.sentis)
**Version**: 2.4.0+
**Documentation**: https://docs.unity3d.com/Packages/com.unity.ai.inference@2.4/manual/index.html

---

## Overview

Unity Sentis is the neural inference engine for Unity, enabling on-device AI model execution across all platforms.

### Key Capabilities
- GPU-accelerated inference via compute shaders
- Cross-platform support (iOS, Android, WebGL, Standalone, Quest)
- ONNX model import with automatic optimization
- Streaming/async inference for responsive applications
- Model quantization (INT8, FP16) for mobile optimization

---

## Package Samples

### Sample Projects (Unity 6+)

| Sample | Description | Key Technologies |
|--------|-------------|------------------|
| **Chat Sample** | Interactive multimodal LLM chat | LLaVA OneVision, text+image input |
| **Tensor Conversion** | Convert data to/from tensors | Blit, ComputeBuffer, ReadbackAndClone |
| **Model Quantization** | Reduce model size for mobile | INT8, FP16 quantization |
| **Async Readback** | Non-blocking inference results | AsyncReadbackRequest |
| **Custom Layers** | Extend Sentis with custom ops | IComputeFunctionParams |

### Installing Samples

```
Window > Package Manager > AI > Sentis > Samples > Import
```

---

## Chat Sample (Multimodal LLM)

### Architecture

```
[User Input] → [Tokenizer] → [Text Embedder]
                                    ↓
[Image Input] → [Vision Encoder] → [LLaVA Decoder] → [Response]
```

### Components

| Component | Purpose |
|-----------|---------|
| `LlavaRunner.cs` | Main inference orchestrator |
| `LlavaTokenizer.cs` | Text tokenization (BPE) |
| `LlavaEmbedder.cs` | Text → embedding vectors |
| `LlavaVisionEncoder.cs` | Image → visual features |
| `LlavaDecoder.cs` | Generate response tokens |
| `ModelScheduler.cs` | Async model execution |
| `HfDownloader.cs` | HuggingFace model download |

### Model Details

**LLaVA OneVision (0.5B parameters)**
- Source: `huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-si-hf`
- Format: ONNX (quantized for mobile)
- Input: Text + optional image
- Output: Streaming text response

### Usage

```csharp
// Editor menu
Sentis > Sample > Chat > Download Models
Sentis > Sample > Chat > Start Chat

// Runtime scene
ChatSample/Assets/ChatLLM/Runtime/Scenes/Chat.unity
```

### Features
- Multimodal understanding (text + images)
- Streaming token-by-token responses
- Conversation history retention
- Redux-pattern state management
- Unity AppUI for modern interfaces

---

## Tensor Conversion Patterns

### Texture to Tensor

```csharp
using Unity.AI.Inference;

// CPU readback (slow, blocking)
var tensor = TextureConverter.ToTensor(texture);

// GPU tensor (fast, stays on GPU)
var gpuTensor = TextureConverter.ToTensor(texture, new TextureTransform()
    .SetDimensions(224, 224)
    .SetTensorLayout(TensorLayout.NHWC)
);
```

### Tensor to Texture

```csharp
// Blit to RenderTexture
TextureConverter.Blit(outputTensor, renderTexture);

// Create new Texture2D
var tex = TextureConverter.ToTexture(outputTensor);
```

### ComputeBuffer Interop

```csharp
// Direct buffer access (no copy)
var buffer = tensor.dataOnBackend.buffer;

// Copy to existing buffer
tensor.dataOnBackend.Download<float>(buffer);
```

---

## Model Optimization

### Quantization

```csharp
// Load and quantize
var model = ModelLoader.Load(modelPath);
var quantizedModel = ModelOptimizer.Quantize(model, QuantizationType.INT8);
```

### Memory Optimization

```csharp
// Use GPUCompute backend (fastest)
var worker = new Worker(model, BackendType.GPUCompute);

// Dispose after use
worker.Dispose();
```

### Batch Processing

```csharp
// Process multiple inputs
var batchTensor = new Tensor(new TensorShape(batchSize, 224, 224, 3));
worker.SetInput("input", batchTensor);
worker.Execute();
```

---

## Platform-Specific Notes

### iOS
- Metal compute shaders
- Use FP16 quantization for A-series chips
- Memory budget: ~1.5GB max model size

### Android
- Vulkan compute preferred
- NNAPI delegate optional
- Test on target GPU (Adreno, Mali, PowerVR)

### Quest
- Vulkan compute shaders
- FP16 required for 6 DoF tracking apps
- 4GB shared memory limit

### WebGL
- WebGPU required (Chrome 113+)
- Model size limited by memory
- Use smallest quantized models

---

## VFX Graph Integration

### Binding Inference Output to VFX

```csharp
public class SentisVFXBinder : MonoBehaviour
{
    public VisualEffect vfx;
    private Tensor outputTensor;

    void Update()
    {
        // Run inference
        worker.SetInput("input", inputTensor);
        worker.Execute();
        outputTensor = worker.PeekOutput("output");

        // Convert to texture for VFX
        TextureConverter.Blit(outputTensor, segmentationTexture);
        vfx.SetTexture("SegmentationMap", segmentationTexture);
    }
}
```

### Segmentation Mask to Particles

```csharp
// YOLO11 segmentation → VFX particle spawn
var maskTensor = worker.PeekOutput("segmentation_mask");
TextureConverter.Blit(maskTensor, maskTexture);
vfx.SetTexture("SpawnMask", maskTexture);
vfx.SetFloat("SegmentCount", detectedClasses);
```

---

## References

- [Sentis Package Manual](https://docs.unity3d.com/Packages/com.unity.ai.inference@2.4/manual/index.html)
- [Sentis GitHub Samples](https://github.com/Unity-Technologies/sentis-samples)
- [ONNX Model Zoo](https://github.com/onnx/models)
- [HuggingFace Hub](https://huggingface.co/models?library=onnx)

---

## Related KB Files

- `_ONNXRUNTIME_UNITY_EXAMPLES.md` - Alternative ONNX Runtime integration
- `_YOLO_SEGMENTATION_UNITY.md` - YOLO11 segmentation patterns
- `_VFX25_HOLOGRAM_PORTAL_PATTERNS.md` - VFX integration patterns

---

*Created: 2026-01-20*
*Source: Unity Sentis 2.4.0 documentation and samples*
# Unity Sentis & ONNX Ecosystem for Mobile/XR: Comprehensive Research (2026-01-20)

**Research Date**: 2026-01-20
**Scope**: Unity Sentis (formerly Barracuda), ONNX Runtime, YOLO models, mobile ML inference for iOS/Android/Quest/WebGL
**Target Platforms**: iPhone (iOS 15+), Android, Meta Quest 2/3/Pro, WebGL
**Unity Versions**: Unity 6 (2023+), Unity 2022 LTS

## Executive Summary

Unity Sentis (formerly Barracuda, now called **Inference Engine** as of Unity 6.2) is Unity's official neural network inference library for on-device ML/CV. While ONNX Runtime offers better cross-platform performance and hardware acceleration, Sentis provides tighter Unity integration and simplified deployment.

**Key Findings**:
- ✅ **Sentis supports all Unity platforms** (iOS, Android, Quest, WebGL) with caveats
- ⚠️ **WebGL has significant limitations** - no compute shaders, single-threaded WASM, memory leaks with GPUPixel
- ✅ **Mobile-optimized models exist** - MobileNet, EfficientNet, YOLOv8n, BodyPix work well on mobile
- ⚠️ **Performance gaps** - Sentis ~8x slower than native frameworks (TensorRT, CoreML) but acceptable for 30-60fps use cases
- ✅ **Quantization supported** - FP16/INT8 quantization available, 4.2x compression with <0.5% accuracy loss
- ⚠️ **Quest 3 achieves ~60fps** for YOLOv8 detection, ~5fps for segmentation (Unity Sentis)
- ✅ **ONNX Runtime Unity plugin** available (asus4/onnxruntime-unity) with CoreML/DirectML/Metal backends

---

## 1. Unity Sentis (Inference Engine)

### 1.1 Current Version & Rebranding

**Package Name**: `com.unity.ai.inference` (formerly `com.unity.sentis`)
**Latest Version**: 2.4.1 (as of 2026-01-20)
**Rebrand**: Sentis → Inference Engine (Unity 6.2+ / Unity AI)
**Documentation**: https://docs.unity3d.com/Packages/com.unity.ai.inference@latest
**GitHub**: https://github.com/Unity-Technologies/sentis-samples (official samples)

**Note**: "Sentis is now called Inference Engine, and the documentation has moved to the Unity AI Inference package."

### 1.2 Platform Support

| Platform | CPU Backend | GPU Backend | Limitations |
|----------|-------------|-------------|-------------|
| **iOS** | ✅ Burst (WASM) | ✅ Metal (GPUCompute) | No CoreML direct access, quantization recommended |
| **Android** | ✅ Burst | ✅ Vulkan (GPUCompute) | Requires Vulkan-capable device (Android 7.0+) |
| **Quest 2/3** | ✅ Burst | ✅ Vulkan | Thermal throttling on Quest 2, ~60fps for small models |
| **WebGL** | ⚠️ WASM (slow) | ⚠️ GPUPixel only | No compute shaders, single-threaded, memory leaks, kernel errors |
| **Windows** | ✅ Burst | ✅ DirectX (GPUCompute) | DirectML not directly supported (use ONNX Runtime) |
| **macOS** | ✅ Burst | ✅ Metal | No ANE (Apple Neural Engine) access |
| **Linux** | ✅ Burst | ✅ Vulkan | - |

**Sources**:
- [Sentis support for Unity Web platform](https://discussions.unity.com/t/sentis-support-for-the-unity-web-platform/1552688)
- [Does Sentis work on Android mobile?](https://discussions.unity.com/t/does-sentis-work-on-android-mobile/346403)
- [Unity Sentis For On-Device ML/CV Models](https://developers.meta.com/horizon/documentation/unity/unity-pca-sentis/)

### 1.3 Backend Types

```csharp
// Unity Sentis 2.x
BackendType.GPUCompute  // Fastest - compute shaders (Metal/Vulkan/DirectX)
BackendType.GPUPixel    // Fallback - fragment shaders (WebGL only)
BackendType.CPU         // Burst-compiled WASM (slow on WebGL)
```

**Performance Ranking**:
1. `GPUCompute` - 10-100x faster than CPU (when available)
2. `CPU` - Burst-optimized, acceptable for small models
3. `GPUPixel` - WebGL only, avoid if possible (memory leaks reported)

**WebGL-Specific Issues**:
- ❌ `BackendType.GPUCompute` not supported (no compute shaders)
- ⚠️ `BackendType.CPU` compiles to single-threaded WASM (slow)
- ⚠️ `BackendType.GPUPixel` has memory leaks, kernel errors ("Transpose not found")
- ❌ No C# multithreading due to WebAssembly limitations

**Sources**:
- [Create an engine to run a model](https://docs.unity3d.com/Packages/com.unity.sentis@1.1/manual/create-an-engine.html)
- [Able to run on WebGL?](https://discussions.unity.com/t/able-to-run-on-webgl-argumentexception-kernel-transpose-not-found/294082)

### 1.4 Model Format Support

| Format | Opset | Notes |
|--------|-------|-------|
| **ONNX** | 7-15 | Primary format, most tested |
| **TensorFlow Lite** | - | Supported via LiteRT (formerly TFLite) |
| **.sentis** | - | Pre-optimized Unity format (from ONNX) |

**Conversion Tools**:
- Unity provides ONNX → .sentis converter (quantization, optimization)
- Meta provides converter tool for Quest deployment
- Hugging Face models often available in .sentis format

**Sources**:
- [Supported ONNX operators](https://docs.unity3d.com/Packages/com.unity.sentis@1.0/manual/supported-operators.html)
- [Supported models](https://docs.unity3d.com/Packages/com.unity.sentis@2.1/manual/supported-models.html)

### 1.5 Quantization & Optimization

**Quantization Options**:
- `None` - Full precision (FP32)
- `Float16` - Half precision (~50% size reduction, minimal accuracy loss)
- `Uint8` - 8-bit integer (~75% size reduction, requires calibration)

**Performance Gains (2025 Research)**:
- **4.2x model compression** with <0.5% mIoU loss (mixed-precision quantization)
- **18ms processing latency** per frame (MobileNet on mobile)
- **89.2% mAP** on COCO (optimized MobileNet)

**Techniques**:
- Mixed-precision quantization with dynamic range calibration
- Structured pruning (remove redundant parameters from nonessential layers)
- Frame slicing (spread inference across multiple frames)
- Custom backend dispatching (optimize specific operators)

**Code Example**:
```csharp
// Quantize model to FP16
var model = ModelLoader.Load(modelAsset);
var quantizedModel = ModelQuantizer.Quantize(model, QuantizationType.Float16);
```

**Sources**:
- [Quantize a Model](https://docs.unity3d.com/Packages/com.unity.sentis@2.1/manual/quantize-a-model.html)
- [Real-Time Object Detection and Boundary Extraction in AR](https://dl.acm.org/doi/10.1145/3746709.3746719)

---

## 2. ONNX Runtime for Unity

### 2.1 Unity Plugins

| Plugin | Backends | Platforms | Status |
|--------|----------|-----------|--------|
| [asus4/onnxruntime-unity](https://github.com/asus4/onnxruntime-unity) | CPU, CoreML, DirectML | iOS, Android, Windows, macOS | ✅ Active (2025) |
| [asus4/onnxruntime-unity-examples](https://github.com/asus4/onnxruntime-unity-examples) | Examples | Multi-platform | ✅ Active |
| [cj-mills/onnx-directml-unity-tutorial](https://github.com/cj-mills/onnx-directml-unity-tutorial) | DirectML (Windows) | Windows | ⚠️ Niche use case |

**asus4/onnxruntime-unity** is the most mature Unity ONNX Runtime plugin with ~500 stars.

**Sources**:
- [ONNX Runtime on Unity (Medium)](https://medium.com/@asus4/onnx-runtime-on-unity-a40b3416529f)
- [asus4/onnxruntime-unity GitHub](https://github.com/asus4/onnxruntime-unity)

### 2.2 Hardware Acceleration Backends

#### CoreML (iOS/macOS)
- **Platform**: iOS 13+, macOS 10.15+
- **Hardware**: CPU, GPU, Apple Neural Engine (ANE)
- **Performance**: 10-100x faster than CPU-only (ANE)
- **Caveats**: May silently convert FP32 → FP16 (accuracy loss)

**Setup**:
```csharp
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_CoreML(CoreMLFlags.COREML_FLAG_USE_CPU_AND_GPU);
var session = new InferenceSession(modelPath, sessionOptions);
```

**Sources**:
- [Apple - CoreML Execution Provider](https://onnxruntime.ai/docs/execution-providers/CoreML-ExecutionProvider.html)
- [ONNX Runtime & CoreML May Silently Convert](https://ym2132.github.io/ONNX_MLProgram_NN_exploration)

#### DirectML (Windows)
- **Platform**: Windows 10+
- **Hardware**: Any DirectX 12 GPU
- **Version**: DirectML 1.15.2 (as of ONNX Runtime 1.20)
- **Status**: Sustained engineering (new features moved to WinML)

**Unity Integration Issues**:
- ❌ DirectML.dll may not be found by Unity
- ⚠️ Requires manual placement in Unity folder
- ⚠️ NuGet packages need copying to Assets/Plugins/

**Sources**:
- [Windows - DirectML Execution Provider](https://onnxruntime.ai/docs/execution-providers/DirectML-ExecutionProvider.html)
- [Trying to get DirectML working in Unity](https://github.com/microsoft/onnxruntime/discussions/14165)

#### Metal (macOS GPU)
- **Access**: Via CoreML Execution Provider (not separate backend)
- **Performance**: GPU acceleration without ANE overhead
- **Note**: CoreML automatically uses Metal for GPU tasks

#### CUDA/TensorRT (NVIDIA)
- **Platform**: Windows, Linux (desktop/server)
- **Status**: Not practical for Unity mobile deployment
- **Use Case**: Research, non-mobile projects only

### 2.3 Performance: Sentis vs ONNX Runtime

**Community Reports**:
- Sentis: ~80ms inference (image enhancement, RTX 3090)
- TensorRT: ~10ms inference (same model, same GPU)
- **8x performance gap** between Sentis and native frameworks

**Why Use Sentis?**
- ✅ Tighter Unity integration (Editor + Player)
- ✅ Single codebase for all platforms
- ✅ No external dependencies
- ✅ Easier to deploy builds

**Why Use ONNX Runtime?**
- ✅ Better performance (native acceleration)
- ✅ Broader hardware support (ANE, CUDA, TensorRT)
- ✅ Faster model updates (no Unity package dependency)
- ❌ More complex integration (native plugins)

**Sources**:
- [Sentis vs Onnxruntime](https://discussions.unity.com/t/sentis-vs-onnxruntime/299328)
- [Expected Performance of Unity Sentis vs TensorFlow](https://discussions.unity.com/t/expected-performance-of-unity-sentis-vs-tensorflow/1674688)

---

## 3. YOLO Models for Unity Mobile/XR

### 3.1 YOLO Versions & Performance

| Model | FPS (iPhone 12) | FPS (Quest 3) | mAP (COCO) | Size | Notes |
|-------|-----------------|---------------|------------|------|-------|
| **YOLOv8n** | ~30-60fps | ~60fps | 37.3% | 6MB | Nano - mobile-optimized |
| **YOLOv8s** | ~20-30fps | ~30fps | 44.9% | 22MB | Small - good balance |
| **YOLOv11** | ~60fps | ~60fps | 61.5% | - | Latest (2025), faster than v8 |
| **YOLO-World** | ~5-10fps | ~5fps | - | Large | Open-vocabulary, slower |
| **YOLOv5n** | ~40-60fps | ~50fps | 28.0% | 4MB | Legacy, still viable |

**Key Findings**:
- ✅ **Quest 3 achieves ~60fps** for YOLOv8 detection (object detection sample)
- ⚠️ **Quest 3 achieves ~5fps** for YOLOv8-seg segmentation (semantic segmentation, Unity Sentis)
- ✅ **iPhone 12 with CoreML** - viable mobile performance for YOLO variants
- ⚠️ **NonMaxSuppression (NMS) bottleneck** - CPU-based NMS causes 60→27fps drop

**Optimization Tips**:
- ✅ Remove NMS layer from ONNX model, implement in C# with Burst
- ✅ Use GPU backends (Metal, Vulkan)
- ✅ Quantize to FP16 or INT8
- ✅ Use smaller model variants (YOLOv8n > YOLOv8s > YOLOv8m)

**Sources**:
- [AR Object Detection on Quest Pro using YOLOv8](https://www.linkedin.com/posts/zakaton_ar-object-detection-on-the-quest-pro-using-activity-7097050766925844480-e9Tl)
- [YOLOv8 object recognition model not suitable for mobile?](https://discussions.unity.com/t/yolov8-object-recognition-model-not-suitable-for-mobile/338745)
- [YOLOv1 to YOLOv11 Survey](https://arxiv.org/html/2508.02067v1)

### 3.2 Unity YOLO Implementations

**Official Unity Models**:
- [unity/inference-engine-yolo (Hugging Face)](https://huggingface.co/unity/inference-engine-yolo) - Verified for Unity Inference Engine

**Community Implementations**:
- [Ultralytics YOLOv8 for Unity](https://github.com/ultralytics/ultralytics/issues/12574) - Discussion thread
- Export to ONNX, use Unity Barracuda/Sentis

**Integration Steps**:
1. Export YOLO model to ONNX (Ultralytics CLI: `yolo export model=yolov8n.pt format=onnx`)
2. Convert to .sentis (optional, Unity Editor: Assets > Sentis > Convert ONNX Model)
3. Load in Unity:
   ```csharp
   var model = ModelLoader.Load(modelAsset);
   var worker = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);
   ```
4. Run inference:
   ```csharp
   var input = new TensorFloat(inputTexture);
   worker.Execute(input);
   var output = worker.PeekOutput() as TensorFloat;
   ```

**Sources**:
- [Ultralytics HUB App](https://docs.ultralytics.com/hub/app/)
- [Model Optimization Techniques for YOLO](https://medium.com/academy-team/model-optimization-techniques-for-yolo-models-f440afa93adb)

### 3.3 Mobile-Specific Considerations

**Quest 3 Performance**:
- ✅ 60fps achievable with YOLOv8n
- ⚠️ Thermal throttling on Quest 2 (use Quest 3/Pro)
- ⚠️ CPU-based NMS causes frame drops
- ✅ Unity Inference Engine supports Quest (Vulkan backend)

**iPhone Performance**:
- ✅ CoreML backend available via ONNX Runtime
- ✅ Apple Neural Engine acceleration (iOS 13+)
- ⚠️ FP32 → FP16 silent conversion (check accuracy)
- ✅ FPS measurements show viable mobile performance

**Android Performance**:
- ⚠️ Requires Vulkan-capable device (Android 7.0+)
- ⚠️ Hand tracking models work on PC but not Android (reported issue)
- ✅ GPU backend available (Vulkan)

**Sources**:
- [Sentis and Meta Quest object detection](https://discussions.unity.com/t/sentis-and-meta-quest-object-detection/356895)
- [Does Sentis require Vulkan on Android?](https://discussions.unity.com/t/does-sentis-require-valkan-on-android-device/297184)

---

## 4. Model Zoo for Unity Sentis

### 4.1 Hugging Face Unity Models

**Official Unity Namespace**: [unity/* models](https://huggingface.co/models?library=unity-sentis)

| Model | Task | Size | Verified |
|-------|------|------|----------|
| [unity/sentis-yolotinyv7](https://huggingface.co/unity/sentis-yolotinyv7) | Object detection | Small | ✅ |
| [unity/inference-engine-yolo](https://huggingface.co/unity/inference-engine-yolo) | Object detection | Multiple sizes | ✅ |

**All models under `unity/` namespace are verified to work in Unity.**

**Sources**:
- [Using Unity Sentis Models from Hugging Face](https://huggingface.co/docs/hub/en/unity-sentis)
- [Models compatible with unity-sentis](https://huggingface.co/models?library=unity-sentis)

### 4.2 MobileNet & EfficientNet

**MobileNet Variants**:
- MobileNetV1 - Original, good baseline
- MobileNetV2 - Inverted residuals, better accuracy
- MobileNetV3 - Neural architecture search, best efficiency
- MobileViT - Hybrid CNN-Transformer, state-of-art mobile

**Use Cases in Unity**:
- ✅ Image classification (AR object recognition)
- ✅ Feature extraction (style transfer, AR filters)
- ✅ Segmentation (person segmentation, background removal)

**Performance (2025 Research)**:
- **18ms latency** per frame (MobileNet-based architecture)
- **89.2% mAP** on COCO validation dataset
- Optimized via architectural pruning, executed in Unity Sentis

**EfficientNet**:
- EfficientNet-B0 to B7 (compound scaling)
- Best accuracy/efficiency trade-off
- Suitable for real-time low-power AR computing

**Sources**:
- [Real-Time Object Detection and Boundary Extraction in AR](https://dl.acm.org/doi/10.1145/3746709.3746719)

### 4.3 Pose Estimation Models

**Available Models**:
- **MediaPipe Pose** - 33 keypoints, BlazePose architecture
- **OpenPose** - Multi-person 2D pose, MobileNetV1 feature extractor
- **BodyPix** - Person segmentation + pose (used in MetavidoVFX)

**Unity Implementations**:
- [keijiro/BodyPixSentis](https://github.com/keijiro/BodyPixSentis) - 24-part body segmentation + 17 keypoints
- [homuler/MediaPipeUnityPlugin](https://github.com/homuler/MediaPipeUnityPlugin) - Full MediaPipe suite
- [keijiro/FaceLandmarkBarracuda](https://github.com/keijiro/FaceLandmarkBarracuda) - Face landmarks (468 points)

**Performance Notes**:
- ⚠️ 3D body pose in Sentis - community interest but limited examples
- ✅ 2D pose estimation works well (30-60fps on mobile)
- ⚠️ Hand detection models (e.g., Minis) - Quest support unclear

**Sources**:
- [Pose estimation project from Sentis keynote](https://discussions.unity.com/t/pose-estimation-project-from-sentis-keynote/315432)
- [3D Body pose in Sentis?](https://discussions.unity.com/t/3d-body-pose-in-sentis/349982)

### 4.4 Segmentation Models

**Models for Unity**:
- **BodyPix** - Person segmentation (Keijiro implementation)
- **U-Net** - Medical imaging, general segmentation
- **DeepLabV3** - Semantic segmentation (COCO, Cityscapes)
- **Segment Anything (SAM)** - Too large for mobile (not recommended)

**Performance**:
- ✅ BodyPix: Real-time on iPhone (30-60fps)
- ⚠️ YOLOv8-seg: 5fps on Quest 3 (Unity Sentis)
- ✅ U-Net variants: 10-30fps on mobile (depends on resolution)

**Sources**:
- [keijiro/BodyPixSentis](https://github.com/keijiro/BodyPixSentis)

### 4.5 ONNX Model Zoo

**Official Resources**:
- [ONNX Model Zoo](https://github.com/onnx/models) - Pre-trained models
- [Intel Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Optimized models

**Recommended Models for Mobile**:
- MobileNet v2/v3 (classification)
- EfficientNet-Lite (classification)
- SSD-MobileNet (object detection)
- YOLOv5n/v8n (object detection)
- U-Net (segmentation)
- ResNet-18/34 (feature extraction)

**Conversion**:
```bash
# Export PyTorch model to ONNX
torch.onnx.export(model, dummy_input, "model.onnx", opset_version=11)

# Import into Unity Sentis (drag-and-drop or use converter)
```

---

## 5. Unity Sentis Official Samples

**GitHub Repository**: [Unity-Technologies/sentis-samples](https://github.com/Unity-Technologies/sentis-samples)

**Stars**: 310 | **Forks**: 66 | **Status**: ✅ Active (2025)

### 5.1 Available Samples

| Sample | Description | Video |
|--------|-------------|-------|
| **Depth Estimation** | Monocular depth estimation | ✅ |
| **Star Simulation** | Physics simulation (tensor-based linear algebra) | ✅ |
| **Object Detection** | YOLO-based object detection | ❓ |
| **Segmentation** | Semantic segmentation | ❓ |

**Note**: Some samples contain video overviews linked in README files.

**Sources**:
- [sentis-samples README](https://github.com/Unity-Technologies/sentis-samples/blob/main/README.md)
- [Samples documentation](https://docs.unity3d.com/Packages/com.unity.sentis@2.1/manual/package-samples.html)

### 5.2 Community Examples

**Keijiro Takahashi** (Unity VFX/Graphics expert):
- [BodyPixSentis](https://github.com/keijiro/BodyPixSentis) - Body segmentation
- [Minis](https://github.com/keijiro/Minis) - Hand detection
- [SelfieBarracuda](https://github.com/keijiro/SelfieBarracuda) - Portrait segmentation (deprecated Barracuda)

**Other Notable Examples**:
- [Computer Vision Examples for Unity](https://rfilkov.com/2024/01/22/computer-vision-examples-for-unity/) - RF Solutions
- [HoloLens AI - YOLOv8 training](https://localjoost.github.io/HoloLens-AI-training-a-YoloV8-model-locally-on-custom-pictures-to-recognize-objects-in-3D-space/) - HoloLens 2

---

## 6. WebGL Limitations (Critical)

### 6.1 Known Issues

| Issue | Impact | Workaround |
|-------|--------|------------|
| **No Compute Shaders** | ❌ GPUCompute backend unavailable | Use GPUPixel (slower) |
| **Single-Threaded WASM** | ⚠️ CPU backend slow | Use smaller models, lower resolution |
| **Memory Leaks (GPUPixel)** | ⚠️ App crashes after extended use | Restart app, avoid GPUPixel if possible |
| **Kernel Errors** | ❌ "Transpose not found" | Check ONNX opset version (7-15) |
| **No WebNN Support** | ❌ No hardware acceleration | Wait for WebGPU/WebNN support |

**WebGL Status (2026)**:
- ⚠️ Sentis team working on WebGL/WebGPU support
- ⚠️ GPUPixel has memory leaks, maintenance unclear
- ⚠️ WebGL more accessible than WebGPU currently
- ❌ No WebNN integration announced

**Sources**:
- [Sentis support for Unity Web platform](https://discussions.unity.com/t/sentis-support-for-the-unity-web-platform/1552688)
- [Blaze Palm detector in Unity Sentis WebGL Build Error](https://discussions.unity.com/t/blaze-palm-detector-in-unity-sentis-webgl-build-error/350148)

### 6.2 WebGL Alternatives

**For WebGL Deployment**:
1. **Use smaller models** - YOLOv5n, MobileNetV3-Small
2. **Lower input resolution** - 256x256 or 320x320 instead of 640x640
3. **Frame skipping** - Run inference every 2-5 frames
4. **Pre-process on server** - Send frames to server for ML inference (if latency acceptable)
5. **Wait for WebGPU** - Better performance when widely supported

**Native Web ML Alternatives**:
- [TensorFlow.js](https://www.tensorflow.org/js) - WASM + WebGL backends
- [ONNX.js](https://github.com/microsoft/onnxjs) - ONNX Runtime for web (deprecated)
- [MediaPipe Web](https://mediapipe.dev) - Google's web ML framework

---

## 7. Migration from Unity Barracuda

### 7.1 Deprecation Timeline

| Date | Event |
|------|-------|
| 2023 | Sentis announced (Unity 2023+) |
| 2024 | Barracuda deprecated (maintenance mode) |
| 2025 | ML-Agents upgraded to Sentis 1.2.0-exp.2 |
| 2026 | Sentis → Inference Engine rebrand (Unity 6.2) |

**Status**: Barracuda replaced by Sentis, Unity stopped active development.

**Sources**:
- [Unity Barracuda GitHub](https://github.com/Unity-Technologies/barracuda-release)
- [ML-Agents Release 21](https://github.com/Unity-Technologies/ml-agents/releases/tag/release_21)

### 7.2 Migration Steps

**1. Namespace Changes**:
```csharp
// OLD (Barracuda)
using Unity.Barracuda;

// NEW (Sentis)
using Unity.Sentis;
```

**2. Tensor Constructor**:
```csharp
// OLD
var tensor = new Tensor(data);

// NEW
var tensor = new TensorFloat(data);  // or TensorInt
```

**3. Backend Type**:
```csharp
// OLD
WorkerFactory.Type.Compute
WorkerFactory.Type.CSharpBurst

// NEW
BackendType.GPUCompute
BackendType.CPU
```

**4. Tensor Layout**:
- ⚠️ Sentis no longer auto-converts tensor layouts
- Must manually ensure input/output tensors match expected layout

**Sources**:
- [Upgrade from Barracuda 3.0 to Sentis 1.0](https://docs.unity3d.com/Packages/com.unity.sentis@1.1/manual/upgrade-guide.html)
- [Upgrading from Barracuda to Sentis with MLAgents](https://discussions.unity.com/t/upgrading-from-barracuda-to-sentis-with-mlagents/268633)

---

## 8. Practical Recommendations for MetavidoVFX

### 8.1 Current Usage (BodyPixSentis)

**Existing Implementation**:
- **Model**: BodyPix (24-part body segmentation + 17 pose keypoints)
- **Package**: `keijiro/BodyPixSentis` (Unity Sentis wrapper)
- **Output**: MaskTexture (24 body parts), KeypointBuffer (17 landmarks)
- **Performance**: Real-time on iPhone (30-60fps with 512x384 resolution)

**File**: `/Users/jamestunick/Documents/GitHub/Unity-XR-AI/MetavidoVFX-main/Assets/Scripts/Segmentation/BodyPartSegmenter.cs`

### 8.2 Potential Additions

**1. Hand Pose Estimation**:
- **Model**: MediaPipe Hands (21 landmarks per hand)
- **Implementation**: [homuler/MediaPipeUnityPlugin](https://github.com/homuler/MediaPipeUnityPlugin)
- **Use Case**: Gesture-based VFX control, hand-tracking interactions
- **Platform**: iOS, Android (Quest has native hand tracking via XR Hands)

**2. Object Detection (AR Placement)**:
- **Model**: YOLOv8n (6MB, 30-60fps on iPhone)
- **Source**: [unity/inference-engine-yolo](https://huggingface.co/unity/inference-engine-yolo)
- **Use Case**: Voice-to-object placement ("Put a cat here"), AR object avoidance
- **Integration**: Combine with Icosa/Sketchfab search (Spec 009)

**3. Depth Estimation (AR Fallback)**:
- **Model**: MiDAS v2.1 Small (256x256)
- **Source**: Unity Sentis samples (Depth Estimation)
- **Use Case**: Monocular depth when LiDAR unavailable (older iPhones, Android)
- **Performance**: 10-30fps on mobile

**4. Face Mesh (VFX Binding)**:
- **Model**: MediaPipe Face Mesh (468 landmarks)
- **Current**: [mao-test-h/FaceTracking-VFX](https://github.com/mao-test-h/FaceTracking-VFX) uses ARKit Face
- **Enhancement**: Use Sentis for cross-platform face tracking (Android)

### 8.3 Performance Targets

| Platform | Target FPS | ML Budget | Notes |
|----------|------------|-----------|-------|
| **iPhone 12+** | 60fps | 16ms | Use CoreML via ONNX Runtime for best perf |
| **iPhone X-11** | 30fps | 33ms | Sentis FP16 quantization |
| **Quest 3** | 72fps | 14ms | YOLOv8n achieves ~60fps, BodyPix ~30fps |
| **Quest 2** | 60fps | 16ms | Thermal throttling, use Quest 3/Pro |
| **Android** | 30fps | 33ms | Vulkan required, device-dependent |

**Optimization Checklist**:
- ✅ Quantize to FP16 (4.2x compression, <0.5% accuracy loss)
- ✅ Lower input resolution (512x384 → 320x240 for 2-4x speedup)
- ✅ Frame skipping (run inference every 2-5 frames)
- ✅ GPU backend (Metal/Vulkan, not CPU)
- ✅ Remove NMS from ONNX, implement in C# with Burst

### 8.4 Integration Pattern

**Recommended Architecture**:
```
ARDepthSource (compute) → VFXARBinder (per-VFX) → VFX Graph
         ↓
  BodyPartSegmenter (ML) → SegmentedDepthToWorld.compute → Body-specific position maps
         ↓
  YOLODetector (optional) → C# ObjectTracker → AR anchors
```

**Why Not Replace BodyPixSentis?**
- ✅ Already optimized for MetavidoVFX use case
- ✅ 24-part segmentation is rich data source (6 position maps)
- ✅ Real-time performance verified on iPhone
- ⚠️ YOLO would add object detection but not improve segmentation

**Where to Add ML**:
1. **Voice-to-Object** (Spec 009) - Use YOLOv8n for object class filtering
2. **Hand Gestures** - MediaPipe Hands for pinch/grab/swipe detection
3. **Depth Fallback** - MiDAS for non-LiDAR devices
4. **Face VFX** - MediaPipe Face Mesh for cross-platform (not just ARKit)

---

## 9. Key Repositories for Reference

### 9.1 Official Unity

| Repo | Description | Stars |
|------|-------------|-------|
| [Unity-Technologies/sentis-samples](https://github.com/Unity-Technologies/sentis-samples) | Official Sentis examples | 310 |
| [Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents) | Reinforcement learning (now uses Sentis) | 16K+ |
| [Unity-Technologies/barracuda-release](https://github.com/Unity-Technologies/barracuda-release) | Deprecated Barracuda package | - |

### 9.2 Keijiro Takahashi (Unity Expert)

| Repo | Model | Use Case | iOS |
|------|-------|----------|-----|
| [BodyPixSentis](https://github.com/keijiro/BodyPixSentis) | BodyPix | 24-part segmentation + pose | ✅ |
| [Minis](https://github.com/keijiro/Minis) | MediaPipe Hands | Hand detection | ✅ |
| [SelfieBarracuda](https://github.com/keijiro/SelfieBarracuda) | BodyPix | Portrait segmentation (deprecated) | ✅ |

### 9.3 Community ONNX Runtime

| Repo | Description | Backends |
|------|-------------|----------|
| [asus4/onnxruntime-unity](https://github.com/asus4/onnxruntime-unity) | ONNX Runtime plugin for Unity | CoreML, DirectML, CPU |
| [asus4/onnxruntime-unity-examples](https://github.com/asus4/onnxruntime-unity-examples) | Example projects | Multi-platform |
| [cj-mills/onnx-directml-unity-tutorial](https://github.com/cj-mills/onnx-directml-unity-tutorial) | DirectML tutorial | Windows |

### 9.4 MediaPipe Unity

| Repo | Description | Platforms |
|------|-------------|-----------|
| [homuler/MediaPipeUnityPlugin](https://github.com/homuler/MediaPipeUnityPlugin) | Full MediaPipe suite | iOS, Android |
| [keijiro/FaceLandmarkBarracuda](https://github.com/keijiro/FaceLandmarkBarracuda) | Face landmarks (468 points) | iOS, Android |

---

## 10. Sources & Further Reading

### Official Documentation
- [Unity Sentis Overview](https://docs.unity3d.com/Packages/com.unity.sentis@2.1/manual/index.html)
- [Unity AI Inference Package](https://docs.unity3d.com/Packages/com.unity.ai.inference@latest)
- [ONNX Runtime Execution Providers](https://onnxruntime.ai/docs/execution-providers/)
- [Hugging Face Unity Sentis Models](https://huggingface.co/docs/hub/en/unity-sentis)

### Research Papers (2025)
- [Real-Time Object Detection and Boundary Extraction in AR](https://dl.acm.org/doi/10.1145/3746709.3746719) - MobileNet + Unity Sentis
- [Real-Time Character Animation Generation](https://dl.acm.org/doi/full/10.1145/3746709.3746725) - Facial recognition with Sentis
- [YOLOv1 to YOLOv11 Survey](https://arxiv.org/html/2508.02067v1) - YOLO evolution

### Community Discussions
- [Sentis vs Onnxruntime](https://discussions.unity.com/t/sentis-vs-onnxruntime/299328)
- [Does Sentis work on Android mobile?](https://discussions.unity.com/t/does-sentis-work-on-android-mobile/346403)
- [Sentis support for Unity Web platform](https://discussions.unity.com/t/sentis-support-for-the-unity-web-platform/1552688)

### GitHub Resources
- [Unity Sentis Samples](https://github.com/Unity-Technologies/sentis-samples)
- [asus4/onnxruntime-unity](https://github.com/asus4/onnxruntime-unity)
- [keijiro/BodyPixSentis](https://github.com/keijiro/BodyPixSentis)

### Meta Quest Documentation
- [Unity Sentis For On-Device ML/CV Models](https://developers.meta.com/horizon/documentation/unity/unity-pca-sentis/)
- [Unity Inference Engine](https://developers.meta.com/horizon/documentation/unity/unity-ai-unity-inference-engine/)

---

## 11. Conclusion

**Key Takeaways**:

1. **Unity Sentis (Inference Engine)** is production-ready for mobile XR (iOS, Android, Quest) but has WebGL limitations.

2. **Platform Performance**:
   - ✅ iOS: 30-60fps with FP16 quantization, Metal GPU backend
   - ✅ Quest 3: 60fps for detection, 30fps for segmentation (YOLOv8n/BodyPix)
   - ⚠️ Android: Requires Vulkan (Android 7.0+), device-dependent
   - ❌ WebGL: Single-threaded WASM, no compute shaders, memory leaks

3. **Model Zoo**:
   - ✅ Hugging Face has verified Unity models (unity/* namespace)
   - ✅ MobileNet/EfficientNet for classification/segmentation
   - ✅ YOLOv8n/v11 for object detection (6-22MB, 30-60fps)
   - ✅ BodyPix for person segmentation (Keijiro implementation)

4. **ONNX Runtime** offers better performance (8x faster) but more complex integration. Use for:
   - Advanced projects requiring CoreML/ANE/DirectML
   - Desktop/server ML workloads
   - Non-Unity platforms (native iOS/Android)

5. **Sentis is recommended** for Unity mobile XR because:
   - ✅ Tighter Unity integration (no native plugins)
   - ✅ Cross-platform single codebase
   - ✅ Editor testing support
   - ✅ Acceptable performance (30-60fps for small models)

6. **Quantization is critical** for mobile:
   - 4.2x model compression with <0.5% accuracy loss
   - FP16 recommended (50% size reduction, minimal loss)
   - INT8 for extreme optimization (75% reduction, requires calibration)

7. **WebGL is not viable** for real-time ML (as of 2026):
   - Wait for WebGPU/WebNN support
   - Use server-side ML for WebGL deployment
   - Or limit to very small models (MobileNetV3-Small @ 256x256)

**For MetavidoVFX**: Continue using BodyPixSentis for segmentation, consider adding YOLOv8n for object detection (voice-to-object placement), and use ONNX Runtime with CoreML backend if maximum performance needed on iOS.

---

**Research Completed**: 2026-01-20
**Auto-added to Knowledge Base**: `/Users/jamestunick/Documents/GitHub/Unity-XR-AI/KnowledgeBase/_UNITY_SENTIS_ONNX_MOBILE_XR_RESEARCH_2026.md`
**Total Sources**: 45+ (web searches, GitHub repos, documentation, research papers)
