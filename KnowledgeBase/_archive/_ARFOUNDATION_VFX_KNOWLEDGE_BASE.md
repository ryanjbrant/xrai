# AR FOUNDATION VFX - KNOWLEDGE BASE

**Purpose**: Track insights, patterns, and techniques learned from analyzing the 520+ GitHub repos in [_MASTER_GITHUB_REPO_KNOWLEDGEBASE.md](_MASTER_GITHUB_REPO_KNOWLEDGEBASE.md)

**Last Updated**: 2026-01-21 (VFXBinderManager deprecated, Hybrid Bridge is current)

---

## ‚ö†Ô∏è CRITICAL: Live AR vs Encoded Streams

### Understanding Pipeline Origins

| Project | Original Data Source | Our Adaptation |
|---------|---------------------|----------------|
| **keijiro/Rcam4** | NDI network stream (iPhone ‚Üí PC) | **Live AR Foundation** (local device) |
| **keijiro/MetavidoVFX** | Encoded .metavido video files | **Live AR Foundation** (local device) |

**Key Insight**: When adapting Keijiro's patterns, extract data from **live AR Foundation camera** NOT from:
- ‚ùå Remote encoded/decoded NDI video feed (Rcam approach)
- ‚ùå Pre-recorded Metavido encoded videos (MetavidoVFX original approach)

### Performance Comparison (Verified)

| Factor | Live AR | NDI Stream | Encoded Video |
|--------|---------|------------|---------------|
| **Latency** | ~16ms (1 frame) | ~50-100ms | ~30-50ms |
| **CPU Overhead** | Minimal | NDI decode | Video decode |
| **Mobile Friendly** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê |

### Multi-Hologram Scalability

> ‚ö†Ô∏è **UPDATED 2026-01-21**: VFXBinderManager is **DEPRECATED**. Use **ARDepthSource + VFXARBinder** (Hybrid Bridge Pipeline) instead.
> See `MetavidoVFX-main/Assets/Documentation/VFX_PIPELINE_FINAL_RECOMMENDATION.md`

**Hybrid Bridge Pattern** (current recommendation):
- `ARDepthSource.cs` - Single compute dispatch for ALL VFX ‚Üí O(1) compute cost
- `VFXARBinder.cs` - Lightweight per-VFX binding (SetTexture only)
- Same PositionMap shared by all holograms
- Each additional hologram = ~0.3ms binding cost only

| Holograms | Single Compute | Duplicate Compute |
|-----------|----------------|-------------------|
| 1 | ~2ms GPU | ~4ms GPU |
| 10 | ~5ms GPU | ~7ms GPU |
| 20 | ~8ms GPU | ~10ms GPU |

**Sources**: [AR Foundation 6.1 Changelog](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/changelog/CHANGELOG.html), [keijiro/Rcam2](https://github.com/keijiro/Rcam2), [keijiro/Metavido](https://github.com/keijiro/Metavido), [Apple Metal Docs](https://developer.apple.com/documentation/metal/compute_passes/calculating_threadgroup_and_grid_sizes)

---

## üéØ KNOWLEDGE CATEGORIES

### 1. Human Depth/Segmentation ‚Üí VFX Patterns

#### Key Technique: ARKit Human Segmentation ‚Üí Point Cloud
**Source**: PeopleOcclusionVFXManager.cs (Portals_6 - VERIFIED)
**Repos**: YoHana19/HumanParticleEffect, keijiro/Rcam2-4, keijiro/MetavidoVFX

**Pattern**:
```csharp
// 1. Get ARKit human textures
Texture2D stencilTexture = m_OcclusionManager.humanStencilTexture;  // Binary mask
Texture2D depthTexture = m_OcclusionManager.humanDepthTexture;      // Depth map

// 2. Convert depth ‚Üí 3D positions via Compute Shader
Matrix4x4 invVPMatrix = (camera.projectionMatrix * camera.worldToLocalMatrix).inverse;
computeShader.SetTexture("DepthTexture", depthTexture);
computeShader.SetMatrix("InvVPMatrix", invVPMatrix);
computeShader.Dispatch(...);

// 3. Feed to VFX Graph
vfx.SetTexture("Color Map", cameraTexture);
vfx.SetTexture("Stencil Map", stencilTexture);
vfx.SetTexture("Position Map", positionTexture);  // From compute shader
```

**Insights**:
- ARKit provides 256x192 resolution = 49,152 points max
- Compute shader on GPU is essential (CPU would be 10-20x slower)
- Inverse view-projection matrix converts screen-space depth ‚Üí world-space positions
- Stencil texture masks non-human pixels (binary: 0 or 1)

**‚ö†Ô∏è CRITICAL: Safe AR Texture Access (Added 2026-01-20)**

AR Foundation texture property getters (`humanDepthTexture`, `humanStencilTexture`, `environmentDepthTexture`) throw `NullReferenceException` **internally** when the AR subsystem isn't ready. The `?.` null-coalescing operator does NOT protect because the exception happens inside the getter.

```csharp
// ‚ùå WRONG - crashes when AR isn't ready:
var depth = occlusionManager?.humanDepthTexture;  // ?. doesn't help!

// ‚úÖ CORRECT - TryGetTexture pattern:
Texture TryGetTexture(System.Func<Texture> getter)
{
    try { return getter?.Invoke(); }
    catch { return null; }
}
var depth = TryGetTexture(() => occlusionManager.humanDepthTexture);
var stencil = TryGetTexture(() => occlusionManager.humanStencilTexture);
```

**Crash Stack**: `UnityEngine.XR.ARFoundation.AROcclusionManager.get_humanDepthTexture()` ‚Üí `UpdateExternalTexture()` ‚Üí NullReferenceException

**Source**: MetavidoVFX BUG 6 fix (2026-01-20), verified on device

**Applications**:
- Particle effects following body silhouette
- Body-reactive VFX (fire, water, electricity around person)
- Volumetric capture-like effects

**Related Repos**:
- keijiro/Rcam2: Remote depth streaming
- EyezLee/ARVolumeVFX: LiDAR volume effects
- cdmvision/arfoundation-densepointcloud: Point cloud visualization

---

#### Key Technique: 91-Joint Skeleton ‚Üí VFX Attachment
**Source**: BoneController.cs (AR Foundation samples - VERIFIED)
**Repos**: fncischen/ARBodyTracking, genereddick/ARBodyTrackingAndPuppeteering

**Pattern**:
```csharp
// ARFoundation provides 91 joints (not 17!)
const int k_NumSkeletonJoints = 91;

void ApplyBodyPose(ARHumanBody body) {
    var joints = body.joints;
    for (int i = 0; i < k_NumSkeletonJoints; ++i) {
        XRHumanBodyJoint joint = joints[i];
        // Attach VFX/brushes to joint positions
        SpawnParticlesAt(joint.localPose.position);
    }
}
```

**Insights**:
- **48 finger joints** (24 per hand) - full hand articulation (thumb: 4, index/middle/ring/pinky: 5 each)
- 7-segment spine - detailed torso tracking
- 18 head/facial joints - eye tracking, jaw, chin, nose
- Perfect for attaching brushes, effects, or physics objects to body parts
- **CRITICAL**: 91 total joints, NOT 17 (see PLATFORM_COMPATIBILITY_MATRIX.md)

**Applications**:
- Brush strokes from hand joints
- Particle trails from fingertips
- Avatar control with full finger tracking

**Related Repos**:
- LightBuzz/Body-Tracking-ARKit: Body tracking samples
- oculus-samples/Unity-Movement: Quest body/face tracking

---

#### Key Technique: ARKit Mesh ‚Üí GraphicsBuffer ‚Üí VFX Particles
**Source**: MeshVFX.cs, SoundWaveEmitter.cs (EchoVision/Reality Design Lab - VERIFIED 2026-01-16)
**Repos**: [realitydeslab/echovision](https://github.com/realitydeslab/echovision)

**Pattern** (AR Mesh to VFX Pipeline):
```csharp
// MeshVFX.cs - Core pipeline
[SerializeField] ARMeshManager meshManager;
[SerializeField] VisualEffect vfx;
const int BUFFER_STRIDE = 12; // 12 bytes for Vector3
GraphicsBuffer bufferVertex, bufferNormal;

void LateUpdate() {
    IList<MeshFilter> mesh_list = meshManager.meshes;

    // Sort meshes by distance to head (prioritize nearby)
    listMeshDistance.Clear();
    for (int i = 0; i < mesh_list.Count; i++) {
        float distance = Vector3.Distance(head_pos, mesh_list[i].sharedMesh.bounds.center);
        listMeshDistance.Add((distance, i));
    }
    listMeshDistance.Sort((x, y) => x.Item1.CompareTo(y.Item1));

    // Fill buffer with nearest meshes up to capacity
    for (int i = 0; i < listMeshDistance.Count; i++) {
        MeshFilter mesh = mesh_list[listMeshDistance[i].Item2];
        listVertex.AddRange(mesh.sharedMesh.vertices);
        listNormal.AddRange(mesh.sharedMesh.normals);
        if (listVertex.Count > bufferInitialCapacity) break;
    }

    // Push to VFX via GraphicsBuffer
    bufferVertex.SetData(listVertex);
    bufferNormal.SetData(listNormal);
    vfx.SetInt("MeshPointCount", listVertex.Count);
    vfx.SetGraphicsBuffer("MeshPointCache", bufferVertex);
    vfx.SetGraphicsBuffer("MeshNormalCache", bufferNormal);

    // VisionPro compatibility: push mesh transform
    vfx.SetVector3("MeshTransform_position", mesh_list[0].transform.position);
    vfx.SetVector3("MeshTransform_angles", mesh_list[0].transform.rotation.eulerAngles);
}
```

**Insights**:
- GraphicsBuffer.Target.Structured with stride=12 for Vector3 data
- Sort meshes by distance - render closest first within buffer capacity
- ARKit iOS: mesh vertices are at world coordinates (position at 0,0,0)
- VisionPro: meshes have non-zero transforms - push MeshTransform_* to VFX
- Buffer capacity 64,000-100,000 vertices typical for mobile
- LateUpdate() ensures camera/pose is updated before mesh processing

**VFX Graph Properties**:
| Property | Type | Description |
|----------|------|-------------|
| MeshPointCache | GraphicsBuffer | World-space vertex positions |
| MeshNormalCache | GraphicsBuffer | Vertex normals |
| MeshPointCount | int | Number of valid vertices |
| MeshTransform_position | Vector3 | Mesh origin (VisionPro) |
| MeshTransform_angles | Vector3 | Mesh rotation euler (VisionPro) |
| MeshTransform_scale | Vector3 | Mesh scale (VisionPro) |

**Applications**:
- AR environment visualization with particles
- Sound wave effects on mesh surfaces
- LiDAR point cloud rendering
- Spatial audio visualization

**Related Repos**:
- keijiro/Smrvfx: Skinned mesh sampling with VFX Graph
- keijiro/Akvfx: Azure Kinect to VFX Graph
- keijiro/Rsvfx: RealSense depth camera to VFX

---

### 2. Audio Reactive VFX Patterns

#### Key Technique: Microphone FFT ‚Üí VFX Properties
**Source**: streamAudio.cs, VFXAudioSpectrumHistoryBinder.cs (Portals_6 - VERIFIED)
**Repos**: keijiro/LaspVfx, keijiro/Reaktion, keijiro/unity-audio-spectrum

**Pattern**:
```csharp
// 1. Microphone capture
AudioClip clip = Microphone.Start(device, loop:true, length:10, freq:44100);
AudioSource.clip = clip;
AudioSource.Play();

// 2. FFT analysis
float[] samples = new float[4096];  // Power of 2
AudioListener.GetSpectrumData(samples, 0, FFTWindow.BlackmanHarris);

// 3. Map to VFX properties
float bass = samples[0..10].Average();
float mid = samples[10..100].Average();
float treble = samples[100..1024].Average();

vfx.SetFloat("BassAmount", bass);
vfx.SetFloat("MidAmount", mid);
vfx.SetFloat("TrebleAmount", treble);
```

**Insights**:
- Use 4096 samples for good frequency resolution
- BlackmanHarris window reduces spectral leakage
- Lower indices = bass frequencies (20-200 Hz)
- Higher indices = treble frequencies (2-20 kHz)
- Update rate: 30-60 Hz (Update() or FixedUpdate())

**Applications**:
- Music-reactive particle spawning
- Bass-driven VFX scaling
- Rhythm-based brush effects

**Related Repos**:
- keijiro/Lasp: Low-latency audio signal processing
- tomer8007/real-time-audio-fft: iOS FFT optimization

---

#### Key Technique: Sound Wave Emission System
**Source**: SoundWaveEmitter.cs (EchoVision/Reality Design Lab - VERIFIED 2026-01-16)
**Repos**: [realitydeslab/echovision](https://github.com/realitydeslab/echovision)

**Pattern** (Voice-Reactive Sound Waves):
```csharp
// SoundWaveEmitter.cs - Multiple concurrent waves
const int MAX_SOUND_WAVE_COUNT = 3;
SoundWave[] soundwaves = new SoundWave[MAX_SOUND_WAVE_COUNT];

void Update() {
    // Smooth audio values
    smoothedSoundVolume = Mathf.SmoothDamp(smoothedSoundVolume, audioProcessor.AudioVolume, ref temp_vel, 0.05f);

    // Emit wave when volume exceeds threshold
    if (audioProcessor.AudioVolume > emitVolumeThreshold) {
        EmitSoundWave();
    } else {
        StopAllSoundWaves();
    }

    UpdateSoundWave();
    PushIteratedChanges();
}

void EmitSoundWave() {
    SoundWave wave = soundwaves[nextEmitIndex];
    wave.origin = head_transform.position;
    wave.direction = head_transform.forward;
    wave.speed = Random.Range(soundwaveSpeed.x, soundwaveSpeed.y);
    wave.life = Random.Range(soundwaveLife.x, soundwaveLife.y);
    wave.angle = Remap(smoothedSoundVolume, 0, 1, soundwaveAngle.x, soundwaveAngle.y);

    // Push to VFX
    vfx.SetVector3("WaveOrigin", wave.origin);
    vfx.SetVector3("WaveDirection", wave.direction);
    vfx.SetFloat("WaveRange", wave.range);
    vfx.SetFloat("WaveAngle", wave.angle);
    vfx.SetFloat("WaveAge", wave.age_in_percentage);
}
```

**Insights**:
- 3 concurrent waves allows overlapping emission while previous waves fade
- Volume-driven angle: quiet = narrow cone, loud = wide spread
- Pitch affects wave lifetime: higher pitch = longer-lasting waves
- Wave parameters packed in transform structs for waves 1-2 (optimization)
- Dual output: VFX properties + material shader arrays for mesh effects

**VFX Graph Properties**:
| Property | Type | Description |
|----------|------|-------------|
| WaveOrigin | Vector3 | Emission point (head position) |
| WaveDirection | Vector3 | Wave travel direction |
| WaveRange | float | Current wave expansion radius |
| WaveAngle | float | Cone angle (90-180¬∞ based on volume) |
| WaveAge | float | 0-1 normalized lifetime |
| WaveMinThickness | float | Minimum wave ring thickness |
| WaveParameter1_* | Transform | Packed params for wave 1 |
| WaveParameter2_* | Transform | Packed params for wave 2 |

**Applications**:
- Voice visualization in AR (EchoVision bat echolocation)
- Audio-reactive ripple effects on AR mesh
- Sound wave propagation visualization
- Spatial audio direction indicators

---

### 3. VFX Graph Architecture Patterns

#### Pattern: Property Binders for Dynamic Data
**Source**: VFXAudioSpectrumHistoryBinder.cs (Portals_6 - VERIFIED)

**Pattern**:
```csharp
// VFX Property Binder pattern
[VFXBinder("Audio/Spectrum History")]
public class VFXAudioSpectrumHistoryBinder : VFXBinderBase {
    [VFXPropertyBinding("Texture2D")]
    public ExposedProperty spectrumHistoryProperty;

    public override bool IsValid(VisualEffect component) {
        return component.HasTexture(spectrumHistoryProperty);
    }

    public override void UpdateBinding(VisualEffect component) {
        // Generate spectrum history texture
        Texture2D history = GenerateSpectrumHistory();
        component.SetTexture(spectrumHistoryProperty, history);
    }
}
```

**Insights**:
- Property binders decouple data sources from VFX Graph
- Custom binders for audio, depth, tracking data
- Update binding once per frame in UpdateBinding()
- Use [VFXPropertyBinding] attribute for type safety

**Related Repos**:
- Unity-Technologies/VisualEffectGraph-Samples: Official VFX examples
- fuqunaga/VFXGraphSandbox: Experimental VFX techniques

---

### 4. Compute Shader ‚Üí VFX Pipeline

#### Pattern: Depth Texture ‚Üí Position Texture via Compute
**Source**: PeopleOcclusionVFXManager.cs (Portals_6 - VERIFIED)

**Compute Shader**:
```hlsl
// NOTE: Use 32x32 threads for modern GPUs (matches AMD warp=64, NVidia=32)
// Dispatch with: Mathf.CeilToInt(width / 32.0f), Mathf.CeilToInt(height / 32.0f)
[numthreads(32, 32, 1)]
void DepthToPosition(uint3 id : SV_DispatchThreadID) {
    float2 uv = (float2)id.xy / _TextureSize;
    float depth = tex2D(DepthTexture, uv).r;

    // Screen-space to world-space conversion
    float4 clipPos = float4(uv * 2 - 1, depth, 1);
    clipPos.y *= -1;  // Flip Y for Unity

    float4 worldPos = mul(InvVPMatrix, clipPos);
    worldPos /= worldPos.w;  // Perspective divide

    PositionTexture[id.xy] = worldPos;
}
```

**Insights**:
- Compute shaders run on GPU (massively parallel)
- Use 32x32 thread groups for texture processing (64 threads matches AMD warps)
- CRITICAL: Dispatch must use same divisor as numthreads (32.0f, not 8.0f!)
- Inverse view-projection matrix converts screen‚Üíworld
- Output to RenderTexture for VFX Graph consumption

**Applications**:
- Depth ‚Üí point cloud conversion
- Custom position textures for VFX
- Real-time mesh deformation

---

### 5. ARKit LiDAR Patterns

#### Key Technique: Scene Depth API
**Repos**: cdmvision/arfoundation-densepointcloud, Unity-Technologies/arfoundation-demos

**Pattern**:
```csharp
// Access LiDAR depth data
AROcclusionManager occlusionManager;
Texture2D depthTexture = occlusionManager.environmentDepthTexture;

// Higher resolution than human depth
// iPhone 12 Pro+: 256x192 typical
// Can reach higher resolutions depending on scene
```

**Insights**:
- LiDAR provides full environment depth (not just humans)
- Higher quality than ARKit depth estimation
- Works in low light (unlike depth estimation)
- Available on iPhone 12 Pro and newer

---

### 6. Multiplayer AR Patterns

#### Pattern: Apple MultipeerConnectivity for Co-located AR
**Source**: holokit/apple-multipeer-connectivity-unity-plugin
**Repos**: realitydeslab/netcode-transport-multipeer-connectivity

**Pattern**:
```csharp
// Local network discovery (no internet needed)
MultipeerConnectivity.StartAdvertising();
MultipeerConnectivity.StartBrowsing();

// Share AR anchor data
ARWorldMap worldMap = ARSession.GetCurrentWorldMap();
byte[] mapData = worldMap.Serialize();
MultipeerConnectivity.SendToAllPeers(mapData);

// Synchronize positions
Vector3 position = transform.position;
MultipeerConnectivity.SendToAllPeers(position);
```

**Insights**:
- No internet required (Bluetooth + WiFi Direct)
- Low latency for co-located experiences
- AR anchor sharing for shared coordinate systems
- Perfect for multiplayer painting, games

**Related Repos**:
- Unity-Technologies/arfoundation-samples: Collaborative sessions
- holokit/netcode-transport-multipeer-connectivity: Netcode integration

---

### 7. WebGL/WebRTC Streaming Patterns

#### Pattern: Unity ‚Üí Web Streaming
**Repos**: Unity-Technologies/UnityRenderStreaming, ossrs/srs-unity

**Pattern**:
```csharp
// Stream Unity camera to web browser
WebRTC.Initialize();
WebRTC.CreateVideoTrack(camera);
WebRTC.CreatePeerConnection();

// Receive input from web client
WebRTC.OnDataChannel += (channel) => {
    channel.OnMessage += (data) => {
        // Handle web input (touch, mouse, gamepad)
    };
};
```

**Insights**:
- WebRTC for low-latency streaming (<100ms)
- Can stream iOS AR to web browsers
- Two-way communication (render + input)
- Phone as controller, browser as display

**Related Repos**:
- microsoft/MixedReality-WebRTC: WebRTC for Unity
- endel/NativeWebSocket: WebSocket alternative

---

### 8. Neural Rendering / ML Patterns

#### Pattern: Unity Sentis for On-Device ML
**Repos**: Unity-Technologies/sentis-samples, asus4/onnxruntime-unity

**Pattern**:
```csharp
// Load ONNX model
Model model = ModelLoader.Load("model.sentis");
IWorker worker = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);

// Run inference
Tensor input = new Tensor(new TensorShape(1, 3, 224, 224), imageData);
worker.Execute(input);
Tensor output = worker.PeekOutput();

// Use output for VFX/gameplay
float[] predictions = output.ToReadOnlyArray();
```

**Insights**:
- Sentis runs ONNX models on GPU
- Good for pose estimation, segmentation, style transfer
- Can replace custom ML pipelines
- iOS/Android support

**Related Repos**:
- homuler/MediaPipeUnityPlugin: MediaPipe integration
- keijiro/FaceLandmarkBarracuda: Face landmarks ML

---

## üß† ARCHITECTURAL INSIGHTS

### Best Practices from 500+ Repos

**1. Separation of Concerns**
- **Data Source** (ARKit, Microphone, LiDAR) ‚Üí **Processor** (Compute Shader, FFT) ‚Üí **Visualizer** (VFX Graph)
- Use Property Binders to connect data sources to VFX
- Keep VFX Graphs simple, do processing in C#/Compute Shaders

**2. GPU Acceleration**
- Always use Compute Shaders for texture/array processing
- VFX Graph runs on GPU (millions of particles)
- Avoid per-frame CPU array operations

**3. Resolution Trade-offs**
- ARKit human: 256x192 (49K points) - good for particles
- LiDAR: Variable resolution - higher quality, more data
- Audio FFT: 4096 samples (2048 frequencies) - balance resolution vs latency

**4. Platform Considerations**
- iOS: ARKit, LiDAR (iPhone 12 Pro+), MultipeerConnectivity
- Android: ARCore (limited depth), MediaPipe ML
- WebGL: Limited to WebRTC streaming, no native AR

**5. Performance Targets**
- 60 FPS target for AR experiences
- 30 FPS acceptable for complex VFX
- Budget: 16ms per frame @ 60fps, 33ms @ 30fps

---

## üìö REPO CATEGORIES & USE CASES

### When to Use Each Type

**Human Segmentation Repos** (Use when):
- Need body silhouette effects
- Artistic VFX around person
- Body-reactive particles
- **Best for**: PeopleOcclusionVFXManager pattern

**Skeleton Tracking Repos** (Use when):
- Need joint positions
- Attaching objects to body parts
- Avatar control
- **Best for**: BoneController 91-joint pattern

**Audio Reactive Repos** (Use when):
- Music visualization
- Rhythm games
- Sound-driven effects
- **Best for**: FFT ‚Üí VFX property binding

**LiDAR/Depth Repos** (Use when):
- Environment scanning
- Occlusion effects
- 3D reconstruction
- **Best for**: Scene understanding, meshing

**Multiplayer Repos** (Use when):
- Co-located AR experiences
- Shared painting/building
- Local multiplayer games
- **Best for**: MultipeerConnectivity pattern

**WebGL/Streaming Repos** (Use when):
- Remote viewing of AR
- Phone as controller
- Web-based experiences
- **Best for**: Unity RenderStreaming

---

## üéØ IMPLEMENTATION PRIORITY

Based on Portals_6 needs:

**High Priority** (Already partially implemented):
1. ‚úÖ Human segmentation ‚Üí VFX (DONE: PeopleOcclusionVFXManager)
2. ‚úÖ 91-joint skeleton (DONE: AR Foundation built-in)
3. ‚ö†Ô∏è Audio reactive (PARTIAL: FFT done, needs brush integration)
4. ‚úÖ Face tracking + blendshapes (DONE: FaceVFXManager)

**Medium Priority** (Next steps):
5. Multiplayer painting (Normcore integration)
6. Brush attachment to skeleton joints
7. Audio-driven brush parameters

**Low Priority** (Future features):
8. WebGL streaming (Unity ‚Üí Web)
9. Neural rendering (Sentis integration)
10. Advanced LiDAR effects

---

## üìù KNOWLEDGE ACCUMULATION PROTOCOL

**When analyzing a new repo from the master list:**

1. **Document the pattern**:
   - What problem does it solve?
   - What's the core technique/algorithm?
   - Code snippets of key patterns

2. **Cross-reference**:
   - Which Portals_6 features use this?
   - Which other repos use similar techniques?

3. **Extract insights**:
   - Performance considerations
   - Platform limitations
   - Best practices

4. **Track applications**:
   - Use cases
   - When to use vs alternatives
   - Integration examples

5. **Update this file** with new sections as patterns emerge

---

## üîó QUICK REFERENCE

**Key Contributors to Study**:
- **Keijiro Takahashi**: VFX Graph master, 50+ innovative projects
- **HECOMI**: Face tracking, OSC, depth visualization
- **HoloKit Team**: Stereoscopic AR, multiplayer
- **Unity Technologies**: Official samples, best practices

**Essential Reading Order**:
1. Unity-Technologies/arfoundation-samples (basics)
2. Unity-Technologies/VisualEffectGraph-Samples (VFX fundamentals)
3. keijiro/Rcam2, Rcam3, Rcam4 (depth streaming evolution)
4. keijiro/LaspVfx (audio reactive)
5. homuler/MediaPipeUnityPlugin (ML integration)

---

## üîß QUICK IMPLEMENTATION SNIPPETS

**Platform Compatibility**: See [PLATFORM_COMPATIBILITY_MATRIX.md](../PLATFORM_COMPATIBILITY_MATRIX.md) for each pattern's platform support.

**Key Notes**:
- ‚úÖ **iOS/Vision Pro**: Full ARKit support (human depth, 91-joint tracking, face tracking)
- ‚ö†Ô∏è **Quest 3/Pro**: 70-joint body tracking (Movement SDK), environment depth (not human-specific)
- ‚ùå **WebGL**: AudioListener.GetSpectrumData() NOT supported (use Web Audio API)
- ‚ö†Ô∏è **DOTS on WebGL**: 10x slower, single-threaded (use traditional ParticleSystem)

**Full implementations**: Each snippet links to verified GitHub repos below.

---

### ARKit Human Depth ‚Üí VFX (5 lines)

**Pattern**: From YoHana19/HumanParticleEffect, keijiro/Rcam2

```csharp
// In Update() or coroutine
var depth = occlusionManager.humanDepthTexture;
if (depth != null) {
    vfx.SetTexture("HumanDepthTexture", depth);
    vfx.SetInt("ParticleCount", 49152); // 256x192 resolution
}
```

### ARKit Human Stencil ‚Üí Edge Detection (10 lines)

**Pattern**: From PeopleOcclusionVFXManager.cs (Portals_6 verified)

```csharp
Texture2D stencilTexture = occlusionManager.humanStencilTexture;
Texture2D depthTexture = occlusionManager.humanDepthTexture;

// Convert depth to world positions via compute shader
Matrix4x4 invVPMatrix = (camera.projectionMatrix * camera.worldToLocalMatrix).inverse;
computeShader.SetTexture(kernelHandle, "DepthTexture", depthTexture);
computeShader.SetMatrix("InvVPMatrix", invVPMatrix);
computeShader.Dispatch(kernelHandle, width / 8, height / 8, 1);

vfx.SetTexture("PositionMap", positionTexture);
vfx.SetTexture("StencilMap", stencilTexture);
```

### 91-Joint Skeleton ‚Üí Particle Trails (8 lines)

**Pattern**: From BoneController.cs (AR Foundation samples)

```csharp
void ApplyBodyPose(ARHumanBody body) {
    const int k_NumSkeletonJoints = 91; // NOT 17!
    var joints = body.joints;

    for (int i = 0; i < k_NumSkeletonJoints; i++) {
        if (joints[i].tracked) {
            vfx.SetVector3($"Joint{i}Position", joints[i].localPose.position);
        }
    }
}
```

### Audio FFT ‚Üí VFX Properties (12 lines)

**Pattern**: From streamAudio.cs, VFXAudioSpectrumHistoryBinder.cs (Portals_6 verified)

```csharp
// Setup
float[] samples = new float[4096]; // Power of 2
AudioListener.GetSpectrumData(samples, 0, FFTWindow.BlackmanHarris);

// Extract frequency bands
float bass = samples.Take(10).Average();
float mid = samples.Skip(10).Take(90).Average();
float treble = samples.Skip(100).Take(924).Average();

// Apply to VFX
vfx.SetFloat("BassAmount", bass * 100);
vfx.SetFloat("MidAmount", mid * 100);
vfx.SetFloat("TrebleAmount", treble * 100);
```

### DOTS Million Particles (Quest 90fps)

**Pattern**: From pablothedolphin/DOTS-Point-Clouds

```csharp
[BurstCompile]
public partial struct ParticleUpdateSystem : ISystem {
    public void OnUpdate(ref SystemState state) {
        new ParticleUpdateJob {
            deltaTime = SystemAPI.Time.DeltaTime,
            time = (float)SystemAPI.Time.ElapsedTime
        }.ScheduleParallel();
    }
}

[BurstCompile]
partial struct ParticleUpdateJob : IJobEntity {
    public float deltaTime, time;

    public void Execute(ref LocalTransform transform, in ParticleData particle) {
        // Update 1 million particles @ 90fps
        transform.Position += particle.velocity * deltaTime;
        transform.Rotation = math.mul(transform.Rotation,
            quaternion.RotateY(particle.rotationSpeed * deltaTime));
    }
}
```

### Hybrid P2P + Normcore (Cost Optimization)

**Pattern**: From holokit/apple-multipeer-connectivity-unity-plugin, Normcore

```csharp
void InitializeNetworking(int expectedUsers) {
    if (expectedUsers <= 8) {
        // FREE: P2P via MultipeerConnectivity (iOS/macOS/visionOS)
        var session = new MultipeerSession("app-id", playerId);
        session.StartAdvertising();
        session.StartBrowsing();
    } else {
        // PAID: Normcore for scale ($0.25/user/month)
        var realtime = GetComponent<Realtime>();
        realtime.Connect($"room-{roomId}");
    }
}
```

### Platform-Specific Particle Limits

**Pattern**: Cross-platform optimization

```csharp
public static int GetMaxParticles() {
    #if UNITY_IOS
        return SystemInfo.deviceModel.Contains("iPhone15") ? 750000 : 500000;
    #elif UNITY_ANDROID
        // Quest 3 has better GPU than Quest 2
        return OVRPlugin.GetSystemHeadsetType() == OVRPlugin.SystemHeadset.Quest_3 ? 1000000 : 500000;
    #elif UNITY_STANDALONE_OSX
        // M3 Max with 128GB RAM
        return SystemInfo.systemMemorySize > 65536 ? 2000000 : 1000000;
    #elif UNITY_VISIONOS
        return 750000; // Conservative for Vision Pro
    #else
        return 100000; // Desktop fallback
    #endif
}
```

### glTF Export with Draco Compression

**Pattern**: From needle-tools/UnityGLTF, KhronosGroup/UnityGLTF

```csharp
using UnityGLTF;

public async Task<string> ExportWorldToGLTF(GameObject root) {
    var exporter = new GLTFSceneExporter(root.transform, new ExportOptions {
        DracoCompression = true,     // 70-90% size reduction
        BinaryFormat = true,          // .glb instead of .gltf + .bin
        TextureMaxSize = 2048         // Optimize for web
    });

    await exporter.SaveGLBAsync("world.glb");

    // Upload to CDN (Cloudflare R2 recommended: $0.015/GB, FREE egress)
    string cdnUrl = await UploadToCDN("world.glb");
    return cdnUrl;
}
```

### WebRTC SFU Setup (10-50 users)

**Pattern**: From Unity-Technologies/com.unity.webrtc, ossrs/srs-unity

```csharp
using Unity.WebRTC;

void SetupWebRTC() {
    var config = new RTCConfiguration {
        iceServers = new[] {
            new RTCIceServer { urls = new[] { "stun:stun.l.google.com:19302" } }
        }
    };

    var connection = new RTCPeerConnection(ref config);

    // Add local video track (Unity camera)
    var videoTrack = camera.CaptureStreamTrack(1920, 1080);
    connection.AddTrack(videoTrack);

    // Receive remote streams
    connection.OnTrack = evt => {
        Debug.Log($"Received track from peer: {evt.Track.Kind}");
    };
}
```

### Unity Sentis ML Inference (On-Device)

**Pattern**: From Unity-Technologies/sentis-samples, keijiro/BodyPixSentis

```csharp
using Unity.Sentis;

public class SentisInference : MonoBehaviour {
    Model model;
    IWorker worker;

    void Start() {
        // Load ONNX model
        model = ModelLoader.Load("model.sentis");
        worker = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);
    }

    void RunInference(Texture2D inputImage) {
        // Preprocess image to tensor
        Tensor input = TextureConverter.ToTensor(inputImage, 224, 224, 3);

        // Run model on GPU
        worker.Execute(input);
        Tensor output = worker.PeekOutput();

        // Use output (e.g., body segmentation mask)
        ApplySegmentationMask(output);

        input.Dispose();
    }
}
```

### Rcam4 RGBD Point Cloud Streaming

**Pattern**: From keijiro/Rcam4 (Unity 6 native RGBD streaming)
**Source**: Phase 2 Research - Portals v3 (2024-12-04)

```csharp
// Rcam4 architecture: Position texture streaming (RGBAFloat format)
public class RGBDPointCloudReceiver : MonoBehaviour {
    Texture2D _positionMap;  // RGBAFloat: (x, y, z, confidence)
    Texture2D _colorMap;     // RGB24: Camera feed

    void UpdateVFXGraph() {
        // Position map encodes 3D coordinates in texture pixels
        // Each pixel = Vector4(worldX, worldY, worldZ, confidence)
        vfxGraph.SetTexture("PositionMap", _positionMap);
        vfxGraph.SetTexture("ColorMap", _colorMap);

        // VFX samples position directly from texture UV
        // No GraphicsBuffer conversion needed
    }
}
```

**Insights**:
- **FREE** unlike Rcam (commercial license)
- Unity 6 native capture APIs (no deprecated Screen.width dependencies)
- Position texture = pre-computed 3D coordinates (not depth map)
- RGBAFloat format allows world-space positions, not just normalized depth
- Production-tested at 30-60 FPS with 1920√ó1080 input

**Applications**:
- Remote device as RGBD camera for Unity Editor preview
- Point cloud streaming over network (LAN or WebRTC)
- AR Foundation depth ‚Üí VFX without compute shaders

**Performance**:
- ~5ms texture upload per frame (1920√ó1080)
- NetworkReader handles streaming with <100ms latency on LAN
- Scales better than GraphicsBuffer for large point clouds (>100k points)

---

### Echovision ARMesh ‚Üí VFX Hologram Pattern

**Pattern**: From keijiro/Echovision (<15ms hologram rendering)
**Source**: Phase 2 Research - Portals v3 (2024-12-04)

```csharp
// ARMeshManager ‚Üí VFX Graph direct pipeline
using UnityEngine.XR.ARFoundation;

public class ARMeshToVFX : MonoBehaviour {
    ARMeshManager _meshManager;
    VisualEffect _vfxGraph;
    GraphicsBuffer _vertexBuffer;

    void OnMeshChanged(ARMeshesChangedEventArgs args) {
        foreach (var mesh in args.updated) {
            // Extract mesh vertices directly
            var meshFilter = mesh.GetComponent<MeshFilter>();
            Vector3[] vertices = meshFilter.sharedMesh.vertices;

            // Upload to GPU buffer
            _vertexBuffer?.Dispose();
            _vertexBuffer = new GraphicsBuffer(
                GraphicsBuffer.Target.Structured,
                vertices.Length,
                sizeof(float) * 3
            );
            _vertexBuffer.SetData(vertices);

            // Feed to VFX Graph
            _vfxGraph.SetGraphicsBuffer("MeshVertices", _vertexBuffer);
            _vfxGraph.SetInt("VertexCount", vertices.Length);
        }
    }
}
```

**Insights**:
- **<15ms render time** - key to maintaining 60 FPS
- ARMeshManager provides world-space mesh automatically (no manual reconstruction)
- GraphicsBuffer avoids CPU‚ÜíGPU texture conversion overhead
- VFX Graph samples vertices as particle spawn positions
- Works on Quest 3 (scene mesh API) and iOS (ARKit scene reconstruction)

**Applications**:
- Hologram effects conforming to real-world geometry
- Particle effects "painting" discovered surfaces
- Environment-aware VFX (particles bounce off walls)

**Performance Tips**:
- Limit vertex count: `_meshManager.meshPrefab.GetComponent<MeshFilter>().sharedMesh.vertexCount < 10000`
- Use mesh subsystem's coarser classification (wall/floor only)
- Update GraphicsBuffer only on mesh changes, not every frame

---

### Normcore Drawing Multiplayer Sync

### 9. Metavido Volumetric VFX Pattern

#### Key Technique: ARKit Depth + Camera Feed ‚Üí Volumetric VFX
**Source**: ARKitMetavidoBinder.cs (Implemented in MetavidoVFX-main)
**Repos**: keijiro/MetavidoVFX

**Pattern**:
```csharp
// 1. Binder Script
public sealed class ARKitMetavidoBinder : VFXBinderBase {
    public override void UpdateBinding(VisualEffect component) {
        // Bind ARKit Environment Depth (LiDAR)
        var depth = _occlusionManager.environmentDepthTexture;
        component.SetTexture("DepthMap", depth);

        // Bind Camera Feed (Color)
        // Note: Requires blit or access to ARCameraBackground texture
        // component.SetTexture("ColorMap", cameraTexture);

        // Bind Inverse View Matrix for World Reconstruction
        var iview = _camera.cameraToWorldMatrix;
        component.SetMatrix4x4("InverseView", iview);

        // Bind Ray Params (Intrinsics approximation)
        float fovV = _camera.fieldOfView * Mathf.Deg2Rad;
        float h = Mathf.Tan(fovV * 0.5f);
        float w = h * _camera.aspect;
        component.SetVector4("RayParams", new Vector4(w, h, 0, 0));
    }
}
```

**Insights**:
- **Volumetric Visualization**: Uses LiDAR depth to reconstruct a dense point cloud of the environment.
- **VFX Graph**: The `Particles.vfx` graph uses the depth map and ray params to position particles in world space.
- **Binder Pattern**: Decouples the AR data source (ARFoundation) from the VFX implementation, allowing for easy swapping of inputs (e.g., recorded data vs. live AR).

**Applications**:
- Real-time volumetric video of the environment.
- "Matrix-like" world visualization.
- AR portals that reveal a point-cloud version of the real world.


**Pattern**: From Normal/Drawing (Normcore ownership-based sync)
**Source**: Phase 2 Research - Portals v3 (2024-12-04)

```csharp
// RealtimeComponent ownership model for stroke synchronization
using Normal.Realtime;

public class NetworkedStroke : RealtimeComponent<NetworkedStrokeModel> {
    List<Vector3> _localPoints = new List<Vector3>();

    // Called when stroke is created locally
    public void BeginStroke() {
        // Request ownership of this stroke object
        RequestOwnership();
    }

    public void AddPoint(Vector3 worldPos) {
        if (!isOwnedLocallySelf) return;  // Only owner can modify

        _localPoints.Add(worldPos);
        model.points.Add(worldPos);  // Auto-syncs to other clients
    }

    public void EndStroke() {
        model.isComplete = true;  // Marks stroke as finalized
        ClearOwnership();  // Release ownership (optional)
    }

    // Called on remote clients when stroke updates
    protected override void OnRealtimeModelReplaced(
        NetworkedStrokeModel previousModel,
        NetworkedStrokeModel currentModel
    ) {
        if (currentModel.isFreshModel) return;  // Skip initialization

        // Rebuild stroke from synced points
        RebuildStrokeMesh(currentModel.points);
    }
}
```

**Insights**:
- **Ownership-based sync** prevents conflicts (only owner writes)
- RealtimeArray<T> handles dynamic point list synchronization automatically
- `isFreshModel` check prevents rebuilding on local creation
- ClearOwnership() allows other clients to edit stroke after completion
- Normcore handles interpolation and packet loss recovery

**Applications**:
- Multiplayer 3D painting/drawing
- Collaborative AR annotations
- Shared VFX effects across devices

**Cost Optimization**:
- <8 users: Apple MultipeerConnectivity (FREE P2P)
- 8-50 users: Normcore ($0.25/user/month)
- >50 users: WebRTC SFU + dedicated server

**Performance**:
- ~10ms update latency on LAN
- ~50-100ms over internet (depends on region)
- RealtimeArray uses delta compression (only sends new points)

---

## üì¶ RECOMMENDED PACKAGE VERSIONS (Verified 2025-11-02)

```json
{
  "dependencies": {
    "com.unity.xr.arfoundation": "6.1.0",
    "com.unity.xr.arkit": "6.1.0",
    "com.unity.xr.arcore": "6.1.0",
    "com.unity.xr.hands": "1.5.0",
    "com.unity.xr.interaction.toolkit": "3.1.1",
    "com.unity.xr.meta-openxr": "2.1.0",
    "com.unity.xr.openxr": "1.14.3",
    "com.normalvr.normcore": "2.16.2",
    "com.unity.webrtc": "3.0.0-pre.8",
    "com.unity.entities": "1.3.8",
    "com.unity.entities.graphics": "1.4.5",
    "com.unity.burst": "1.8.21",
    "com.unity.visualeffectgraph": "17.1.0",
    "com.unity.sentis": "2.1.2",
    "io.holokit.unity-sdk": "https://github.com/holokit/holokit-unity-sdk.git",
    "jp.keijiro.bodypix": "3.0.0",
    "jp.keijiro.klak.motion": "1.1.0",
    "jp.keijiro.smrvfx": "1.1.6"
  }
}
```

---

## üîó CROSS-REFERENCES

### Auto-Fix Patterns (106 total)
See `_AUTO_FIX_PATTERNS.md` for auto-applicable fixes:
- **AR Foundation**: TryGetTexture, depth scale (0.625), UV rotation
- **VFX Graph**: ExposedProperty, global texture limitation, HLSL signatures
- **Compute Shaders**: Thread group 32x32, CeilToInt dispatch
- **Hand Tracking**: Hysteresis, velocity-driven emission

### Quick Fixes
See `_QUICK_FIX.md` for error‚Üífix lookup:
```bash
kbfix "AR texture"     # AR texture null fixes
kbfix "VFX property"   # VFX not updating
kbfix "depth"          # Depth-related issues
```

### Intelligence Systems
- `_INTELLIGENCE_SYSTEMS_INDEX.md` - Central reference
- `_CONTINUOUS_LEARNING_SYSTEM.md` - Pattern extraction workflow
- `_SELF_HEALING_SYSTEM.md` - Auto-recovery patterns

---

**Last Updated**: 2026-01-22
**Status**: Knowledge base with quick implementation snippets + auto-fix integration
**Next**: Analyze repos as needed, add insights to relevant sections
